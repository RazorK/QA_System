2,inzinjerstvo,I believe it's termed injecting not pumping but yes it's certainly possible Coal fired electric power plants grind the coal to a very fine dust and then blow the dust into the central burner This particular article is a summary of some DOE research for a Blast Furnace Granulated Coal Injection System Demonstration Project And this article discusses some of the hazards from a National Fire Protection Association perspective Coal dust having the particular dangerous property of being explosive and all that As far as specifics regarding danger I suspect it will vary depending upon the material(s) involved in the powder
24,inzinjerstvo,Most stainless steels contain nickel around 10% It is combined normally with chrome and molybdenum The main problem that a natatorium (i e swimming pool) is a highly corrosive environment and most stainless steel alloy is a little bit slowly corroding here Even aluminium gets a greyish-white surface layer In the case of constructive materials the situation is a little bit better First because they are mostly in the wall and the concrete around them partially protects them In case of freely available surfaces plastic or painting is yet used
248,inzinjerstvo,The barometric height formula is defined as $$ p(z) = p(0)\exp(-Mgz/(RT))$$ For a centrifuge the upper expression is transformed to $$ p(r) = p(0)\exp(M\omega^2r^2/(2RT))$$ As you see the $g$ is transformed to the radial acceleration $a = \omega^2 r$ The factor $2$ stems from the integration which you have to do during the derivation of the barometric height formula (see barometric height formula derivation) And of course the minus sign disappeared because the force points outward Both formulas hold also for a mixture of gases I cite Kemp R Scott Gas Centrifuge Theory and Development A Review of US Programs Science and Global Security 17 (2009) 1-19 When the rotor contains a mixture of gases the distribution holds independently for each species So as a first approximation it is as I suggested in the comment section The cited source gives the following equations for a two-isotope gas $$ p_A(r) = p_A(0)\exp(M_A\omega^2r^2/(2RT))$$ $$ p_B(r) = p_B(0)\exp(M_B\omega^2r^2/(2RT))$$ A separation factor can be calculated $$ \alpha_0 = \frac{p_A(0)}{p_B(0)}/\frac{p_A(r)}{p_B(r)} =\exp((M_B- M_A)\omega^2r^2/(2RT))$$ Keep in mind that all formulas here don't include convection of any kind In earth's atmosphere we have too much convection and temperature differences for those formulas to work Furthermore in centrifuges to improve their performance there is a current-flow introduced (see the cited source )
8,inzinjerstvo,Unlike within the United States and the PE license it doesn't appear that it's possible to receive an EUR ING license by direct comity EU Engineering licensing is handled by FEANI and on their EUR ING page they state Application is open only to individuals if they are members of an engineering association represented in FEANI through a National Member But it may be possible to join one of the German FEANI members and see if they can support a reciprocity or comity process 1 1 There are other member organization in other countries but your question specifically mentioned Germany Digging a bit deeper it does appear that there may be limited cases where an EUR ING will be awarded based upon sufficient experience as a professional engineer Take a look through the FEANI EUR ING guide in particular Section 5 which details the minimum requirements Most of the cases cover qualifications based upon an education received within an FEANI member country Section 5 4b deals with special circumstances and provides the potential pathway that you could receive an EUR ING license Nevertheless it is possible to consider such alternative routes Very strict procedures however have then to be followed (see 7 1) and the applicant must have at least 15 years of Professional Engineering Experience recognized by FEANI So it does appear to be possible but again it's not as simple as the comity process within the United States
30,inzinjerstvo,The work history and temperature history of any metal can make a big difference in the final shape after machining If the material was cold worked (e g rolled) there may be significant residual stresses within the material When you start selectively removing material those stresses may cause the part to warp into a new shape Cast metals and ones that have been given surface treatments can have the same problem You might be able to address the problem by roughing the part to near-final shape and then running a finishing pass to remove that last little bit of material Otherwise take a look at the specifications of the material You may need to heat treat the part to relieve those stresses This is a problem that can exist with just about any metal not just Al
39,inzinjerstvo,Paradoxically maybe the home-sized magnetic levitation could be a narrow area where superconducting solutions could be cheap I explain why Although room-temperature superconductivity is a dream room superconductivity is not -) There are soon superconducting materials which can be cooled by liquid air and they aren't even costly Liquid air is also relatively cheap Maybe a liquid air processing machine were costly but you don't need to buy that Only to buy the superconducting alloy and the liquid air (nitrogen) For the first I tried ebay But it depends what you want to reach I once played chess with figures levitated by Meissner-effect The result could be
142,inzinjerstvo,A yahkchal is an example of a type of passively cooled building in Iran They utilise a combination of passive evaporative cooling and thick thermally insulating walls in order to keep the interior temperatures low enough First wind is directed into underground aquifers known as qanat They are then cooled due to the low humidity desert air causing water to evaporate The cooled air then flows through the interior of the yakhchal cooling the interior The thick insulating walls (filled with earth and various insulating materials such as straw and feathers) help to insulate the cool interior from the hot exterior therefore maintaining a low temperature inside the yakhchal
19,inzinjerstvo,The combustion chamber and nozzle of the Space Shuttle main engine were cooled with liquid hydrogen Liquid hydrogen was also the fuel It was used as a coolant before it was burned ( regenerative cooling ) Another example is the air breathing rocket engine SABRE Here too liquid hydrogen is used as fuel It's also used for liquefying the atmospheric oxygen In the industry one advantage that liquid helium and nitrogen have over hydrogen is that helium and nitrogen are inert and non-combustible Probably less corrosive too p s The question has a superconduction tag So I don't know if this is the sort of application of liquid hydrogen cooling that the O P is looking for
36,inzinjerstvo,The main problem which I can see in your idea that your system will have a cumulative error Only calculating this won't be enough you will have to find alternate solutions too In similar (but maybe bigger) scenarios for exampe for drones there is a similar problem The solution is using the wheel rotation counters to get a fast real-time but buggy input data (which contains the cumulative error as well) In case of flying drones this data is coming from giros and accelerometers your task is much simpler compared to their But you should get an alternate information source too (In case of the drones it is normally the GPS) This can be GPS or some other thing - there is a wide spectrum of possibilities Visual image processing Precalibrated ultrasound markers Marker paintings on the floor If I suspect correctly the size of your experiment maybe the last would be most promising to you
83,inzinjerstvo,You can use a sort of double diameter washer I've made that name up but I am pretty sure such a device exists That's a cross section of one of your linkages The white things are the aluminum pieces the grey one is the screw the yellow is the nut while the red is the double diameter washer If you can found some washer that is slightly longer than your aluminum bars thikness you are good to go Of course if it's too long the linkage may become lousy but with a good cutter you can trim them down or you can also add some regular washers between the red one and the nut
43,inzinjerstvo,You could try Wikipedia but they note that the article may not meet there quality standards so I'll stay away from there as a source The World Nuclear Association seems more credible anyway Their page discusses the process Quoting the section entitled Reprocessing today – PUREX (emphasis mine) All commercial reprocessing plants use the well-proven hydrometallurgical PUREX (plutonium uranium extraction) process This involves dissolving the fuel elements in concentrated nitric acid C hemical separation of uranium and plutonium is then undertaken by solvent extraction steps (neptunium – which may be used for producing Pu-238 for thermo-electric generators for spacecraft – can also be recovered if required) The Pu and U can be returned to the input side of the fuel cycle – the uranium to the conversion plant prior to re-enrichment and the plutonium straight to MOX fuel fabrication Block of text $\to$ coherent summary Chemical solvents are used Further below the WNA notes The used fuel is chopped up and dissolved in hot concentrated nitric acid The first stage separates the uranium and plutonium in the aqueous nitric acid stream from the fission products and minor actinides by a countercurrent solvent extraction process using tributyl phosphate dissolved in kerosene or dodecane In a pulsed column uranium and plutonium enter the organic phase while the fission products and other elements remain in the aqueous raffinate In a second pulsed column uranium is separated from plutonium by reduction with excess U4+ added to the aqueous stream Plutonium is then transferred to the aqueous phase while the mixture of U4+ and U6+ remains in the organic phase It is then stripped from the organic solvent with dilute nitric acid The plutonium nitrate is concentrated by evaporation then subject to an oxalate precipitation process followed by calcination to produce PuO2 in powder form The uranium nitrate is concentrated by evaporation and calcined to produce UO3 in powder form It is then converted to UO2 product by reduction in hydrogen Compounds of importance tributyl phosphate kerosene and dodecane and aqueous affinate are used as solvents after the nitric acid Then there is a reaction with $U^{4+}$ and uranium and plutonium are separated That's the chemical part I haven't been able to find much information on the specific mechanical methods (i e the pure engineering you were looking for) In fact the original patent for PUREX does not cover the mechanical processes leading me to believe that the exact setup can vary This pdf is somewhat helpful It says that the input materials are first chopped up Perforated baskets are then used to separate this machinery from the rest of the solution; a wheel dissolver may be used I do have some information albeit for a non-PUREX method For aqueous reprocessing (go to page 12) mixer settlers (mixing chambers with settling chambers) columns and centrifuges are the main pieces of equipment
111,inzinjerstvo,I know your question is about springs but I don't think that springs are the answer to your desired application As you mentioned the inward force from air pressure is very large so not only do you need a strong spring but you also need a material for the paneling which is light and can take those kind of forces Additionally you would need to evacuate the air that seeps in through the material at a molecular level There are good reasons lighter-than-air aircraft use a low-density gas inside the balloon There is no requirement for exceptionally strong and light materials There is no (or very low) pressure differential between the inside and outside and therefore no tendency for the gas to escape or the atmosphere to seep in You may be interested to read the Wikipedia article on vacuum airships in particular the section on material constraints This shows the proof that even using a strong shape (hollow sphere) made of diamond the pressures are too great for the structure to withstand once you thin it down to be light enough to become buoyant Given that in these ideal conditions it is not possible it seems unlikely that a spring in a balloon would be possible However the Wikipedia article does leave open the possibility that if the wall of the sphere was not solid for example if it was made of a honeycomb structure then it may still be possible I have no idea if anyone has done the math on this though
40,inzinjerstvo,Double nutting can be an easy option for this I was about to describe it myself but these folks do a great job This image from the article models the forces that do the job Take note of the contact surfaces between the treading of the bolt in the top vs bottom nut
205,inzinjerstvo,For tapping threads there are a few steps between free hand tapping and fully automatic machines Which one is right for you will depend on the material you're tapping the size of the threads you're tapping and how large your production run is The first step would be to buy a guided tap wrench which is a regular hand tap wrench with a bushing you can put on the top and chuck into a drill press (or mill) or lathe This helps keep your tap square to the workpiece and also lets you apply a little pressure with the quill If you want to get into power tapping (And need more precision than a cordless drill with the clutch turned down ) there are reversing tapping heads you can attach to a drill which reverses the tap out when threading is complete These are available for drill presses or hand-held drills They run around \$500-$1 000 here's another style of tap chuck called a tension compression chuck which allows a little vertical give but I believe that is only of much use if your spindle is computer controlled If you want a more robust solution and have some money there are tapping presses that start at around $3 000 It's not quite 6 figures but they aren't cheap Depending on what you are making self threading screws may also be a valid option There are a wide variety of types that work in different situations As for external threads after die cutting the only cheaper method I'm aware of is roll threading This requires a dedicated machine that is relatively expensive unless you're working on very small threads
75,inzinjerstvo,Most bridges (and overpasses) are built to cross over something With a few notable exceptions most of these somethings are relatively long perpendicular to the desired crossing direction and fairly narrow parallel to it Therefore a simple two directional bridge best meets the needs of the engineering problem Engineers always try to solve a problem in the simplest possible way to prevent introducing more problems than they have solved If a simple two-way bridge solves the problem then there is no reason to complicate the solution Along those lines vehicular traffic flow on a three-way bridge can be fairly complicated and may not easily facilitate high traffic volume I believe that is why you will find more pedestrian bridges created in this style As for Michigan I don't know It's entirely possible that a structural engineer or engineering company in the area had a fondness for that type of structure and bid on those projects with three-way designs
41,inzinjerstvo,Very probably no Galvanic corrosion required the existence of a reactive and conductive medium which you don't have If there is some (for example at least moist air or similar) best you can do to paint both of the metals if you can Generally if none of the materials would corrode in an environment galvanic corrosion is a non-issue (in that environment)
99,inzinjerstvo,In general you want to stay below the recrystallization temperature Steel is composed of grains and different types of steel have different grain sizes The size of these grains affects the steels behavior once it gets past the yield point At the recrystallization temperature new grains will nucleate and grow which undoes any sort of hardening that the steel may have previously gone through However this temperature will vary depending on the alloying elements in the steel so if you don't know the grade it'll be hard to know the recrystallization temperature Unless you're going to be working it around 900°F (500°C) I don't think that should be an issue This chart shows the temperatures at which different heat treatments are done Source
53,inzinjerstvo,Based on it being an EN steel grade The first number is 100x the carbon content percentage (so 0 11%) the letters are added elements (sulphur and maganese) and the last number is the sulfur content (0 30%) You can see the full details here The full format seems to be [X][% carbon][added elements][% of added elements hyphenated] Note that the X is only present for stainless steels Here is a good example Note also that this scheme is somewhat ambiguous The percentages are only an approximation and the example you gave is interesting because it lists Sulfur before Maganese despite the naming convention stating that they should be listed in order of content That's alright for getting quick basic info about the steel but for anything else you may want to use the EN number 1 0715 rather than the name Wikipedia has details on the format Given this classification you can find out much more about the steel's properties and see what general category it fits into The site I linked first says this EN 10277-3 2008 Bright steel products Technical delivery conditions Free-cutting steels EN 10087 1999 Free cutting steels Technical delivery conditions for semi-finished products hot rolled bars and rods
1945,inzinjerstvo,SC-FDMA is encoded and transmitted from the handset where there are power tradeoffs This encoding is better than OFDMA for the handset for the following related reasons Reduces peak to average power ratio Increases efficiency of the power amplifier Increases battery life So at the base station they can employ transmitters that can handle a higher peak transmit power with lower efficiency as it has a significant amount of power available But on a mobile handset this requires more expensive components more space and probably most importantly more power since peak transmissions are not as efficient as transmitting at a lower level but for a longer period of time (ie higher peak to average ratio means lower energy efficiency more heat more power) The tradeoff is a little lower spectrum efficiency but it's worth it for increased battery life and particularly since most high speed data is going to the handset not coming from it This short video explains further
105,inzinjerstvo,Two points make a line so a long tube with a photodiode on the bottom should be able to find the sun (assuming no clouds ) In regards to Solar Photometry I think Forrest Mims is making photometers from the wavelength response of different LEDS using LEDs as detectors
96,inzinjerstvo,I will leave the main part of the previous answer below as it contains more information about the differences between several radio protocols As for the differences between Zigbee 3 0 and Z-Wave let's do a side by side comparison Frequency Zigbee uses the 2 4GHz band while Z-wave uses the 868MHz band in Europe and the 900MHz ISM band in the US (see the frequency coverage here ) OSI Layers Zigbee is based on the IEEE 802 15 4 spec for the PHY/MAC Layers where Z-Wave is based on ITU-T G 9959 rPHY/MAC The Z-Wave protocol stack was developped by Sigma Designs (and the standard is maintained by the Z-Wave Alliance) whereas Zigbee is developped by the Zigbee Alliance Both networks have mesh capabilities Ecosystem Z-Wave claims to have more than 1200 products on the market Zigbee claims a similar figure Documentation Zigbee 3 0 spec is (for now) only available to Alliance members (the other Zigbee standards can be downloaded after entering some information (which can be bogus if you somehow want to stay anonymous)) Z-Wave requires you to buy the dev kit or join the Alliance to access the spec Certification Both protocols will require a certification to be fully certified and to be able to use the protocol logo More generally here is what you need to take into account when choosing a protocol Frequency The protocols you named all uses the 2 4GHz band but a lot of other protocols uses the 868MHz the 915MHz the 433MHz band The idea is the lower the frequency the lower the energy use and greater the range At least in theory One also have to consider the physical size of the antenna (the lower the frequency the bigger the antenna) OSI Layers used Some of the protocols out there only implements one layer of the OSI model and you will need to have another layer implemented to be able to communicate Ease of use of protocol some protocols are very easy to use (for example EnOcean) some are more complicated (Zigbee BLE) They differ in their capabilities and the freedom you have to send the information Bitrate frame size useful payload How much information you can send in a given timeframe Depending on your use you may need to send a lot of data where different protocols may or may not behave well with this Collision and packet collision avoidance how does the protocol manages the band use and prevent packet collision (two objects sending at the same time) Ecosystem Are there a lot of other devices out there using this protocol What do you want to connect your object to (smartphone directly Internet directly Something else) Do you need a local gateway to access internet Documentation availability Is all the information necessary freely available Is it locked (does it require membership to an association to be read) Is it behind a paywall Last but not least Cost of use To be implemented and sold in your products some protocols will require you to join the Alliance so you can certify your product compliance with the protocol This often cost a lot of money (depends on the protocol) I thinks this pretty much sums it all and should give good pointers to the right direction to look for more information
247,inzinjerstvo,I don't know about any ready-made multi-position bi-stable solenoids but let me help with What are the alternatives Depending on what force you need to exert and distance to cover a monostable solenoid driven by PWM might suffice but that seriously limits the range (as the magnet power rises with square of distance after certain threshold the shaft will drop into the solenoid) Alternatively you can stack two 3-position ones; needle of one moving the frame of another For more than two you'll be better off using some servo/stepper actuators instead And last but not least get a 2-position bi-stable solenoid and some position sensors - either a linear optical encoder or just some open-frame optocouplers along the way at the stop points Power the coils with PWM adapting to current position of the shaft modifying the pulse width ratio so that given optocoupler gives out half the normal level (covered halfway) stopping the shaft wherever you want
116,inzinjerstvo,Considering that reverse osmosis is not the only way to desalinate water I think that yes there is a lot of development potential in desalination but that potential might not lie in improvements to existing techniques To justify this conclusion and illustrate some areas where there could be a lot of development potential I present to you my idea for a combined wave wind and solar desalination and power plant I haven't done any maths on this to calculate the area of land needed or costs or output so it might not be viable as-is But I think the concepts described below (and remember this is just one idea) demonstrate that there is development potential in the following areas Using renewable energy sources on-site to power the plant Using direct-drive energy instead of electrically transmitted energy Directing and amplifying natural processes of desalination Combined wave wind and solar desalination and power plant Inputs No external energy input Cleverly harnessed wave wind and solar Outputs Energy (electricity) Fresh water Cool air Location This plant requires a hot location with large area of cheap land by the ocean and a relatively consistent wind Stage 1 - Wave Pump A wave-powered pump raises sea water into a large lake on land Here is an example of a direct wave-powered pump other types of wave power harnessing typically convert mechanical motion into electricity However that motion can be easily used to directly drive a pump Stage 2 - Evaporation Lake The evaporation lake is a large shallow area covered in a greenhouse-like way to aid evaporation The sea-water flows away from the ocean along channels in the lake-bed then back again towards the ocean in the next adjacent channel where it drains back into the sea This prevents the build-up of deposits as the returning sea-water will take them with it and return to the sea more concentrated The roof may contain Fresnel lenses or other solar concentrators to help evaporation A wind-catching tower blows air across the lake to lower air pressure and aid in evaporation This tower could be like those used in Masdar City or a standard wind turbine tower with either electrical or direct transmission to a series of fans The result is a continuous airflow across the lake which carries the water vapor to the far side where it is channeled up a wide column into the next stage Stage 3 - Condensing Tower The water vapor is channeled up a large column to a condensing chamber high on the tower Here a series of fins are cooled by a heat-pump driven directly by a wind-turbine on top of the tower the water condenses on the fins and drains into a fresh-water tank near the top of the tower Stage 4 - Power Generation The water from the condensing tower is lowered to a height suitable for a standard water-tower through one or more water turbines to generate power Stage 5 - Filtering and treatment The salty sea-air will also condense on the fins and there may be small airborne particles and particles from wear on stages within this process that get into the water so it will probably need further filtering and treatment to make it drinkable Some of the power from the water turbine may be used for this There you have it you have clean water above ground level so pressure is already availabl
58,inzinjerstvo,I don't think they existed and it has its reason First a solar panel can be characterized mainly by its efficiency spectrum on which wavelength which ratio of light energy can it convert to electric power This needs to have its maximum around the visible light becausd the Sun gives most of its energy in this wavelength interval This is because our eyes can see best in this spectrum We simply evolved to the sunlight And this is because home light is also in this wavelength this is what we human like at most There were no need for different solar panels But the power of the sunlight is around some hundred $\frac{W}{m^2}$ although it varies very heavily The light power of a house bulb is around sometimes 10W - not for a $m^2$ but for a whole room Maybe we see in a well lighted room just so good as in sunlight but it is only because our eye is very adaptive The actual light power density is tenth or even hundredth smaller as in sunlight And the efficiency of most solar panels is around 10-20% There are experimental very costly version reaching 40% A solar panel in a room couldn't produce valuable energy at most some watts - on the cost of the price of a solar panel on the roof And the cost is their main problem even with their many many times bigger solar power
64,inzinjerstvo,Customer specified acceptable brightness is going to be a subjective affair unless you have something to calibrate their responses against Loosely you'll need to measure the available light within a particular area and then correlate that measurement with customer surveys With a large enough sample of responses you should be able to determine what constitutes an acceptable level of lighting for your customers I would expect that customer responses will fall along a curve of some sort and you'll eventually be able to determine what the standard deviations are for acceptable One factor to be concerned about is that differing rooms will have differing levels of reflection So even though two rooms may be of similar size and furnishings the more darkly painted room will reflect less light and will be viewed as more dim on a subjective basis To workaround that you might create a dedicated test room and have potential customers come through and provide their feedback While this is a more expensive approach it's a very good way to isolate out variables that you can't otherwise control against To get you started with measuring available light you'll likely need a light meter of some sort This and this are just two examples of what I'm sure you'll be able to readily find with some searching Once you have a lux measurement for the area you can convert that to lumens using a standard calculator Searching will turn up a number of sites such as this one I didn't put the equations here as there are a number of steps involved including determining the area of the room
76,inzinjerstvo,I think you are talking about roundabouts not traffic circles It is baffling to those of us in the UK that Americans think roundabouts are a new idea In the UK we have so many variants from mini-roundabouts all the way up to full motorway junctions (a giant roundabout above or below the motorway) So do roundabouts take up more space Not necessarily this is a mini roundabout It's nothing more than a slightly domed area of paint on the road no lights are necessary you can actually drive straight over the top of it rather than around it its main purpose is simply to dictate who has right of way so that everyone knows who should yield and who should go In a busy town or city environment roundabouts do not work well because excessive traffic from one direction with right of way can completely stop all other traffic causing congestion in other directions Some roundabouts have lights or peak-time signals to prevent this One great thing is that they're easy to modify (adding lights making it mini (drive-overable) adding another entry-point etc Everywhere other than in busy grid-based towns/cities they are ideal So from a highway engineering perspective The main pros are Cheap to build Agile (Flexible / extensible) Scalable to suit any junction size Mutable (add peak signals bypass lanes extra incoming roads) Modular (google image search for double roundabout magic roundabout ) Easy and safe for drivers to use (rules don't change in any configuration) Aids navigation of complex junctions from simple road-signs (just count the exits) Cons are Annoys drivers on country roads when you'd like to just bypass Not suitable for busy city grids
70,inzinjerstvo,It sounds like what you're looking for is a bandpass filter It will filter out any signal outside of a given frequency range If the noise isn't too strong the main signal should come through fairly clearly The actual design of the bandpass filter is another matter depending on what frequency range you need and how prevalent the noise is It may not spit out a perfect sinusoid but it might get you close enough that you could use your least-squares method on the bandpass output to generate the sinusoid
69,inzinjerstvo,Short answer make it thicker Long answer The moment of inertia affects the beam's ability to resist flexing Use one of the many free online moment of inertia calculators (like this one ) to see how increasing the height of the beam will have an exponential effect on increasing the stiffness of the beam And this site helps provide a pictorial view of the load(s) upon a beam depending upon differing configurations such as where the supports are and where the load is applied It also provides a calculator to determine the forces involved Wikipedia has a decent article for area moments of inertia In your particular case you're asking about a filled rectangular area and I x = bh 3 /12 The height has an exponential factor of 3 whereas increasing the base does not have an exponential factor So for the same amount of material increasing the height stiffens the beam better To be clear you can make the beam sag less by increasing the width of the plate It's just more effective to make the plate thicker Current moment I x = 30 * 2 3 / 12 = 20 mm 4 Increase width by 1mm I x = 31 * 2 3 / 12 = 20 6 mm 4 Increase height by 1mm I x = 30 * 3 3 / 12 = 67 5 mm 4 And if for some reason you can't easily increase the thickness of the plate you can consider a different beam structure Currently your beam is a simple rectangle You can easily use a T-beam or an I-beam in order to stiffen the plate instead Again while I've provided some suggested links to online calculators feel free to search for and use others that you may prefer
201,inzinjerstvo,My first thought was to use a small amount of chromium and then another metal (gold) This is how its done for electronics structures on glas The chromium does react with the silicium-oxid and hence has strong bonding I've done this only with layers of very small thickness (sub µm) and only via a PVD -technique But as you can read on this wikipedia page it can be done in this way for glas-metal seals For example with chromium and stainless steel As such I think it is rather cost effective at least if you compare it with pure indium and gallium If you application has temperature gradients be aware of the maybe very different temperature coefficients
314,inzinjerstvo,If you have access to a lathe you should be able to quickly machine a flanged reducer out of Nylon 6 Not sure what temperature you're operating at but make it a bit thick and it will easily be able to handle 3 bar
118,inzinjerstvo,I'm not a pump expert but I would have thought that an Archimedes screw pump would be ideal in such a situation
90,inzinjerstvo,First of all the main documents you need to know DIRECTIVE 2004/108/EC relating to electromagnetic compatibility http //eur-lex europa eu/LexUriServ/LexUriServ do uri=OJ L 2004 390 0024 0037 en PDF DIRECTIVE 2006/95/EC on low voltage devices http //eur-lex europa eu/LexUriServ/LexUriServ do uri=OJ L 2006 374 0010 0019 en PDF (and the version entering into force in April 2016 http //eur-lex europa eu/legal-content/EN/TXT/PDF/ uri=CELEX 32014L0035&amp;from=EN ) Those documents form the basis of your work Also they define if your device is subject to the directive or not Depending on the case you could also be subject to the R&amp;TTE Directive and RoHs Directive) The Annex I of both documents informs us on the essentials requirements to meet Mainly it says that your product must be safe for humans pets and property and that it should not be impacted by the electromagnetic emissions of other nearby devices nor it should impact them with its own emissions What's really interesting lies in the Annex II of the first document and in Annex IV of the second document These parts defines the conformity assessment procedure you'll have to follow There are 8 procedures (called modules) of conformity assessment internal production control (module A); CE type-examination (module B); conformity to type (module C); production quality assurance (module D); product quality assurance (module E); product verification (module F); unit verification (module G); full quality assurance (module H) In our case the one to follow is called Internal Production Control Basically what you have to do is write the technical documentation This document should be made of those parts a general description of the electrical equipment conceptual design and manufacturing drawings and schemes of components sub-assemblies circuits etc descriptions and explanations necessary for the understanding of said drawings and schemes and the operation of the electrical equipment a list of the standards applied in full or in part and descriptions of the solutions adopted to satisfy the safety aspects of this Directive where standards have not been applied results of design calculations made examinations carried out etc test reports We learn what we have to do in order to respect the directives The main constraint here will be showing that you have indeed accessed the harmonised standards This can cost quite a hefty sum of money However Afnor (the French certification organism) provides subscription to their database which can be cost effective compared to buying individual standards) Your local certification organism should have a similar offer The harmonised standards list to be followed is available here for the EMC directive http //ec europa eu/enterprise/policies/european-standards/harmonised-standards/electromagnetic-compatibility/index_en htm for the LVD directive http //ec europa eu/enterprise/policies/europ
114,inzinjerstvo,There are a number of standards to address hydrogen embrittlement in the manufacture and coating of fasteners Here in the US ASTM F1940 and ASTM F519 would apply Hydrogen embrittlement is also a concern for structural welds but should be addressed by your welding code The problem doesn't seem to be in the spec as often as in the quality control at the point of manufacture In situations where this is a big risk requiring third party inspections and testing are likely the most effective step the engineer can take
93,inzinjerstvo,In case Eurocodes do not provide enough information some sources exist In the case of elastic critical moment for lateral-torsional buckling an NCCI (Non-contradictory complementary information) document exists The document code is SN 003 and one version (maybe not the latest) can be accessed here Hopefully this will cover your current needs In addition the French technical centre for steel construction has developped a couple pieces of software that can run the calculation for you LTbeam for beams under bending sollicitations LTbeamN for beams under bending and compression sollicitations
115,inzinjerstvo,So there's an incorrect assumption underlying your question In an ideal world the lifting capacity required of the jacks would be the self-weight of the bridge divided by the number of jacks (+ allowances for wind/snow etc ) And the assumption there is that the lifting capacity is equivalent only to the weight of the bridge The problem is that if anything goes wrong you're likely to see a catastrophic failure of some sort which could lead to irreparable damage Real world lifts don't operate in that ideal manner and instead rely upon a safety factor in order to make sure that the lifted weight is well within the limitations of the equipment And in some cases the safe working limit (SWL) may be derated further from the working lift limit (WLL) if there are any extenuating circumstances such as worn equipment or hazardous weather So the ideal lift capacity is one that is significantly larger than load to be lifted The actual lift capacity used is tempered by the fact that you generally pay for that lift capacity whether you need it or not According to the Wikipedia article on safety factors a factor of 2 is common with building materials and 3 is common for automobiles You need to weigh the risk to human health or safety within the lift you're considering and use an appropriate safety factor A conservative approach would be to use a higher safety factor of 3 so you need at least 3 times the bridge weight for the lifting capacity Assuming you're staying within the SWL and WLL of the lift equipment you're using that still doesn't necessarily account for the binding forces caused by corrosion between the bridge and the bearings supporting it Static friction can also come into play if the bridge itself has to be slid out of the supporting structure Unfortunately it's hard to determine what that binding force is going to add up to without a lot more detail At a minimum you would need to know the materials involved and the cross sectional area of contact between them You would also want to approximate how long they've been exposed to the elements and what sort of conditions the elements have brought - such as salt water exposure vs mountain air This is where I'm going to wave my hands in an airy fashion and not attempt to swag the binding force created by corrosion Static frictional force can be guessed at a little more easily than the corrosion binding $\mu_{static}$ ranges from 0 6 to 0 8 for various materials in contact with steel And while the general formula for calculating the Force from friction is $F_{static} = \mu_{static} * F_{normal}$ that equation also assumes horizontal movement As you're likely lifting vertically not sliding horizontally the static frictional forces will be less since the equivalent $F_{normal}$ for vertical motion will be less Based upon that I'd use a conservative guess of $\frac 23$ or $\frac 34$ of the weight of the bridge to estimate the static friction forces involved in the lift Experience and the particulars of the lift will guide you in adjusting that guess up or down So depending upon your safety factor it could very well be that the SWL of the equipment will provide sufficient lift to overcome any static friction or binding caused by corrosion Or it could be that you need to increase the lift capacity requirements to overcome that effect And it's worth pointing out that equipment can exceed those limits so a lower safety factor may be good enough to get past the effects of static fr
239,inzinjerstvo,It was stated previously that a perfect sphere cannot exist in terms of engineering or manufacturing but ignoring trivialities let's answer the question A Prince Rupert's drop is such that molten glass is viscous enough to droop off your rod and into a bucket of water causing the glass to cool rapidly enough to create high amounts of internal tension which causes the famed effect of making an unbreakable teardrop Even if you were to spin the rod quickly so as to not have a long tail some thin dragging would still exist and make a tail It may be small but it would still be there If you were interested in making it more spherical you might think to shave off the tail end but as you know a single nick or disturbance to the tail end results in a solid glass explosion Let's say you spun the rod in a way (in a magical world) so that there was no tail Then you wouldn't have a Prince Rupert's drop The answer to your question is no it is not possible to make a spherical Prince Rupert's drop because either the glass would explode or you simple don't have the drop you were looking for
104,inzinjerstvo,This is something that I've looked at with tunnels rather than pipes and arguably with smaller deflections Hopefully it'll be some help however If you can satisfy yourself that the rate of curvature is relatively small- then you can approximate the stress generated by the lowering using simple beam theory $$\sigma=\frac{Ey}{R}$$ Where E is the Youngs Modulus y is the distance from the neutral axis you are measure the stress at (i e the radius of the pipe itself) R is the radius of curvature Mechanically you'll need to check that the stresses in the pipe along the parabaola that defines your curve never exceed the limiting stress of the pipe material and (probably more critcally) the capacity of the connections This is a bit of a simplification and depending on the type of pipe the joints themselves will have some play
124,inzinjerstvo,I think I know what you mean The terms selling and salesmanship are somewhat confusing in this context Really you just need to be able to explain the situation and the reasons behind it in a context that the other side can understand This differs when talking to management or engineers I would call this more justification than selling 1 Justifying to management If you are talking to management to get a green-light for an idea you need to not only explain why it is technically viable but also what the business benefits are of this idea above other ideas For example it costs less to initially implement; it is easier to maintain (costs less); it is harder to maintain (creates lucrative service contracts) What is 'good' really depends on the business strategy of the company as highlighted by the last two examples which are opposite from a technical perspective but both might be considered beneficial under different business strategies If you don't like or disagree with the business strategy of your company leave and start your own business and treat your customers better Or at least you can begin to understand why your ideas are not adopted despite being technically fantastic 2 Justifying to engineers It sounds like you've already got the go ahead from management and now you're struggling to convince other engineers why this way is actually better Well the reason you're struggling is because those other engineers don't understand the business strategy and how your idea fits in with it So now you find yourself in the odd position of having to explain the business strategy to engineers You don't have to justify the business strategy itself only why your technical solution fits in better with the business strategy dictated to you from management You need to do this in terms that non-business oriented engineers will understand which can be tricky because quite often the best technical solution is at odds with the business strategy Example I'll use a real-world example that I had to deal with recently to explain This is from a software company but applicable to any engineering situation The company wanted to have two separate work streams (two teams) doing essentially the same thing creating two different products (that do the same thing) for two markets Utter madness technically especially when both products could share so many components and in fact could even be the same product with some minor configuration options to address the differences in these two markets Engineers were struggling to agree with this approach until I explained to them that 80% of the revenue was coming from one market and the competition there was also much more fierce so the business strategy was to focus one team solely on this market without any consideration for supporting the other market so that they could move quickly and stay ahead of the competition The secondary market was still worth exploiting and growing so the second team would focus on that This is the strategy given to us this is the problem we need to solve not that other problem of making the single-product Conclusion I don't think you need to learn sales you just need to be able to identify when other engineers are trying to solve a different problem than you (normally caused by ridiculous non-technical things like customers profit economics and all that malarkey) Then you need to be able to explain what the problem actually is and why so that they jump on board with your (already accepted by management) idea
123,inzinjerstvo,I've found a fairly comprehensive source There are five main types Mechanical pumps A piston and a rotary valve are used the push the concrete through Hydraulic pumps Hydraulics are used to pump the concrete Schwing pumps Two cylinders are used One to receive concrete from the input hopper and one to release it into the output pipe The two cylinders are connected by a flat gate valve Apparently Fredrich Schwing started a company to manufacture concrete pumps Unfortunately their website does not have much information Thomsen pumps Similarly to the Schwing pump two cylinders are used with internal pistons The difference is that a flapper valve (a type of check valve ) is used Squeeze-crete pumps A rotating cylinder is used to squeeze the concrete through
134,inzinjerstvo,You are correct that the cutting speed of the material is what determines the rpm for your drill-bit This actually makes the calculation very simple $$ \text{Spindle speed (RPM)} = \frac{\text{Cutting speed}}{\text{Circumference}} = \frac{\text{Cutting speed}}{? \cdot \text{Diameter}} $$ The thing you need to be careful of is the units of cutting speed and diameter For example Metric If your cutting speed is in $m/min$ and your diameter is in $mm$ then you need to multiply your cutting speed by 1000 so that it is in $mm/min$ Imperial If your cutting speed is in $ft/min$ and your diameter is in $inches$ then you need to multiply your cutting speed by 12 so that it is in $inches/min$ For more information see spindle speed calculations on Wikipedia
139,inzinjerstvo,The primary difference between Rankine and Coulomb earth pressure theories is that Coulomb's considers a frictional retaining wall In other words the interface between the soil and the retaining wall is not assumed frictionless (as it is in Rankine theory) That being said it is typically considered that Rankine underpredicts the true orientation of the failure surface whereas Coulomb overpredicts the orientation In that sense you could use both methods and use the two solutions to bound what will likely occur Terzaghi (and Peck)'s method is largely empirical It simply uses the soil's classification and the backfill slope then they simply tabulated coefficients of lateral earth pressure That being said they aren't bad it's just that any empirical solutions like this tend to be relatively site specific so the solution needs to be taken with a grain of salt
131,inzinjerstvo,This is quite a vague question so only a very broad and vague answer can be given Here are some ways to achieve this Invest in marketing to increase sales or hold more stock so that you can buy parts in larger quantities and get better pricing Negotiate with suppliers and make a commitment to buy a large quantity over time in order to get better pricing without a large initial investment Look for areas of your design where some parts are potentially unnecessary For example non load-bearing parts may be clipped on or glued in place rather than using bolts Fewer or smaller parts may be used in some areas etc Split your product into a core product with optional add-ons to reduce the base-price for entry-level customers Look for alternate materials and alternate parts that achieve the same function without compromising the product design goals
145,inzinjerstvo,You need to ensure that even in the worst case scenario you still meet your measurement spec of 10 +- 1mm If your tolerance is 0 2mm of your measurement then a measurement of 11mm while may look like it meets spec it doesn't because it could be 11 1mm So the worst case that still meets your spec is a measurement of 10 9mm because then with a max tolerance of 0 2mm you still meet 11mm With a 0 2mm tolerance your 10 +-1 spec becomes 10 +- 0 9 How should a measurement of 9 9mm be treated So revised spec is between 9 1mm and 10 9mm so 9 9mm is within spec
149,inzinjerstvo,I would think the friction regimes will be very different especially for something like a gum barrel where the projectile is contained At high speeds plastic deformation of the surface will be much more significant as well as possible chemical reactions due to the heat/energy Therefore I wouldn't necessarily expect the coeffcients to be similar You could possibly could set up a test rig using a high speed touching a flat plate to test the friction at high speed outside a barrel However I haven't thought about the details and this would probably be difficult\expensive\dangerous to set up A quick google gives me this paper which supports my theory that friction would not behave consistently
159,inzinjerstvo,Keep in mind that P-N junctions are created in the first place by diffusing impurities (dopants) into the silicon at elevated temperatures (1000 - 1500 K) The efficiency of the junction is related to how sharply the dopant concentration changes at the junction (its gradient) At lower temperatures such as those to which a solar array is exposed (say 270 - 330 K) diffusion is much much slower but it does not completely stop Given enough time the dopant gradients become less steep reducing the effectiveness of the P-N junction Over the extreme long term (millennia) semiconductor devices will stop working as their junctions disappear altogether
192,inzinjerstvo,The RM-1 Russian submarine reactor had a core of less than one cubic metre It had about 100kg fuel load which was 90% enriched (i e 90kg) Uranium 235 This was liquid-metal cooled [specifically a eutectic lead-bismuth alloy (44 5 wt% lead 55 5 wt% bismuth) - source as below p40] so didn't need a moderator Submarine 901 had in its right-board reactor just 30 6 kg of Uranium 235; this was at 20% enrichment so a total fuel load of 153 kg These were controllable chain-reaction based reactors Source NKS-138 Russian Nuclear Power Plants for Marine Applications Ole Reistad Norwegian Radiation Protection Authority Norway Povl L Ølgaard Risø National Laboratory Denmark Published by Nordic Nuclear Safety Research April 2006 ISBN 87-7893-200-9 http //www nks org/scripts/getdocument php file=111010111120029
161,inzinjerstvo,It's common to start with a shorter stiffer tool such as a center drill or a spotting drill In addition using the shortest drill bit that drills the hole you need will increase stiffness Because of the flutes in a drill the stiffness goes down geometrically as the length increases The other variable you have control over is how you are holding the tool A collet will keep the tool in better alignment than a chuck If you're doing this often it's worth making sure you have a good grasp of feeds and speeds and that you're using the best type of drill for the job Both of those could have some affect on hole perpendicularity
164,inzinjerstvo,As it happens I just recently went through that calculation myself for a different site Given the following facts from a quick web search it isn't difficult to work out the numbers The maximum efficiency of a (large) windmill is about 40% The density of air is 1 225 kg/m 3 You need about 50 mW (10 mA at 5V) to light up an LED First we'll need about 50 mW / 0 40 = 125 mW of air power flowing through the windmill to create the electricity we need (ignoring other factors such as the actual efficiency of a small windmill and the efficiency of the generator) The power of the air flowing through the windmill is 0 5mv 2 where m is the mass rate of the air flowing through the disk defined by the diameter of the blades For example suppose we have a disk of 0 03m 2 (about 20cm in diameter) The mass rate of the air is the area of the disk multiplied by the air velocity multiplied by the density of the air $$\text{Mass rate} = 0 03 \text {m}^2 \cdot v \cdot 1 225 \text{ kg/m}^3 = v \cdot 0 03675 \text{ kg/m}$$ The power of that air is therefore $$P = 0 5 \cdot \text{Mass rate} \cdot v^2$$ Substituting and solving for $v$ $$v = \sqrt[3]{\frac{0 125 \text{ W}}{0 5 \cdot 0 03675 \text{ kg/m}}} = 1 9 \text{ m/s}$$ or about 7 km/h Taking into account the efficiencies we ignored earlier plus the losses in a gear train that might be needed to get the generator RPMs up to a usable level I would probably shoot for about 4&times; the area or about 2&times; the diameter (40-50 cm) in order to get reasonable results
177,inzinjerstvo,Theoretically pontoon bridges with rope anchors keeping them to the bottom would work against wind and flow overcoming the problem jhabbot mentioned in his answer (same as train length limit - stretching force) In practice these come with more problems of their own They drift on water surface and as result rise and fall with water waves The larger the body of water they span the higher the waves; at certain point in stormy weather the bridge would just launch the vehicles into the air The anchoring isn't exactly simple if it's to withstand such forces You could just as well go with pillars these don't add much to complexity They are only a short way above the surface Waves could roll over them washing vehicles off Also they stay level to the local surface - a wave wouldn't need to roll over it - it could just flip a vehicle sideways by twisting the bridge Since segments need to be mobile relative to each over their joints will be uneven forcing a severe speed limit Unless they float freely they'd be very limited with water level If you anchor them firmly water rising (even due to a storm) may submerge them And yet again as the segments need to be at least partially mobile a longitudal force stretching one side of the bridge may lead to the other side to stack segments against each other We have construction technologies that are extremely durable against longitudal forces (stretching compressing) - reinforced concrete steel ropes etc But add lateral forces and the design becomes much harder to keep strong; buckling twisting and loss of stability become very severe With keeping the bridge suspended you keep lateral forces limited to wind If the bridge is partially submerged this goes out of the window The primary upside of pontoon bridges is the simplicity - they can be deployed in matter of hours and as such they play huge role in the military But since they are vulnerable against weather and due to the slew of problems they create - especially with increasing span = size of the body of water = mechanical influence of water conditions they make very poor permanent bridges and so firmly supported bridges are simply superior also where the ratio of vehicle traffic demand versus length of the span of water to cross is too low the right solution is a ferry Ferries can take many vehicles on board and cross the distance (and depths ) not viable for any bridge and of course their cost is a tiny fraction of cost of the bridge covering that distance Note pontoon bridges are okay as temporary solution (say in place of a bridge washed off by flood or for duration of construction of permanent bridge or in locations where building a permanent bridge would be overly expensive or difficult) but they are always considered a poor man's substitute - and while they are okay for crossing a moderately sized river the engineering problems scale up to insurmountable levels as the distance covered rises - they are really unsuitable for very long spans of water
167,inzinjerstvo,The transistor The transistor was the revolutionary replacement of the vacuum tube which had been at the heart of computers for the first half of the 20th century Vacuum tubes themselves had only two main problems They were power-hungry and they were relatively big Relative to their replacements that is They also had a tendency to burn out or leak during operation which could prove disastrous In 1947 John Bardeen and Walter Brattain along with William Shockley their boss at Bell Labs successfully amplified an electric current using germanium This point-contact transistor as it was called was soon used to speed up computing and to make computers smaller and more efficient A good example of the transistor and the vacuum tube is the construction of the Manchester Computers developed at the University of Manchester The first the Small-Scale Experimental Machine (SSEM) (developed in 1947) was a state-of-the-art testbed for new innovations in computing such as the Williams tube But it still used vacuum tubes It had 550 valves and took in 3500 watts of power The SSEM's successor the Manchester Mark 1 (developed in 1949) was much more powerful It used 4050 valves and consumed 25000 watts of power Yet the aptly-named Transistor Computer built in 1955 used only 200 transistors and 1 300 diodes and only used 150 watts It wasn't the first computer to exclusively use vacuum tubes but it was a huge step forward It's tough to say exactly why the transistor was created when it was (I'm answering the last part of your question now) but it could be argued that the computational advances of World War II (such as the Harvard Mark I ) ensured that many new advances would be made in computing; the transistor happened to be one of them The integrated circuit The integrated circuit developed about a decade after the transistor also had profound effects on computing It was developed by Jack Kilby in 1958 - though many others were involved along the way and there are disputes as to who should get the credit for inventing it first - at Texas Instruments He used semiconductors to create an entire computer chip - the integrated circuit An integrated circuit can contain incredible amounts of transistors and it is this complexity and compactness that make it so useful Manufacturing was also much easier and quicker Integrated circuits started a second computing revolution which laid the groundwork for cheaper computers that could be available to the masses Edit Now that the question is focused on the transition between vacuum tubes and transistors I'd like to add something about semiconductors because they play a key role in transistors Semiconductors allow for good conduction of electricity but one of their really useful properties is that their conduction can be modified in a process called <a href= https //en wi
173,inzinjerstvo,The device for taking horizontal and vertical angles that you mention is called a theodolite Theodolites only started being phased out as the main surveying tool in the 1980s when total stations where introduced Below is a Soviet theodolite from 1958 (ex Wikipedia) Theodolites were analogue devices and the angles measured had to be written in a notebook Total stations were electronic devices essentially electronic theodolites with electronic distance measuring devices based on infrared signals These devices could be connected to a portable electronic memory unit with a keypad to store the measurements The surveyor still had to manually enter a point identifier for each reading but didn't have to enter the measured angles When starting a survey a reference marker from the nation system of surveying markers closest to the surveying region was chosen as this had a known/established northing easting and elevation A picture of US survey marker follows (from Wikipedia) The theodolite would be set up and the first reading would be to the known marker peg to establish the baseline for the survey For very accurate surveys a surveying target on tripod was placed over the survey marker; either a plate with a cross on it or a short pointed rod with the point upwards A similar target would then be placed on a temporary marker and the horizontal angle between the two targets measured The vertical angle from theodolite's horizontal plane (in the eye piece) to the first target would be measured as would the vertical angle to the second target Each theodolite has specifically marker dot on it at eye piece (telescope) height This is the reference marker for the theodolite from which lateral distances are measured A measuring tape was place against the dot on the theodolite and the other end of the tape was place at the centre of each target cross or the tips of each pointed target rod to measure the slope distances The measuring tape had to have a certain tension applied and the readings would be recorded Later in the office the measured slope distances would be corrected for tape sag Additionally the heights of the theodolite and the two targets above the ground would be measured with a tape measure Having done all of that another temporary marker would be established the theodolite moved between the last two pegs and the process repeated For each set up the heights of the theodolite and targets was needed as were the slope distances vertical angles and horizontal angle Using trigonometry on all this data one could determine the co-ordinates and elevation of each peg Another method used of measuring was called stadia This used a theodolite but instead of a cross targets or pointed rod targets being used to sight to at each of the survey pegs surveying rods were used See the picture below from http //www tigersupplies com The surveying rod would be placed on each peg and three height reading were taken from the surveying rod the top cross hair the central (main) cross hairs and the bottom cross See the picture below The reading from the central cross hairs gives the height for the elevation The difference between the upper and lower cross hair readings multiplied by an optical constant for the optics of the theodolite gave the distance between the surveying rod and the theodolite Except for some Japanese the
174,inzinjerstvo,Generically speaking the first step would be to determine if your failures have any factors in common You could also examine your successes and see if they have anything in common These commonalities could be things such as the employee working on the product location where the tests were performed specific piece of equipment on which the tests were performed time of day etc Once you have identified some commonalities you can attempt to determine if any of them are the proximate cause of the failures and work backwards until you can determine the ultimate cause Of course the details of how to do this depend on the details of your situation
388,inzinjerstvo,This answer owes a lot to @HDE 226868 who put me on the right track If we simplify the whole bridge into 2D thin beam with a constant section size no internal damping and subject only to small vertical deflections then the natural frequency is determined by simple harmonic motion $$ n_0 = \frac{1}{2 \pi} \sqrt{ \frac{ k } { m } } $$ Where $ n_0 $ is the natural frequency $ k $ is the ratio between restorative force and deflection (the equivalent 'spring stiffness') and $m$ is the mass per unit length of the beam In a beam the restorative force is the internal shear caused by the deflected shape As the force exhibited by a beam is proportional to the rate of change of shear which is related to the stiffness ($EI$) and the rate of change of moment it can be shown (note the deflection is proportional to the length of the beam) that $$ k = \alpha \frac{ EI } { L^4 } $$ Where $E$ is the Young's Modulus of the beam material $I$ is the Second Moment of Inertia of the beam section $L$ is the length of the beam and $\alpha$ is a constant determined by the support conditions and mode number of the response All of the literature I have seen expresses this in a way that more convenient for the frequency equation $$ k = \left( \frac{ K }{L^2} \right)^2 (EI) $$ Substituting back in $$ n_0 = \frac{ K }{ 2 \pi L^2 } \sqrt{ \frac{ EI } {m} } $$ Calculating the value of $K$ is quite involved and there is an exact approach for simple solutions and approximate methods including the free energy method and Raleigh Ritz A few deviations for a simply supported beam can be found here It should be noted that this equation would have been enough but as it requires a table for $K$ and the calculation of a value of $EI$ that represents the bridge as a homogenous beam the authors of the Eurocode seem to have decided it would be better re-integrate the assumption that $k$ is constant along the beam To do this they have used the following relationship $$ \delta_0 = C \frac { w L^4 } { EI } $$ Where $\delta_0$ is the maximum deflection $C$ is a constant dictated by the support conditions $w$ is a constant uniformly distributed load across the length of the beam Under self-weight $w = gm$ where $g$ is acceleration due to gravity (9810 mm/s 2 ; as deflection in this equation is given in mm ) Therefore (re-arranged ) $$ \sqrt { \frac { EI } { m } } = L^2 \sqrt { 9810 } \frac { \sqrt { C } } { \sqrt { \delta_0 } } $$ And so $$ n_0 = \frac { 15 764 K \sqrt { C } } { \sqrt { \delta_0 } } $$ General values for $K$ and $C$ can be found in structural tables- for example here and here respectively For a simply supported beam $$ K = \pi ^ 2 \text{ and } C = \frac { 5 } { 384 } $$ $$ 15 764 K \sqrt { C } = 17 75 $$ $$ n_0 = \frac{ 17 75 } { \sqrt { \delta } } $$
181,inzinjerstvo,This is typically done with a PID (Proportional Integral Derivative) control algorithm There are heaps of literature about designing and optimizing PID controllers so there's not much sense going into a more specific detail here Typically you use the PID controller to regulate speed Assuming your stopping point is also critical there will be some trade-off between accuracy of position and accuracy of deceleration at the end point With relatively low inertia systems and reasonable deceleration values this is not usually a very big issue Wikipedia has a very in depth article on the design of PID controllers at https //en wikipedia org/wiki/PID_controller There are a number of sample and open source implementations available as well depending on what platform you are working on If you have already done this basic research and have a more specific question please clarify
184,inzinjerstvo,The ESA has a page on compressor blades They give a good dimensioned diagram of an approximate shape; here are some basic dimensions Length 300 mm Width 30 mm Height 70 mm Thickness 5 mm I can't find any complete open source designs (i e high-quality technical engineering drawings) but this is a good approximation
189,inzinjerstvo,Steel is defined as an alloy of iron and carbon; there is no such thing as a non-ferrous steel If you alloy some other metal with carbon it becomes something other than steel Looking for a steel without iron in it would be like looking for brass or bronze without copper You can alloy things other than copper with zinc tin or aluminum but those would not be kinds of brass or bronze As far as other alloys that contain carbon this Wikipedia article has a good list of various kinds of alloys (as you can see there are a lot of them) and searching through it you'll see that there aren't a lot of other things that are alloyed with carbon besides iron As for why this is I don't have a good answer
529,inzinjerstvo,That is correct there are a number of unwanted or tramp metals (Cu Sn Sb As) that enter the recycling stream from for example car bodies that are ground into scrap without removing all the copper wiring or tin-coated steel cans Antimony and arsenic tend to creep in from low-quality and low-cost primary iron sources The answer to the question is no Recycled steel is mixed as evenly as possible from varied sources its composition is measured and then pure iron is added as needed to dilute the tramp metals to tolerable levels for resale or further processing such as meeting a specific steel grade for a specific product or application Stainless steels and other high-alloy grades which are known at recycling time are processed separately due to the value of Ni Cr etc It is currently uneconomical to reprocess iron to remove tramp elements and so it simply isn't done at all Two books mention the process as a regular and economical one ( Minerals Metals and Sustainability Meeting Future Material Needs p 284 starting at dilution ) and ( Steel Production Processes Products and Residuals starting on p 104 read until it isn't relevant anymore) The reason it is uneconomical is that the tramp elements react more weakly with oxygen than iron at constant temperature so to remove them by oxidation would require oxidizing all of the iron first The reason for this is thermodynamic and predicated on the fact that among competing reactions those with the largest decreases in free energy proceed virtually to completion prior to other reactions even starting especially with large differences in free energy among the competing reactions To determine which reactions have the largest decreases an Ellingham diagram may be used In the Ellingham diagram below the horizontal axis is temperature the vertical axis is change in Gibbs free energy The lines running across the diagram at various angles correspond to free energy change caused by element oxidation reactions with oxygen as a function of temperature In our case the diagram may be read by choosing a temperature of interest and reading up from the bottom to find the first element to react with oxygen For example if we have steel with Fe Mn Sn and Cu in it we can see that at 1000K then Mn Fe (to FeO) Sn and Cu are the order of largest to smallest drop in free energy Granted the temperature of interest is closer to 1900K (above the melting point of iron) but the general trends of each Gibbs free energy change function continue to the right on the diagram and iron remains below the tramp elements Cu Sn As and Sb at practical temperatures and likely to their respective boiling points As a result removing tramps from Fe would require oxidizing effectively all of the iron first And because Sn Sb As and Cu are slightly soluble in iron they require separation via chemical reaction One can see the solubility of tramps from their phase diagrams with iron of which I have posted Sb-Fe below The diagram has temperature against composition with
194,inzinjerstvo,Your question sort of has two parts How to supply heat and how to keep it in Large open rooms with a high ceilings are most efficiently warmed with radiant ceiling heat Warm air rises which renders forced-air systems inefficient because the pumped heat ends up at the ceiling and the coldest part of the room is near the floor where you actually want the heat Radiant floor systems are limited to about 87F because they are in contact with occupants and so their peak output may not be enough to keep the space comfortable They also lose more heat to convection than radiant ceilings (See this ref ) As for keeping heat in besides solid insulation/barriers air doors (a k a air curtains) are the standard solution in high traffic passages
206,inzinjerstvo,Here are some other criteria Cycle time Piston speed Piston friction Device lifespan (most likely a function of the number of cycles completed) Number of suction/discharge ports Type of valves used Hope that helps
208,inzinjerstvo,I have written mostly about CFD in this answer however same points should also work for FEA or other simulation techniques CFD is mostly used for design optimization and parametric study of the design Following are a few examples showing how engineers use simulations Selection of a design Read A conceptual study of airfoil performance enhancement using CFD This thesis shows use of CFD for selecting the best design out of a number of candidate designs Engineers often go for simulations to select 'the one' out of many Shape optimization using CFD This paper gives an example of wing shape optimization using CFD And this amazing YouTube video is an excellent example of the way an engineer would use a CFD software ( OpenFOAM ) and genetic algorithm CFD makes it possible to arrive at a better design without actually building a number of prototypes and testing (which is an expensive and long process) Actually design optimization is the most common way the CFD is used According to this survey mechanical design engineers make the use of CFD the most (note I do not know the authenticity of the report) Using simulations where experiments are difficult to carry out / might cost a lot of resources (or life) Applications where experiments are not possible to carry out such as the heat transfer in hypersonic re-entry vehicles ( examples here ) or blood flow in human body can be simulated with a computer and final design can be tested Another example; CFD is used for placement of probes on a wind tunnel model CFD gives for example the position of the stagnation point on a surface of the model and there we can have the pressure probe placed and then test the model in actual wind tunnel This presentation explains how CFD and wind tunnel are complimentary to each other Also CFD is used to predict the results where experimental results are not available (one can not have probes everywhere on the model) Design and optimization of the experimentation facility itself Simulations are commonly used for design of the facility itself For example this report describes how CFD is used for design of the wind tunnel To develop a theoretical model This is often seen in cosmology Scientists carry out simulations based on a model and validate with the experimental data This iterative process results in better understanding of the physics and working of the universe NASA astrophysics group have done some simulation of Supermassive Black Holes this video talks more about it In movies art and animations This question and following answers on Scicomp SE show how much a role CFD has to play in movies and animations (disclaimer I have asked the question) Some other applic
222,inzinjerstvo,One of the problems that plagued older rechargeable batteries (e g Nickel Cadmium ($\text{NiCad}$) and Nickel Metal Hydride ($\text{NiMH}$)) was the memory effect The memory effect occurs when a rechargeable battery is not fully discharged It then forgets that it has a greater capacity than it thinks it has and so in the future it discharges less A good example is a water bottle Initially water bottles have a certain capacity for water Let's say that I drink most of the water in a water bottle during one usage If the memory effect affected water bottles I would not be able to drink any water in the future occupying the space that had held the water that had not been drunken the last time That extra space would be forever lost Over time this can wear down a rechargeable battery Fortunately this generally only affects $\text{NiCad}$ and $\text{NiMH}$ rechargeable batteries I haven't been able to find much about effects that influence only lithium ion batteries but there are a lot of across-the-board factors Here's a short list Chemicals breaking down Passivation (which affects lithium ion batteries) which is when a layer of unwanted chemicals form on the battery cell This discusses a related phenomenon on page 4258 Unfortunately on recharge the lithium has a strong tendency to form mossy deposits and dendrites in the usual liquid organic solvents (cf Figure 15B) This limits the cycle life to 100-150 cycles (considerably lower that the 300 cycles required for a commercial cell) as well as increasing the risk of a safety incident Mechanical stresses and leaking Batteries can be damaged in a variety of ways causing internal components to break and causing chemicals to leak out This can be very dangerous to humans There are other long-term factors that increase battery aging The page I linked for the above list seems to be fond of the Arrhenius equation $$k=Ae^{-E_a/RT}$$ which shows that the rate of chemical reations changes as temperature changes High temperatures mean faster reactions but also possibly a shorter life; this can affect non-rechargeable batteries significantly Finally there's the phenomenon of self-discharge which is when unwanted reactions in the battery eat away so to speak at the battery's capacity The process can differ based on the type of battery Battery University has a page on it which you may have already seen It reiterates that temperature can speed up this process Scarily enough lithium ion batteries may discharge as much as 5% within the first 24 hours slowing down to 1-2% per month after that
235,inzinjerstvo,Generally speaking meeting the EMC/EMI requirements is the EE job The biggest part of radiated EM waves usually comes from a poor PCB and of course there is little the ME can do about that Providing more space if possible can relax some constraints for the EE that will have more space to properly route his/hers tracks In your case it appears to me that no EM problem should be present the fastest signal lying around would pass through the USB connection but you say it's used only for power Your device is also battery power and that gives a huge help since there's no risk to inject some unwanted frequencies in the mains through the power supply If there's some high speed clock inside and I guess there is proper routing should be enough A great help would come from a metallic enclosure as you probably know a grounded metallic enclouser greatly helps to keep unwanted EM from escaping your device Bonus points it also keeps unwanted EM from messing with your circuit that might be a bigger issue To help with the routing relaxing the position of the usb connector and/or the lcd can help but really in this case it seems quite trivial to me to make a circuit that has no problems Without additional informations on the specific device in question I'd say just relax make a prototype and test it you're probably getting away with it without any problem
252,inzinjerstvo,This will at least depend on the Rate of Cooling Magnetic field strength Exact composition The magnetic field will alter the microstructure as you can read in for example Yudong Zhang Nathalie Gey Changshu He Xiang Zhao Liang Zuo Claude Esling High temperature tempering behaviors in a structural steel under high magnetic field Acta Materialia Volume 52 Issue 12 12 July 2004 Pages 3467-3474 ISSN 1359-6454 http //dx doi org/10 1016/j actamat 2004 03 044 G M Ludtka R A Jaramillo R A Kisner D M Nicholson J B Wilgen G Mackiewicz-Ludtka P N Kalu In situ evidence of enhanced transformation kinetics in a medium carbon steel due to a high magnetic field Scripta Materialia Volume 51 Issue 2 July 2004 Pages 171-174 ISSN 1359-6462 http //dx doi org/10 1016/j scriptamat 2004 03 029 ( http //www sciencedirect com/science/article/pii/S1359646204001770 ) For me it is behind a paywall But as you can read in the abstract 30 Tesla will result in for example more ferrite The other paper reveals that for a hyper eutectoid steel you will have particle like cementite I am not aware of any models with which you can make predictions about tensile strength and so on But for the question Can we change steel properties by application of magnetic field while quenching it is a clear yes A more complete lookup in the literature would be the next step for models and experiments for a more specific case
317,inzinjerstvo,Yes Michigan left This page (pdf here ) is very informative though you really have to dig to get what you want In a sub-section of 10 2 2 Median U-Turn Crossover I found this (A Michigan left is referred to as a median U-turn crossover ) A study on a Michigan corridor used simulation to compare median U-turn crossovers with two-way left-turn lanes (TWLTL) The study showed that during peak hours the corridor with median U-turn crossovers had a lower travel time by 17 percent and a 25 percent higher average speed than the same corridor with a TWLTL However vehicles made more stops on the arterial with median U-turn crossovers In nonpeak hours the median U-turn crossovers had the same efficiency as the TWLTL even though a higher delay for left-turning vehicles had been expected due to the higher travel distance a vehicle must cover to turn left using a median crossover So that's a yes for stopping congestion during peak hours More information on that specific simulation can be found under found under footnote 149 which isn't too easy to find Other simulations reportedly found similar results Simulation studies using a range of intersection configurations (number of through lanes on the major and minor street) and volumes from intersections in Virginia and North Carolina suggest a reduction in overall travel time for all movements through the intersection when compared to a conventional intersection -21 to -2 percent during off-peak conditions and -21 to +6 percent during peak conditions The studies also show a general increase in the overall percent of stops when compared to a conventional intersection -20 to +76 percent during off-peak conditions and -2 to +30 percent during peak conditions The rest of 10 2 2 has some more safety information The collision rate is lowered slightly There are less conflict points (i e locations where collisions are likely to happen) Jersey jughandle The Jersey jughandle (referred to as simply a Jughandle ) does reduce conflict points though not as much as a Michigan left It too appears to increase efficiency Simulation studies using a range of intersection configurations (number of through lanes on the major and minor street) and volumes from intersections in Virginia and North Carolina suggest a reduction in overall travel time through the intersection when compared to a conventional intersection -6 to +51 percent during off-peak conditions and +4 to +45 percent during peak conditions The studies also show a large increase in the overall percent of stops when compared to a conventional intersection +15 to +193 percent during off-peak conditions and +19 to +108 percent during peak conditions Is there a clear winner Both clearly reduce travel time and congestion so the answer to your question is a definite yes The Michigan left has many less conflict points (16) than the Jersey Jughandle (26) which I consider quite the advantage (the standard four-way intersection has 32) It also has a lesser increase in stops I'd give the edge here to the Michigan left though both are probably improvements over your standard four-way intersection
220,inzinjerstvo,It isn't completely infeasible Just to get a ROM (rough order of magnitude) let's assume that a typical household that isn't using electricity for heating uses about 1 kW on average and that you'd like to be able to store a half day's energy or 12 kWh which is roughly 45 MJ Commerical air compressors can easily achieve 15 Bar or so (over 200 PSI) The energy in a tank of compressed air is equal to the pressure times the volume so to store 45 MJ at 15 Bar would require $$\frac{45 \text{MJ}}{15 \text{Bar}} \cdot \frac{1 \text{Bar}}{10^5 \text{Pascals}} = 30 \text{m}^3$$ or about 8000 gallons Can you get or build a tank that size that can hold the pressure The real key is what kind of thermodynamic efficiency you can achieve while making the conversion from electricity to pressure and back again When you compress the air it will get hot and some of that heat will be lost to the environment However you can get some of that heat back if you run the expanding gas through a suitable heat exchanger (and get some free air conditioning in the process)
221,inzinjerstvo,According to the ASME Process Piping Code (B31 3) $$p = \frac{2 * t * S * E}{D - 2 * t * Y}$$ where $p$ = internal pressure $t$ = wall thickness $S$ = material's tensile strength $D$ = outer diameter $Y$ = wall thickness coefficient (B31 3-1999 Table 304 1 1) $E$ = material and pipe construction quality factor (B31 3-1999 Table A-1A) Note that this equation does indeed have a safety factor included
225,inzinjerstvo,This actually isn't as much of an engineering question as it is a physiology question There are actually a number of widely used estimates to predict your one-rep maximum aka 1RM if you know how many repetitions you can do at a lower weight See here for more info All of the methods are based on empirical studies and are basically look-up tables of coefficients Brzycki Epley Lander Lombardi Mayhew et al Connor et al Wathen National Strength And Conditioning Association (NSCA) Coefficients
228,inzinjerstvo,A bimetallic strip is used Two different metals like steel and copper expand at different rate and a strip of the two bound together as result bends with temperature changes A contact placed at the end of the strip will close when the temperature is right - and the temperature can be tuned by turning a screw that pushes the strip closer or farther from the contact In simpler systems the natural inertia of the system is used to create the hysteresis - as the refrigerator compressor works it takes time for the temperature drop to reach the bimetallic strip and switch it off so the temperature is brought below the cut-off point by some hard to control factor In more complex systems a two-level switch (either through two strips or just mechanism that shorts two switches at two different temperature levels) in connection with an analog RS switch (usually based on a relay) creates a better-controllable hysteresis As temperature is too high both switches are engaged and so the cooling compressor starts Temperature drops At cut-in temperature the On switch disengages but power to the relay is still supplied through the Off switch At cut-out temperature the Off switch disengages and the cooling process stops With temperature rising the Off switch engages but since the relay is open it doesn't supply power to the coil Further rise causes the On switch to power up the coil and the relay switches both the compressor and the Off switch circuit on With temperature drop the on switch will disconnect but the coil powered through the relay will remember the state until Off opens (and I'm sorry but I don't know about calibrating thermostats with jars of water My vote would be against as they are rarely submersible )
451,inzinjerstvo,In Ye olden days DC generators were brushed commutated devices They had a one or more stator windings and an armature winding Field wound DC generators as well as motors were commonly connected in one of three methods Series Shunt and Compound Without getting into details each had its own set of strengths and weaknesses But you only have to remember these two things the voltage of a DC motor is dependent on its input shaft speed Current is a function of torque More voltage means more RPM's and more amps means more newton-meters (or foot-pounds) So with all that you need a constant speed source to get a constant voltage And you need to ensure you have enough torque to satisfy the current demand of your load otherwise voltage drops off Old automobiles had commutated generators They couldn't regulate the voltage so they used a range of around 10-14 volts and used a relay that simply closed when the engines speed was within the voltage range If the voltage went too low or too high the relay opened Primitive by today's standards The Alternator in today's automobile uses a voltage regulation circuit that varies the armature current which changes the field strength based on the stators output voltage Lower speed means more current to the armature and less current at higher speeds So how different were DC generators from motors Not very different at all If anything they mostly differed in mechanical design as they were to be coupled to a prime mover (steam ICE electric etc ) Though in much larger dynamos they had adjustable commutator brushes to compensate for the shift in the commutation plane as a result of heavy load charastics A hand wheel would turn a worm gear which would advance or retard the commutation plane to bring the generator back into its normal operating parameters You dont need to worry about this as I am sure you motor isn't megawatt capable I am guessing your motor is a permanent magnet type motor Its nameplate RPM is what you need to spin the motor at to get the nameplate voltage This means if you have a 12V motor that spins at 6000 RPM you need 6000 RPM to get 12V If you don't have a constant speed source you have no way to regulate the voltage You would need a buck-boost switching regulator to get a constant voltage from your motor If you are using this for a renewable energy project like wind or hydro a charge controller is usually designed for a wide input voltage swing via a buck/boost regulator Solar panels are a close analogy to a permanent magnet DC generator no internal voltage regulation and a varying amount if input energy Sun might be shining bright one minute and a minute late be blocked by a cloud So the charge controller does its best to make a useful steady-voltage from its varying input From there use storage batteries to capture that power for later use and to act as a buffer for low input events And just for reference an AC motor can also generate power if you spin it faster than its nameplate RPM usually at synchronous speed But again no voltage regulation and a constant speed is needed More trouble than its worth Also of note jet planes use a very elaborate mechanical speed regulator to produce constant shaft speeds which ensures a constant 60 or 400Hz AC frequency as the throttle is varied
304,inzinjerstvo,While some of these answers are close they are (at the time this answer is written) all incorrect to some degree Pressure and stress are very closely related -- in fact one could argue that pressure is in a sense a subset of stress To be specific the pressure in a material is the isotropic part of the total stress in a material Pressure is a scalar quantity -- the same in every direction while stress is a tensor quantity that captures all deforming forces Pressure and stress are related as follows if the components of the stress tensor are given by $\sigma_{ij}$ then the pressure is (using Einstein notation) $$p = -\frac{1}{3}\sigma_{ii}$$ That is to say the pressure is the opposite of the average of the diagonal elements of the stress tensor When speaking more specifically in terms of a boundary condition or an applied load for a structural analysis problem it refers specifically to an applied normal stress over a given area
233,inzinjerstvo,I know would describe those kind of programs as 3D parametric feature-based modeling (just to clarify and distinguish between these and dumb geometry solid modeling or 2D drafting) There are a number of free tools for solid modeling including some parametric feature-based ones FreeCAD is a strong contender really looking to provide functionality to commercial software http //www freecadweb org/wiki/index php title=Feature_list SOLVESPACE might be an option as well http //solvespace com/index pl Other solid modeling (non-parametric) Blender OpenSCAD Sketchup(not open but has free option)
288,inzinjerstvo,Your intuition is correct those are high However you would need to be moving very slowly for wave-making resistance to be negligible And since it is typically higher than skin-friction I don't think that you can realistically expect to have a significant skin friction and a negligible wave-making resistance Perhaps a better simplified approach would be to ignore the skin friction and focus only on the wave-making resistance
242,inzinjerstvo,There are cases where DC power generation is superior to AC If it's going to be transmitted by DC and you can generate at the transmission voltage or you can step up the voltage to transmission levels without too much loss DC is superior If your load is DC and is close to your generator (in space and in voltage) DC is superior Both of these assume you've got suitable DC breakers available electrical systems are usual designed to be fault tolerant to the degree that no single fault can be dangerous to humans This typically means ensuring the system fails to safe and sometimes that requires breaking the current With the development of DC breakers even for HVDC applications this has improved considerably previously it was one area where AC had the advantage in an AC circuit current hits zero twice every cycle making safe circuit-breaking much easier And sometimes you just don't have a choice and the technology (e g photovoltaics) will only generate DC and if you want AC you're going to have to convert it
333,inzinjerstvo,This may not fully answer your question but hopefully it will be a good start I thought a distributed mass model would be a good approach for this so I did some searching and found this paper Real-Time Deformable Soft-Body Simulation using Distributed Mass-Spring Approximations (PDF) I also found this which goes beyond what you need including variable cross-sections and sheer stresses Bars under Torsional Loading A Generalized Beam Theory Approach I think this second one is what you need I included the first one because I can actually understand it whereas the second one is way beyond me If you can simplify out the bits you don't need by substituting in suitable constants it might be what you're looking for
1831,inzinjerstvo,Per ACI 318 13 2 two-way slabs are designed based on column strips and middle strips To paraphrase the code A Column strip is a design strip with a width on each side of a column centerline equal to 0 25L 1 or 0 25L 2 whichever is less Column strip includes beams if any L 1 and L 1 are the span lengths in the two slab directions A Middle strip is a design strip bounded by two column strips This is a simplified method in the code and has some criteria to meet This method then allows you to accommodate holes in the slab by placing the missing reinforcement on the edges of the hole I don't know how this compares to the Hilleborg method
512,inzinjerstvo,The choice of time step sets the bandwidth of the control loop The highest unity gain frequency (UGF) you can hope to achieve in the closed loop is the Nyquist frequency $$ f_N=\frac12 f_s=\frac{1}{2\ \Delta t} $$ where $\Delta t$ is the sample time Practically the UGF will be somewhat lower than this This means that above this frequency your feedback will not be suppressing the disturbance fluctuations in your system The UGF also limits how much gain you can have at frequencies below but near the UGF For frequencies within an order of magnitude of the UGF $\text{UGF}/10$ you won't be able to have a gain much higher than $\sim10$ A gain of $10$ in the closed loop means that disturbance fluctuations at those frequencies are suppressed by a factor of 10 So the choice of operating frequency is a practical one Faster systems are more expensive; slower systems may not provide enough disturbance suppression
276,inzinjerstvo,Water meets the low compressibility requirement but there are many other considerations in the design of a hydraulic system Boiling point/vapor pressure If the system warms up during operation the fluid may boil which results in high compressibility and thus decreased effectiveness of the hydraulic system Hydraulic fluid has a higher boiling point than water to help combat this Related to this is the concept of vapor pressure Hydraulic systems often involve small orifices which can cause cavitation (localized boiling) This cavitation has the same effects as boiling and can cause pitting damage to the components near the cavitated region Hydraulic fluid has a lower vapor pressure which helps here Freezing point It would not be a good thing if your car's brake lines froze every time it got cold outside Most hydraulic fluids have much lower freezing points to prevent this from happening under normal circumstances Oxidation/corrosion Water being an electrolyte will cause rust inside the lines as soon as air inevitably leaks into the system or the system isn't bled properly Water will also exacerbate galvanic corrosion when dissimilar metals are used in the system Lubrication Hydraulic components use seals and often involve sliding interfaces (cylinders and spools for examples) Using an oil as the fluid means the working fluid can also function as a lubricant Organic growth If perfectly distilled water and a closed system could be guaranteed this would be a non-issue But in practice this is never the case Oil-based hydraulic fluids are much less conducive to organic growth than water Water is used in some systems where other considerations trump these (for example some food-grade applications) but for a wide variety of applications oil-based hydraulic fluids are the better choice because of the design considerations above
282,inzinjerstvo,Short Answer YES you can Long answer A) Limits of continuum mechanics The continuum model of fluid dynamics is valid only till the fluid behaves as a continuous medium This is characterized by the Knudsen number The Knudsen number is given by $Kn = \frac{\lambda}{l_s}$ where $\lambda$ is the mean free path and $l_s$ is the characteristic dimension of the channel (diameter in the case of the circular pipe) Non equilibrium effects start to happen if $Kn &gt; 10^{-3}$ Modified slip boundary conditions can be used for $10^{-3} &lt; Kn &lt; 10^{-1}$ and condinuum model completely breaks if $Kn &gt; 1$ ( Fun fact because the distance between two vehicles on a crowded road is much smaller than straight portion of the road itself (length scale in $1d$ flow) we can model the traffic flow with a PDE However it will not work if there is only one car on a long stretch of road) Coming back to water as the water molecules are not freely moving and are loosely bound we consider the lattice spacing $\delta$ for computing $Kn$ For water $\delta$ is about $3 nm$ So continuum theory will hold good for a tube of diameter $300 nm$ or larger $^*$ Now this is a good news $^*$ Reference Liquid flows in microchannels B) Applicability of Hagen Poiseuille equation Since your tube is in sub-millimeters range it is much larger than the minimum diameter required (sub-micrometer)for the continuity equation However depending on the shape of cross section of the tube the results will differ ( Link to ref ) Liquid flows are much simpler to analyse since they are characterized by much smaller Reynold's number and velocities Also density essentially remains constant So there should not be a problem in considering the theory to be valid Now since the Hagen Poiseuille flow is derived from the Navier Stokes equations it follows the assumption of continuity If your flow is through a porous medium you might have to consider effects like electrokinetic effect There might be other complications in straightforward application of H-P equations to microfluidic flows but I am unable to comment since do not know much in this field C) Some examples In a report on microfluidics networking Biral has used the continuum theory for modeling and simulation (in OpenFOAM) of the microfluidic flows Fillips discusses more about the Knudsen number in his paper- Limits of continuum aerodynamics This report clearly mentions that HP equation is applicable even to microfluidic flows This document on PDMS Viscometer gives derivation of HP equation for microfluidic flows Finally here is a You
278,inzinjerstvo,Controlling the pressure with variously sized holes will be a fairly difficult task It may be simpler to run hose with no perforations along the length of the planter and then place the hoses with perforations as branches off of your main line That still won't lead to a perfect distribution but would help Another common approach to this is to use so called 'drip emitters' which are very cheap regulated orifices They are designed to put out a certain volume per time relatively independent of the pressure I don't know if they can be buried
292,inzinjerstvo,I can't give a definite answer since I hardly plan either type So please read this answer as an incomplete list of questions you could ask the sales engineers of your pumps Both can be operated hydraulically or pneumatically (for example if you don't want to bring a motor into an ex-zone or there would be cooling problems) In the case of piston pumps you'd need extra air/oil pistons for that The diaphragm can be operated directly by the fluid so the pump can be simpler So for a sump application you could have a diaphragm pump in your sump powered by a compressor nearby I've seen small and huge piston pumps but I've never seen huge diaphragm pumps (huge= flowrate >10m³/h) I assume some economy of scale here that makes large piston pumps cheaper than comparable diaphragm pumps (But I could be totally wrong) Diaphragm pumps have no seals that touch the medium This is an advantage since you probably need to service them less This also means that they are better for aggressive media or applications where cleanliness is important With any pump you need to know if there are any solids or abrasive stuff in your medium As for your question what specifications should be stated ask your vendor(s) What do you need to know to size and configure a pump
294,inzinjerstvo,From my brief involvement in shocks I think the most likely solution would be to image the exhaust probably optically but maybe using interferometry or something depending on what the exhaust is The most obvious indication that you have supersonic flow is if you can see a shock diamond I think you could probably also work it out from the length of the exhaust but I cannot remember how Alternatively you could also look at the thrust generated You should be able to calculate the expected thrust This is what they do when testing rockets/jet engines as they don't actually care if the flow is supersonic just that it generates enough power The simple way for pipes is to just measure the exit flow Its a pipe so flow should be constant However in practice I suspect long pipes also have regular inspection hatches/areas where they measure the flow somehow to check for leaks/faults
286,inzinjerstvo,Seriously D There are going to be two parts to the solution you are looking for A) Till the steak is in the cannon B) The steak leaves the cannon is into the air and cooking starts A) Internal ballistics You are dealing with compressible flow NEVER use simple Bernaulli's equation beyond Mach no 0 3 Make sure you are using correction terms till Mach number 0 7 and beyond that use equations of gas dynamics (refer Modern Compressible Flow by John Anderson) That said your case is same as an air rifle case Instead of pellet you are shooting steaks So if you know the muzzle velocity you can design your cannon as shown in this paper Now your question is how does one get $P_0$ mentioned in this paper right For that you will have to do reverse calculations B) Steak leaves the cannon Assuming that you want your steak medium (as rare is not recommended apparently ) figure out the internal and surface temperatures for cooking Also time required for cooking At these temperature your steak most probably will be flying at supersonic speeds Then there will be a bow shock in front of the steak You can safely approximate it as a normal shock and use normal shock relations to calculate total temperature ratio across the shock Now $T_{01}$ becomes the atmospheric temperature and $T_{02}$ becomes the surface temperature on the steak (using total pressure ratio and gas dynamics relations) This will give you required shock strength and hence the flying Mach number Assuming STP conditions at sea level find acoustic velocity and hence the steak velocity Now this is average steak velocity But there is going to be wave and pressure drag on the steak all the time Use this Stanford supersonic wing drag calculator to calculate this drag In this take aspect ratio (AR) = 1 $C_L = 0$ put length of steak and its thickness / length as t/c So compute the muzzle velocity by using newton's second and then first law Now substitute this muzzle velocity in point A discussed above That will give you your chamber pressure Also I found one report in which internal ballistics of spring loaded gun is considered There is a matlab code as well You can take author's permission to use it Another issue is as you are going to use pre compressed pneumatic cylinder the temperature is going to drop considerably when expansion happens So flames is not a problem however during compressing of the gas in that cylinder things are going to heat up so using helium is smart move Another way you can do this exercise is to write a small code in your favorite language and carry out those iterations you mentioned However don't use Bernaulli's equation All the best for your paper Speculation If your
291,inzinjerstvo,Distributed DC power is actually used in some new construction It's driven less by the efficiency of the transformer than by other logistics Here in California at least we have a law (known as Title 24) which requires some fairly sophisticated controls of lighting as well as fairly low power consumption per square foot The control requirements include compensating for daylight by automatically dimming lights occupancy sensing and brownout usage reductions This means that the controllers are fairly sophisticated and expensive and that LED light fixtures are sometimes preferred in new construction As a result some systems are being sold with the controllers that output dimmed DC which directly powers the LED fixtures DC distribution reduces the number of wires that have to go to each fixture (DC+ and DC- instead of AC hot neutral and a separate control line) and saves some money on electronics As far as I know though there is still at least one controller per room I'm not aware of any systems that distribute DC all throughout a building I imagine this is because as lengths and currents increase the advantages of high voltage in reducing wire size become more significant
301,inzinjerstvo,You need to control how fast you apply the power Some sort of damper mechanism that would allow power to be gradually applied and not cause it to spin out You could try and wrap the 'power string' over different sized pulleys so that you changed the gear ratio If you used a cone it would be similar to a CVT where starting out you would have a low gear ratio to get moving then as the string unwound down the cone it would propel the car faster rather than just spinning out Edit You can either guess at the gear ratio or use some basic statics to figure out coefficient of friction and how to avoid slip while applying torque
321,inzinjerstvo,I'm going to have to make this an answer If someone feels it's inappropriate I can delete it what follows is the start of my comment Grin I bought my house with an outdoor wood stove we have a love/ hate relationship I love It burns all my waste paper and meat products (Including the treats our cats bring us and the scraps the dogs drag home during deer season ) It heats my garage and preheats hot water (It could heat the house but more wood ) Gathering and processing of wood yes it's work but it's also an excuse to be in my woods which I enjoy I sit on my butt most of the day so wood = exercise That the ashes are all outside I hate When something breaks down it's always on the coldest day of the year (of course that'a not true but plumbing and outside windy below freezing is not fun ) It belches smoke because of poor burning If I leave my house during the winter (say Xmas) then I either have to drain it or have someone come over and feed it Of course some of that is open to engineering I'm leary of overly complicated and I burn a lot of wood on cold winter nights Here's a pic It's cold here tonight (7 F) and it's burning nicely
334,inzinjerstvo,It's important to remember that these pipelines don't exist as a single isolated line and will have a number of branches that tie into the main pipeline and split off for gas to be sold to different locations This touches on a very broad subject of flow assurance and pipeline network modeling The pipeline will also be broken into smaller sections by compressor stations because as you mentioned friction losses will require the gas to be re-compressed as it travels along the pipeline To answer your question the gas flow rate will be regularly measured using a variety of flow meters such as an orifice meter ultrasonic meter Coriolis meter turbine meter etc This will occur at any point were gas flows into the main pipeline or is branched off the be sold It also may occur at regular intervals (i e the compressor stations) along the pipeline If you want to determine the mach number you could simply calculate it from your measured velocity I am not aware of any target mach number or velocity in pipelines it will certainly be subsonic the rest will depend on the specifics of the section of pipe For example the elevation profile the presence of liquids corrosion and the required pressure at the destination all will have an effect on the velocity you can transport the gas
340,inzinjerstvo,I suspect you won't find a definitive answer on the fate of those two particular 747's for the following reasons The Stratolaunch project is likely still actively using the parts from those planes so there's no need to dispose of them yet 747 parts are becoming quite commonplace and their value in the parts market continues to decline So that's a bit of a killjoy answer for which I apologize Aeronautics is a wonderful field for generating compelling romantic ideals that appeal to our dreams The thought of a plane (or two) being used for historic progress and then just being parted out is frankly somewhat depressing But when we look at the project management behind an engineering project like the Stratolaunch the numbers become pretty telling This Bloomberg article details how the 747-400 is increasingly being sold off by various airlines Now ten-year-old passenger 747-400s are worth a record-low $36 million about 10 percent less than similarly aged planes last year according to London-based aviation consultancy Ascend as carriers seek more fuel-efficient models There’s even little interest in converting the passenger jets into ?air freighters because of a slump in air cargo demand Some 48 of the humpbacked passenger 747-400s worldwide have also been placed in storage according to Ascend The onetime “Queen of the Skies” has been shunned in favor of Boeing’s smaller 777 widebody (which has two fewer engines sucking fuel) or Airbus’s mammoth A380 double-decker “There’s not a lot of demand for the 747 ” says Paul Sheridan Ascend’s head of consultancy Asia “They’re mostly being broken up for parts ” And this Aviation Week article highlights how 747-400s are increasingly being sold off for parts as it's more cost-effective to sell them than to continue operating them with airfield space often at a premium airliners get withdrawn re-cycled and chopped up in a much smaller timeframe The airline can recoup more money selling the aircraft than operating it for several more years before it might otherwise have retired Likewise this NPR article piles on the bad news for the 747-400 Boeing has scaled back production to only one and a half new 747s per month Aviation consultant Ernest Arvai expects the company to keep the line running just long enough to replace Air Force One To draw that together We have a number of elements pointing us towards the fact that 747s and their associated parts are depreciating assets that are possibly depreciating faster than traditional valuations would have accounted for Depreciating assets receive proportionately less attention as their value continues to decline Stratolaunch is well funded and is focused on pushing the edges of space exploration technology While they may be concerned about how to wind down operations once they're done I tend to doubt it Nor are they likely worried about maximizing resale value of the 747-400 parts due to their current funding levels I suspect they are more focused on how to solve the problems they set out to tackle than they are on afterwards So we have parts whose value is approaching that of salvage only and a company focused on broader objects without mu
311,inzinjerstvo,A pneumatic tire provides the mechanical decoupling of the tiniest variations in a road surface (the highest freqeuncies) involving a small unsprung mass (the rubber of the tread) and a spring (the air pressure) working against the sprung mass of the wheel and the axle The vehicle's suspension (springs shock absorbers etc ) working between the axle and the frame of the vehicle decouple the larger slower (lower frequency) variations in the road surface In this case the entire tire + wheel + axle assembly is considered the unsprung mass While there are other tire designs that have similar qualities they are generally heavier and much more complex to manufacture At least for now the pneumatic tire remains the most cost-effective way to get the desired overall performance (Not to mention the fact that the entire vehicle service infrastructure is currently set up to deal with pneumatic tires )
315,inzinjerstvo,The blades are indeed made of or tipped with a hard steel/carbide but they also don't contact each other The shredding action is created by the shear forces of the two corotating drums of teeth and this non-contact between the hard points high shear strength and gearing advantages allow it to power through whatever you throw at it https //www youtube com/watch v=aVkTj9VrH4o
320,inzinjerstvo,This gives you the starting point for a return on investment calculation It tells you the value in fixing each issue in terms of how many defects would be prevented The idea is that the curve shows the cumulative percentage of issues vs the total issues I've drawn a red cross-hair at where the curve crosses the 56 8 mark - 56 8 is 80% of the 71 issues This means if you fix the Important issues on the left of that point you will have solved 80% of the problems so they are more valuable The engineer should then estimate the effort/cost for each possible course of action for those important issues on the left Estimates just need to be relative to each other not in any absolute terms You can then divide the value by the cost to get the relative return on investment For example if I estimated to fix the radiator core issue is about 90 units of difficulty/cost to fix the fans is 20 and to fix the thermostat is 15 my ROI calculations would be as follows $ \text{Fix Radiator Core ROI} = \frac{31}{90} = 0 34444 $ $ \text{Fix Faulty Fans ROI} = \frac{20}{20} = 1 $ $ \text{Fix Faulty Thermostat ROI} = \frac{8}{15} = 0 53333 $ This gives a good idea of where to start in order to maximise the value added as quickly as possible (you are maximising the area under the curve if you plotted total value added against time as you work on fixing these issues) As you can see it's not as simple as starting with the first one because it will fix the most issues - this may take a long time due to the effort and in the meantime you could have fixed the simpler fans and thermostat issues The ROI gives you the order to work on these issues fixing the fans gives you the best return then the thermostat then the radiator core You've then fixed 80% of the problems and you can do the same again to work on the other 20% Of course there may be other factors that add to the 'value' component - such as negative press around one issue Likewise the effort estimate may have different weightings for time vs financial costs opportunity costs of using your resources on this as opposed to something else etc All of these factors should be considered in the 'value' and 'cost' components of this ROI calculation It depends how detailed you want to get Note that you might think you could just work out the ROI for all issues including the less important ones but in practice doing the estimates themselves may be quite a lot of work especially if there are lots of issues (this example only has 6) So the Pareto analysis allows you to very quickly work out the most important issues so that you can estimate those first
319,inzinjerstvo,In dry/arid areas a wind catcher tower in conjunction with a qanat is a great way to keep buildings cool The underground water stays cool and cools the air passing over it that is drawn in through the wind catcher However in a humid climate you would want to isolate the incoming warm/moist air from the cool underground water You could pass pipes through a large underground water tank and draw the air through those this may also condense some of the moisture out of the air and you'd need drainage outlets from the air-pipe that come outside of the water tank Obviously you'd use a much better heat exchange method than I drew in this rather crude picture but hopefully it explains the basic idea
330,inzinjerstvo,The PWM solution is better under any aspect plus the chip you linked has a quite high full scale resistance $5\text{k}\Omega$ that will lead to use less steps than available PWM is better because its efficiency is near unity and because it does not require any external IC if the micro can handle the led current which is usually true When you use high power leds their drivers usually have a PWM input anyway and a resistor would be impractical anyway because of the amount of power it should dissipate Digital potentiometers are used to attenuate an analog signal such as a line level audio signal you're trading some power dissipation for linearity Such a signal is essentially a voltage signal little to no current should flow so power dissipation in the resistors is not really a concern When you want to power an LED instead you actually want power to flow into it that's why you say power an LED and a resistor dissipates power that's inevitable I'd just add a series fixed resistor to protect the led or to use the whole PWM scale if that's a concern A standard red led wants some 2V @5-10mA to work properly a 3V3 output is enough to burn it adding a series small resistor in the $200\Omega$ range allows you to set the pwm up to 100% and protects the led in case of an hw/sw fault
399,inzinjerstvo,This is mostly an issue of safety trade-offs and not necessarily technical shortcomings While a WIG vehicle often referred to as a Ground Effect Vehicle (GEV) has improved efficiency it is also forced to fly very low Above around 50 feet (wingspan-dependent) you will not see much in the way of ground effect benefits The problem is that even over water there are numerous obstacles that could be in a WIG vehicle's flight path Commercial aircraft normally fly above 18 000 feet (5 500 meters) so that they stay in Class A airspace This allows them to remain far above obstacles and it also allows air traffic control to deconflict air traffic and prevent dangerous proximity There is no practical way to deconflict like this near the surface of the water where ubiquitous craft such as sailboats would pose deadly peril to a WIG vehicle Furthermore it is very difficult to maneuver at such a low altitude Aircraft can only safely pull upwards from a WIG vehicle flight altitude and that is the least-advantageous direction from an aircraft power perspective Also due to the low altitude an obstacle may not be visible until very shortly before a possible impact; even on a clear day with good visibility As for why hovercraft are more common they are able to (and typically do) move at relatively slow speeds and even stop on the water They can slow down stop and make sharp turns more easily and more safely than a WIG vehicle Due to their relatively slow speed they also typically have more time to react to an emergency situation Finally hovercraft technology is much less complex and much less expensive Reference Aeronautical engineering experience and pilot experience
335,inzinjerstvo,This angle is determined by the lift characteristics of the rotor and the rotational inertia of the whole helicopter You are exactly correct that the angle changes the phase of the feedback For the purpose of discussion let's assume the helicopter has pitched forwards slightly and needs to be corrected backwards also let's assume a single rotor that spins anti-clockwise when viewed from above If we adjust the rotor when it is exactly at the front to give some more lift the time it takes for the additional lift to overcome the rotational inertia of the whole helicopter would mean that the net effect of the lift was applied at some point to the front-left of the helicopter instead of exactly at the front The next correction would then be slightly further around and the next further round again This would cause the helicopter to oscillate in the same manner that a coin does when you knock it over on the table So you pretty much answered it yourself the angle controls the phase offset between the detected error and where the lift begins to be applied Remembering that the net effect of the lift will be as if applied between where the rotor is adjusted and where the rotor is reset (after some time the helicopter has moved to correct the error) Changing the offset angle of the flybar will change the angle at which the correcting force is applied relative to the error The effect of this will be an oscillation in one direction or the other Up to 90 degrees from where it should be the flybar will give negative feedback out of phase so should (at least mathematically) remain in a stable oscillation Beyond 90 degrees from where it should be the flybar will start to give positive feedback instead of negative feedback again out of phase until 180 degrees causing the thing to exponentially spin out of control
344,inzinjerstvo,The turbulence model can make a big difference in your simulation There are many turbulence models around It becomes a tough job to select one out of them There is no perfect turbulence model It all depends on several parameters like Reynold's number whether the flow is separated pressure gradients boundary layer thikness and so on In this answer brief information about a few popular models is given along with pros and cons and potential applications However interested users can see this excellent NASA website and references therein to know more about turbulence modeling A) ONE EQUATION MODEL 1 Spalart-Allmaras This model solves for one additional variable for Spalart-Allmaras viscosity According to a NASA document there are many modifications in this model targeted for specific purposes Pros Less memory intensive Very robust fast convergence Cons Not suitable for separated flow free shear layers decaying turbulence complex internal flows Uses Computations in boundary layers entire flowfield if mild or no separation aerospace and automobile applications for initial computations before going to higher model compressible flow computations Applicability to your case a good candidate for reducing simulation time You can predict the drag fairly well with this model However if you are interested in knowing the flow separation region this model will not give highly accurate results ________________________________________________________________________________ B) TWO-EQUATIONS MODELS $k$-$\epsilon$ turbulence model A general purpose model This model solves for kinetic energy ($k$) and turbulent dissipation ($\epsilon$) The equations for this models can be found at this cfd-online page This model requires wall functions to be computed for the implementation Suitable only for fully turbulent flows Pros simple to implement fast convergence predicts the flows in many practical cases good for external aerodynamics Cons Not suitable for axi-symmetric jets vortex flows and strong separation Very low sensitivity for the adverse pressure gradients difficult to start (need initialization with Spalart-Allmaras) not suitable for near wall applications Uses Suitable for initial iterations good for external flows around complex geometries good for shear layers and free non wall bounded flows Applicability in your case Although this model is good for external bluff body computation it is suitable only for turbulent flows Since the velocities are low flow is going to experience transition from laminar to turbulent (max $Re = 1 98*10^6$ using this calculator ) You might benefit better with a variant like realizable $k$-$\epsilon$ model 2 $k$-$\omega$ turbulence model Solves for $k$ and turbulence frequency $\omega$ Gives better results for near wall flows Predicts transition (although early sometimes) Quite sensitive to the initial gue
341,inzinjerstvo,I think the best (and simplest) way to describe something like this is Bernoulli's equation $$P+\rho gh+\frac12 \rho v^2=constant$$ To use this we're looking only at instantaneous velocity because as the air leaks the pressure will go down We also should assume that the valve is really more of a small hole than anything that fluctuates too much with pressure variations because that complicates it a bit more The constant in the Bernoulli equation gets applied to any point in a continuous stream So what we want to do is pick two points one on either side of the hole and relate those two points using the Bernoulli equation What we'll get will look something like this $$P_{tire}+\rho gh_0+\frac12 \rho v_0^2 =P_{atm} + \rho gh_1 +\frac12 \rho v_1^2$$ In this situation we'll say that any vertical movement of the air is small enough to neglect Also the velocity of the air inside the tire is negligible as well if not in practice than for the case of determining the relationship between pressure and velocity Lastly there's an important distinction between absolute pressure (which is in the equations above) and gauge pressure (which would be what we measure with a tire pressure gauge [go figure]) Gauge pressure is defined as $P_{gage}=P_{abs}-P_{atm}$ Bringing that all together we get the following $$P_{tire gauge}=\frac12 \rho v_1^2$$ The other couple of important points to make with this is that it's valid for inviscid flow (no friction) with constant velocity The first assumption is pretty valid if the pressure differentials are significant the second may not be especially since that starts to drive up the fluid velocity and constant density goes out the window when we get to compressible flows ($Ma&gt;0 3$) Again though for the simple case of examining the nature of the relationship this evaluation should be fine
347,inzinjerstvo,First here's a great answer to a similar question on the physics SE I will attempt to summarize it a bit and tune it to your question but I think that all of your answers are already in that thread if you don't wish to read it from me I don't think that the mesh is tuned to particular frequencies for EM waves It's just cheaper than building walls out of thick sheet metal The thickness of the resulting mesh must be thick enough to attenuate the frequency (see Skin Effect) and the holes in the material must be smaller than the wavelength of signal Overlapping layers of mesh provides the right thickness and the randomness of one layer of mesh on top of another should eliminate the holes for most applicable wavelengths making it a high-pass filter
349,inzinjerstvo,Here's a really (and I mean really ) quick and dirty set of calculations that might give you an idea of the magnitudes of settlement you could be dealing with The settlement potential of the tank location can be determined a number of ways but probably the best thing to do would be a plate load bearing test The test can be run to simulate the range (though not the duration) of loads you are expecting A test like this will give you a spring constant $k$ that represents the modulus of subgrade reaction of the bearing soil (for the tested loading range) However it's a short term test that doesn't take into account creep so the long-term $k$ value will be lower In general a short-term $k$ will run from something like 80pci for a very soft clay to something like 250pci for a very dense sand (caveat this is just from the top of my head without looking anything up) So let's use the worst case scenario here and to take into account creep let's do what geotech engineers do best and slap a 2 5 safety factor on it So we have about a 30pci modulus of subgrade reaction Let's also assume that most of the differential settlement will occur as a result of the uneven loading of the empty tank and that the emptying/filling of the tank is going to have a negligible contribution to the differential settlement This isn't too terrible of an assumption since the difference in applied surface pressure (which determines differential settlement) is much greater in the empty state and it's also conservative because it will only be empty 10% of the time anyway So here we go (I'm American so we're doing everything other than what you gave for dimensions in imperial-scum units first and then converting - sorry ) $k = 30 \frac{lbf}{in^3}$ $\gamma_{concrete}=150\frac{lbf}{ft^3}$ $H_{concrete}=2m$ Applied pressure under half the tank $q_c=H_c\times\gamma_c=0 98ksf=5 3 \frac{tonf}{m^2}$ Settlement under loaded half of tank $S=\frac{q_c}{k} = 0 23in = 5 8mm$ If we assume the other side of the tank does not settle at all our differential settlement comes out to about 6mm Now this number assumes the loaded side of the tank is free to settle while the unloaded side remains static This is not the case Assuming the tank is nice and stiff some of the applied pressure on the loaded side will be transferred to the unloaded side (which will reduce the settlement of the loaded side) I don't know what the application is for this tank but the above is probably a pretty conservative analysis of the situation you described I would be surprised if differential settlement potential turns out to be a problem for you EDIT One thing to note is that the tank will wiggle when it is being filled/drained What I mean is the entire thing will settle more when it is filled but it will settle more in the unloaded side (thereby undoing some of the differential settlement in the empty condition) Then when drained the soil will rebound and the tank will return to the more-tilted empty condition when the unloaded side rebounds more than the loaded side (though it is likely neither side would rebound fully) Assuming the 6mm of settlement from above the deflection angle for the 24m diameter tank comes out to be $\arctan\frac{6mm}{24m}=0 014^{\circ}$ Pretty tiny
355,inzinjerstvo,This is related to the amount of current (Amps) the electrical parts of the fixture can take without overheating and causing damage $$\text{Watts} = \text{Amps} * \text{Volts}$$ So at 120v a 50W bulb will draw $\frac{50}{120}$ (0 416) Amps and a 75W bulb will draw $\frac{75}{120}$ (0 625) Amps All electrical conductors have some resistance this makes them warm up when you pass a current through them The higher the current (more Amps) the faster it will get hot and the hotter it will get There are many factors that determine how hot something can get before it will fail for example how fast it can dissipate heat what temperature the insulation can withstand The rating is there so that you don't put too high a current through the electrical components of the fixture which would cause it to get too hot and melt catch fire etc How hot something will get can be predicted mathematically but ultimately the rating value is determined by testing the fixture in a lab with light bulbs of different powers and measuring how hot it gets The rating is set such that it will always remain at a safe temperature within a safety factor
426,inzinjerstvo,And so as promised I'll do it First assumption Let's work with 1m long strips of gutter It'll be easier to calculate everything starting from here Let's say the gutter is already full of ice (We'll work on the ice filling problem later on) The standard gutter size (according to this site ) is 5-inch K-Style or 6-inch half round If we use the half round version we can learn it holds around 9L of ice The latent heat of fusion of 9L of ice is 3000kJ or 833Wh This means that if you wanted to melt this ice in an hour you'll need 833 Watts of power For each meter of gutter But what's the available solar energy Second assumption Let's say the sun shines all day with the perfect orientation regarding the gutter and the gutter absorb all the energy it receives from the sun Let's assume it's the shortest day of the year at the US mean latitude (around 38°N) According to the third chart the length of day on the shortest day of the year is around 9h30m Let's round this up to 10h (I like round and easy numbers for my ballpark calculations) The cross section of our gutter is around 0 15m² The sun irradiance reaching the ground stands at around 1000W/m² This means around 150W reaches our gutter On a 10h day course that would be 1500Wh Hey that would be enough Well yes if you take your gutter and put in the perfect orientation regarding the sun Which it's not true In this case the energy received will be lower Moreover one also has to take into account the efficiency of the energy conversion High quality solar thermal collector (which collect solar radiation and convert it to heat) typically have efficiency at around 60% This means in our case (where the efficiency will be lower we'll receive at best 900Wh If we take into account the fact that our gutter has a fixed orientation the energy received will be even lower than that Thus we won't have enough energy to melt the ice Given this data I'd say it's not possible As for the ice filling the gutter The problem is still the same Making sure that the water flowing from the roof stays hot enough will still means providing enough energy to keep it above freezing Also usually water melts on the roof but doesn't flow alone i e it brings down some snow and ice in the gutter which you'll have to melt of you want to prevent ice buildup in the gutter
358,inzinjerstvo,Disclaimer - I'm not an expert in batteries but I do some related work with those particular types Lithium-ion batteries are a little different from regular lead acid batteries in that the Li-ion batteries can provide more power for short periods of time than what their nameplate rating would otherwise indicate The trade measurement term for this effect is called C-rate The downside of exceeding a 1C (normal nameplate discharge) rate is that it creates more heat within the battery wears out the battery faster and can only safely be done for short periods of time So the ability to exceed the regular 1C rate and the fact that batteries tend to be exposed to a lot of environmental wear from being banged about leads to problems within the internal connections of the battery This blogpost from The Economist does a good job explaining the thermal cascade that causes the thermal breakdown When the battery is charged lithium ions are driven from the electrolyte into a carbon anode When the battery is discharged they flow back creating a balancing flow of electrons in a circuit that powers the device The trouble comes about if there is a small fault or damage is caused to the extremely thin separators that keep the elements of the battery apart This can lead to an internal short-circuit and a subsequent build-up of heat This can trigger what is known as a “thermal runaway” in which the battery overheats and can burst into flame
368,inzinjerstvo,The experiment can be done during daylight but you need a very tall building overlooking the ocean &amp; having an uninterrupted view of the horizon This video show a group of US students doing the experiment You Tube - Measuring the Radius of the Earth To get an accurate measurement you will need to be able to view the horizon unimpeded have reasonably accurate way to measure vertical angle a theodolite would be ideal but not practical with younger kids know the height of the viewing platform The main reason why an unimpeded view of the horizon is required is for accuracy the longer the target distance the more of the Earth's curvature is being considered Also the higher the building the further the viewable horizon The other thing is to decide which point on the horizon to measure to consistently One of the problems will be weather and air clarity particularly heat shimmer haze absence of fog etc The guys in the video had an error of 78% with their measurement due to air clarity using a thick point marker and not having very accurate angle measuring equipment The third option you list could be done during a school camp by the sea but I don't know that is practical or even possible in your situation
387,inzinjerstvo,To begin with I am assuming each of your horizontal surfaces desktop and the three selves are each made the same material To use a ridiculous and exaggerated case the left half if the desktop is not heavy marble and the right side is not light weight balsa wood The desktop is composed of one uniform material and each self is made of its own uniform material wood glass metal particle board and laminex plywood whatever As shown each of the shelves and the desktop are independently attached to the vertical supports that acts as the legs Hence the weight of each horizontal surface is directly transferred to the vertical supports All horizontal surfaces are of uniform materials that have a uniform weight distribution Consequently each leg is carrying half of the combined weight of all the horizontal surfaces The section of each leg that experiences the full weight of what is above it is the short section between the two triangular braces the one for the desktop and the other for the stand/foot of the desk The stress in each of these short sections of leg will be the weight carried by each leg divided the cross sectional area of the leg in the z-plane (breadth by width of the leg) The sloping part of the desktop brace will carry some of the weight of the desktop Whereas the short vertical part of the desktop brace carries all the weight of the vertical support above the desktop the three shelves and some of the weight of the desktop What the proportion of the desktop weight carried will be will depend on the perpendicular (normal) distance from the vertical support Likewise at the base of the leg the triangular brace will redistribute the load in the foot according to your triangular configuration This is just a general overview of how to think about things relating to your design As @Rick Teachey states you really need to do a course in statics get numbers for weights and cross-sectional dimensions of supports and plug it all into some formulae
374,inzinjerstvo,The first thing to remember is that the naming of eras such as the Stone Age or the Bronze Age is never done by those living during the period It was always done by others much later To a certain degree the reason why bronze was the first important alloy was luck For whatever reason design or mistake someone at some stage during antiquity mixed copper and tin in a furnace and bronze was produced Prior to the use of bronze copper was used Copper being a soft metal became blunt very quick when used in tools and needed to be sharpened at frequent intervals Additionally copper corrodes easy compared to bronze When bronze proved superior to copper copper was abandoned as the metal of choice for tools During the time of the Pharaohs in Egypt copper was for tool making The Bronze Age was from about 3300 to about 800 BCE The first bronze made was arsenic bronze When tin was discovered it replaced arsenic as the alloying metal Tin bronze was better than arsenic bronze because the alloying process was more easily controlled and the resulting alloy was stronger and easier to cast Bronze became important because It was a strong metal It was easy to cast It was easy to sharpen It maintained its sharp edge for a long time Weapons that maintained their sharp edges were very useful in battles Likewise for non military uses such as knives and chisels It is resistant to salt water corrosion making it useful for fittings in boats and ships It has a high resistant to corrosion and has fatigue endurance It does not oxidise beyond the surface It was useful as armour in ancient times It was fashioned into tiles for building construction It has a relatively low casting temperature When struck against a hard surface it does not generate sparks It's heavy usage in ornamental work cannot be ignored Statues and ornaments were important in ancient times and bronze was easy to produce and cast In short there was a need for metals and bronze was available Iron only started being used when the tin trade was disrupted Steel wasn't invented until much later and until it was iron was very soft it corroded easily and it was not as useful as bronze
379,inzinjerstvo,There is heat that can be recaptured but you won't get much of it away As one of the commenters has mentioned your absolute maximum is the Carnot efficiency $$\eta_{Carnot}=1-\frac{T_c}{T_h}$$ This is an idealized condition you'll never reach this efficiency But to find our limit let's figure it out anyway $T_c$ will just be room temperature it might be slightly warmer inside the tower but we'll give ourselves the benefit of the doubt and pick a nice round number at 20C (293K) $T_h$ will vary as the GPU works harder (this is one of the issues with this design in general; the power you get from the cooling system won't be consistent because GPU temperature varies depending on how much you're stressing the chip ) We don't want to run it too hot and damage the card that defeats the purpose of a cooling system After some quick searching (Google GPU operating temperatures you'll see a bunch of forum posts that give a lot of different numbers none of which I think are strong enough to cite but I'm pooling their data to make my own assumption) it looks like most cards have a strong upper limit of ~100C before you start to do serious damage However running that hot will still reduce the life of your card and judging by the picture in the question this is a nice card for which we've paid a pretty penny and we want to keep it around as long as we can 70C is a good place to shoot for but 80C (353K) is still probably pretty safe and we want our best possible case With those numbers we get $$\eta_{Carnot}=1-\frac{293K}{353K}=0 17$$ This means that at the very maximum the best we can do is to get 17% of the heat we're generating on the card back as electricity to power something in the tower We can vary the card temperature and as it goes between 60C and 100C efficiency goes between 12% and 21% Regardless we're not getting a lot back That's the max efficiency though This site which sells thermoelectric generators says that the top of the line TEGs will run at 8% efficiency While this is better than the nothing that we'd have been getting before the real issue here is cost and implementation TEGs are not cheap and cooling fans are A basic cooling system is also much easier to install Even if we can hook up a TEG to cool the card we have to find something we can do with that electricity and we don't want the variable power to be used for an critical components Tower lights and extra fans are probably the extent of our usage So to answer your actual question in there I'm sure we can find all kinds of creative ways to get that heat converted into electrical or mechanical work Making it useful is an entirely different story
381,inzinjerstvo,I'm going to answer my own question because just before posting it I found an answer The Otto Lilienthal Museum has a comprehensive list of Lilienthal's designs One is listed as the small wing-flapping machine It didn't use a propeller (or jet engine of course ) but instead used a small engine weighing about 22 pounds when fully fueled Its wingspan was 22 feet which was quite impressive Wikipedia lists it as Small Ornithopter Another page calls it the kleiner Schlagflügelapparat which Google Translate translates to small flapping wing apparatus Here's a picture of Lilienthal with it pre-flight
385,inzinjerstvo,Some reasons why noise reduction in vehicle cabins is a standard feature yet As @Trevor Archibald states safety is a very good reason There is still a need to hear some noises from outside the vehicle such as the sirens of emergency vehicles police ambulance fire fighters truck Hearing car horns from other drivers is still needed The sound of the engine lets people know if the engine is performing as it should It's an added cost some people may not want to pay People haven't asked for it Most people don't object to hearing some noise as long as it's not intrusive Insulating vehicle cabins against noise by using sound proofing materials has suited most peoples needs until now It has been introduced in a small number of cars Auto Makers Shush Cars but these are a bit more expensive than the average car See also Cars Go Quiet Bose Noise Cancelling in Cars However introducing electronic noise reduction technology in cars could reduce the weight of cars by reducing the amount of sound proofing materials used Harman Quiets the Car On a different angle in the 1980s electronic noise cancelling had been used to cancel the engine noise made by heavy vehicles used in inner urban development site to reduce the amount of noise heard by nearby residents
1738,inzinjerstvo,There are good reasons why although most of your links are from several years ago very little has come of any of them The economics and the engineering just aren't favourable Part of the art of an engine is dumping the heat as quickly as possible; furthermore heat is a low-quality form of energy so once your energy is heat you've already lost all your best opportunities to get more work done with it Let's look at those two things in detail getting rid of the heat quickly is the name of the game The exhaust is designed to take heat and the products of combustion away from the engine The radiator does a similar job of removing and dissipating the heat Dumping heat quickly is crucial in a heat engine as the efficiency depends on the temperature of the cold reservoir and the delta to the higher temperature So anything you put in the way such as your proposed energy harvesting device will slow the rate at which heat leaves the engine This not only reduces efficiency (it seems bizarre that you've excluded efficiency as a consideration why else would you look at energy harvesting anyway ) It will also raise the equilibrium temperature of the car's working parts shortening its life If fuel is that valuable that energy harvesting looks attractive then it's worth making the car more efficient in the first place so that there was less waste heat first reduce the consumption of high-value energy before trying to recycle low-value energy And that brings me to energy versus exergy Heat is in a large proportion of cases a waste product It's almost always the least useful form of energy That's really what the Carnot efficiency limit is telling you that to get any work out of low grade heat you can only do so with very low efficiency; that is almost all of the heat will stay as heat When doing engineering with heat and other forms of energy it's very useful to build up an intuition to distinguish between energy (the thing measured in joules) and exergy (the thing that gets work done) The form that energy is in determines how much work it can do Chemical energy such as that in fuel can do huge amounts of work efficiently - it has very high exergy But the same amount of energy as heat can do much less work - it has very low exergy Internal combustion engines are so inefficient because they take a high-grade form of energy (chemical) and immediately convert into a low-grade one (heat) The humble alternator is very good at its job Solid-state thermo-electric generators simply don't come close not in cost not in performance Using the relative rotation of a conductor and a magnet is a very effective means of turning heat into electricity that's why it's used in power stations cars bike dynamos and many other uses So although energy harvesting looks clever it's bad engineering its trying to fix one symptom rather than address the underlying cause If you want more work out of those joules then get that work done before those joules are in the form of heat
391,inzinjerstvo,TL;DR It depends but probably not You might argue that all 15 samples are within spec so the supplier should be approved Not so fast depending on the parameters of your full production run the 15 samples may or may not be statistically significant There are numerous calculators online to do these calculations; I used this one A good resource for the actual formulas is online at NIST's Engineering Statistics Handbook or in any undergraduate Statistics textbook From the spec we know that our confidence interval is 10% (two-sided) This means that any parts within 10% of nominal are acceptable Next we need to determine the confidence level This is usually 95% but sometimes 99% and represents how sure we can be of the result The final piece of information we can give if it's known is the population size In this scenario this is the total number of parts to be ordered from the vendor over the lifetime of the product/process Most calculators allow this to be left blank and if missing assume a large value because the effect of the population size decreases as it grows in relation to the sample size Assuming a 95% confidence level and our 10% confidence interval with population left blank we need a sample size of 96 parts in order to have a statistically significant result Increasing to a confidence level of 99% requires a sample size of 166 parts So for industry-standard confidence levels we cannot conclude that the vendor be approved based on the initial sample of 15 parts alone Wait a minute—the entire FAIR run was within spec Well yes but what's to say the next 15 won't be out of spec We don't know—that's why we have statistics -) Well under what circumstances is our sample significant Just for illustration I entered population values until I found a statistically-significant result at $n=15$ for 95% confidence level $n=15$ samples would be significant only if the population (total production run) is 18 parts For a confidence level of 99% the situation is even worse significance only if the production run is 16 parts Other Notes The calculations above assume that the process follows a normal distribution and that the sample is representative of the population In practice both of these assumptions may be inaccurate
396,inzinjerstvo,As others stated before induction loops are the primary - most reliable method the coils (usually just several loops of wire) embedded in the road; fed given frequency from a generator in presence of metal the frequency of the LC circuit changes and the sensor circuitry detects the change of frequency producing a presence signal In some cases these may fail to detect bicycles but they are by far most common as they aren't affected by weather (or more precisely the detection circuit tunes in to slow changes of frequency caused by weather) and are immune to accidental false positives Note the loops can be localized (~2m size) or cover a lengthy part of a lane Detection is performed by cards like these and by induction loops made with wire laid in grooves like these or placed in pipes under the road surface at construction time (in the photo is a loop for tram detection but pre-built loops are similar) Videodetection - cameras connecting to a specialized card with detection zones defined through specialized software detect the vehicles They are vulnerable to bad weather and tend to produce false positives from glare of car headlights shadows of vehicles on neighbor lane and such but in certain cases - primarily where road surface makes installing detection loops impossible (gravel or bad road surface) they are preferred Additionally the video detection cards are significantly more expensive than cards for detection loops There are a few lesser used techniques like geomagnetic (detecting changes in magnetic field; These largely depend on size of the vehicle so a large truck can trigger a sensor in neighbor lane - but they are more durable) radar (detect only moving vehicles * - but are frequently used to detect pedestrians as they rarely stay immobile) laser (measuring distance to road surface; vehicle in the way changes the distance measured Quite reliable but only point-detection no area detection) Pictured below is a geomagnetic sensor and a radar sensors (short range for pedestrians and bicycles and long range for cars) I heard of pneumatic and piezzoelectric but I've never seen these in use for traffic control - probably problems of wear and durability; I know these are used for automated barriers for parking lots but they obviously support an order of magnitude lower traffic For city transport traffic the vehicles are equipped with an on-board computer with a short-range radio (up to 500m) and GPS and they broadcast messages about entering pre-defined checkpoints to the traffic system alongside with data about intended turn direction delay against schedule and some others allowing the controller to prioritize An alternative is a system that feeds vehicle position to a central unit which then contacts controllers with messages about prioritizing these vehicles Last but not least cameras/sensors detecting strobe lights of specific frequency give immediate priority to oncoming emergency vehicles (and take a photo of the vehicle in question to prevent abuse ) Controllers can communicate with each other and share t
398,inzinjerstvo,Chronic animal experiment This is not the only stage of evaluation of the material but it's a mandatory stage Chronic means that the animal is survived after implantation for a period of time and observed Afterwards the animal is sacrificed and autopsied Humans have a strong immune system compared to other animals So you would want to use an animal that also has a strong immune system Pigs become likely candidates 1 Dogs too Indeed since the immune response and repair functions in the body are so complicated it is not adequate to describe the biocompatibility of a single material in relation to a single cell type or tissue Sometimes one hears of biocompatibility testing that is a large battery of in vitro test that is used in accordance with ISO 10993 (or other similar standards) to determine if a certain material (or rather biomedical product) is biocompatible These tests do not determine the biocompatibility of a material but they constitute an important step towards the animal testing and finally clinical trials that will determine the biocompatibility of the material in a given application and thus medical devices such as implants or drug delivery devices [ exceprt from wiki ] 1 Pig's also weigh roughly the same as humans 2 Nick worked on several projects developing electronics for surgical devices As a part of my duties I assisted with a handful of animal experiments However those devices weren't of an implanted sort Biocompatibility was not one of my direct responsibilities; the mechanical design team was responsible for it An actual biocompatibility expert can provide this question deeper justice I'm writing this answer only as a fallback
415,inzinjerstvo,Buy a depth finder As @SF pointed out you can use an ultrasonic device to measure the depth of the water These devices work by sending out an acoustic impulse and recording the return signals The acoustic impulse reflects off of any density boundaries in the medium By measuring the time for the signal to come back the distance from the transducer can be inferred by knowing the acoustic speed in the medium If the sediment is very dense then you will only get a signal from the sediment boundary but if the sediment is not so dense (like muck on the bottom of a lake) then you will get a signal from both the sediment boundary and the bottom of the tank You might think that this sounds like an expensive device to design and build but they are mass produced for boaters Almost every lake-going boat I've ever been on has one They range in price from \$50 to \$350 on Amazon depending on features and accuracy
407,inzinjerstvo,One is training to be an engineer and one is training to be a technician The best way to see the differences are to look at the course curriculum for each Purdue BS EE Plan of Study Purdue BS EET Plan of Study Purdue BS ME Plan of Study Purdue BS MET Plan of Study both are able to become PEs This depends on your local rules for becoming a PE Also note that being a PE isn't required for a lot of ME &amp; EE engineering jobs in the USA because they fall under industrial exemption Canada and other countries where Engineer is a protected title have different rules I was hoping to get answers from the point of view from an engineer who has hired people with these backgrounds I haven't directly hired both but I've interviewed both and personally the question comes down to what is needed To me it's like asking if you should go to a Dentist (DDS) or a Medical Doctor (MD) they both share the word doctor but they are different jobs with different training If I need a technician I wouldn't hire an engineer If I needed an engineer I wouldn't hire a technician Like all jobs and majors the longer you are out the less it matters as knowledge learned on the job overshadows schooling So 5-10 years down the road both could have the potential to end up in the same spot Anecdotally engineers make more than technicians but it still comes down to company and location Edit I want to point out that I'm using technician/technologist interchangeably here I've heard it both ways but I know some areas may be separate jobs The wiki on technologist is pretty good And don't pick your answer based on which one makes more It's irrelevant since I know a lot of people that have tried to force their way through one way or the other and ended up hating their education/jobs because of it I would explore both and see which one you like better
418,inzinjerstvo,Standard threads are classified for accuracy by a tolerance class (You can see a bit about the metric thread fit classes at http //www amesweb info/Screws/IsoMetricScrewThread aspx ) The screws you find at your local hardware store will probably be a relatively rough tolerance class meaning that the threads are designed to have some gap between them and also that there will be a large variation in the dimensions of each individual bolt or nut Unfortunately for your question the requirements of the tolerance classes are about the diameter of the crests and roots (and therefore truncation) of each thread but not about the accuracy of the pitch (how many threads are in any given unit length ) So there isn't a great theoretical way to predict the backlash for a thread that wasn't really designed for use as a leadscrew I think your estimate of 1?m is pretty optimistic though For point of reference a well maintained manual milling machine might have as little as 001 (~25?m) and that's using a specifically designed leadscrew with a low-backlash design A less pristine machine could easily have as much as 010 (~ 25 mm) of backlash If your design is using a smaller diameter leadscrew there will be some improvement but the tolerances designed in standard threadforms will make it worse So your backlash would probably be on the order of magnitude of 1mm - 1mm for common leadscrew sizes (with 10mm being at the lower end of that range ) There are strategies for dealing with backlash including a 'split nut' where two separate nuts are coupled together and with either spring tension or fine adjustment forced into contact with opposite edges of the bolt thread The other strategy is to always approach the target from the same side If this is an option for you it is the simplest way to solve the problem of backlash without any materials cost
432,inzinjerstvo,You build the delay into your system [This first requires your system to be repeatable If your delay varies then this method won't work ] You would collect lab data and map all of your actuators and their respective delays with the hydraulic line length etc Say you have 4 hydraulic actuators and the results show the following delays Act1 30 ms Act2 50 ms Act3 200 ms Act4 1 ms If you wanted Act3 and Act4 to run at the 'same time' you would artificially add a 199ms delay to Act4 The timing of the events would look like this &lt;0ms Determine what you want to do Move Act3 and Act4 by a certain amount 0 ms Send the Act3 signal 199 ms Send the Act4 signal 200 ms Act3 and Act4 move at the same time It is similar to how moder compression ignition fuel injectors are calibrated Each actuator has a slightly different delay based on multiple environmental factors By mapping out the different delays you can repeatably inject fuel when required
419,inzinjerstvo,It is called a steam wand gasket Technically it's a ball-and-socket joint made out of two concave PTFE (Teflon) gaskets and an EDPM gasket for the seal There may be other designs too; I learned this by watching Saeco StarBucks Barista - How to Replace Steam Wand Seals and Gasket (YouTube video)
422,inzinjerstvo,The testing of ventilation systems and of building air-tightness is done using smoke sticks or similar - non-toxic smoke generators that produce no ash residue One of these may be suitable for your needs Smoke generators typically use mixes of two or more of alcohol glycol glycerol water I suppose strictly speaking they're fog generators rather than smoke generators There are a wide variety of parameters to chose from There are fast-dispersing mixes and lingering ones There are generators that run off batteries and some that run off mains power Some generate high flows some low flows Some start generating within seconds of being switched on others take a few minutes before they start producing the fog (smoke) (Drager Flow Check - source as above)
434,inzinjerstvo,You are looking to build a sewer robot so you maybe want to look at other sewer robots I have no diret experience with designing or handling sewer robots but I can talk about the ones I saw at trade fairs Some builds have sets of 3 or 4 wheels with a mechanism that pushes the weels against the pipe walls or - more typically - sets of 2 wheels In the first case the wheels are quite small In the latter case the wheels are sometimes shaped so that when looking at the robot from the front the cross section is more or less circular Find a handful of pictures of the seconfd type here The first type with an X shape you can see here I think it's far larger than 4 Not that this is not a classical sewer robot but for some pipe building application that I don't understand One small robot I have seen had a small pivotable arm at the front with a light source and a camera At a branching in the pipes the robot could push the arm into one of the branches When the robot moved forward it would be slide into this branch because of the arm I've never seen tracked robots that would fit into a 4 pipe - again don't take my word for it Note Most hits for the term sewer robot where on german sites Maybe the actual english term is different and only germans insist on calling them sewer robots
424,inzinjerstvo,This is a constant acceleration problem on an inclined plane which ignores friction The force acting on the truck is the force of gravity The acceleration the truck will experience will be solely due to gravity $9 8\text{ m/s}^2$ Using the 38 degrees for the slope you have to find the component of gravity/acceleration going down the slope Given the truck will travel 100 m down the slope and the acceleration in the plane of the slope that you calculated you have to rearrange the following equation to get the time required to travel the distance $$s=ut+0 5at^2$$ The initial velocity $u$ will be zero if the truck starts rolling from a standing start To find the final velocity of the truck you use the following equation $$v^2=u^2+2as$$
1811,inzinjerstvo,I would suggest a Dremel-like cutting wheel positioned at the front which can be rotated around 360? such that the wheel stays parallel to the inner surface of the pipe The small diameter of the cutting wheel will mean you can cut quite close without damaging the pipe This is assuming you will have a camera and light on the front to see where to orient the cutter Another approach would be to have a hole-cutter type blade on the front mounted just inside another slightly larger cylinder to prevent the blade from hitting the inside surface of the pipe as it moves when biting into roots
2016,fitness,The main difference is in the purity how much lactose and fat is left with the protein after filtering Whey isolate usually contains around 90% protein and whey concentrate is more like 70-85% If you have trouble digesting the lactose or are trying to minimize carbohydrate content then whey isolate would be a good choice Otherwise it probably doesn't matter; just pick the concentrate since it's cheaper in terms of protein grams/dollar
2039,fitness,I'd say this depends on your fitness the intensity of the workout and perhaps how much you've eaten the night before Assuming jogging means a running speed of about ~10 km/h or ~6 mph I expect you don't use that much energy that you can't cope with without jogging More importantly low intensity workouts mainly burn through fat and anything you would eat either isn't absorbed yet or only serves you energy for a couple of minutes which won't help you during 'longer' workouts Off course if you start to feel faint at the end of the workout you could take some glucose with you just to recharge when you need it So my advise go running and enjoy your breakfast after the workout Resources Skip breakfast before exercise to burn more fat studies say The Benefits of Exercising Before Breakfast
2019,fitness,Body weight squats will help with leg strength and balance Bicycle crunches and situps for oblique and core Pushups or elevated pushups for triceps I would avoid excessive bicep curls as big biceps will decrease your punching speed
2363,fitness,Get outdoors Because you're sitting inside all day it's great to get some fresh air I'd recommend either going jogging/running or cycling because they're very accessible and you can do it practically everywhere The big advantage of jogging is that it burns through calories like crazy and you're using a lot of different muscles If you have a pool nearby I would also recommend swimming During swimming you're using your arms in a totally different way than when working behind the desk It also strengthens your arm and shoulder muscles which hopefully let's you bear the stresses of working with stretched out arms all day Especially focusing on backstroke is good because when sitting behind the desk you're 'exercising'/using your breast muscles more so training the back muscles helps balance that Furthermore swimming is like a master-combo of all the arms movements anti-RSI campaigns make you do If your commute isn't too long you could cycle commute Ride your bike from home to work in the morning and from work to home at the end of the day This is easiest in the summer when the days are longer and there's less rain but possible all year If there's a shower available at or near work you can cycle long and hard and then clean up but if you go a bit slower you should be able to do it in regular clothes and just change your shirt and fix your hair when you get to work If you're depending on exercise to get around it's harder to skip it Doing a combination of multiple workouts every other day keeps it fun because you're not constantly doing the same It takes quite some discipline to do workouts with weights every day and if you're coming from being a couch potato it's all the more important your doing something fun Go join a team or set yourself goals to keep you motivated because while it's great that you've decided to want to get started you need to find something you want to keep doing as well Take it easy If you haven't done any exercise for quite some time chances are your fitness is very low Doing too much too fast can cause injuries Furthermore working out very hard with a poor physical condition is almost a guarantee of having terrible muscle soreness the next day If you have to endure that for several weeks not very motivating Instead of rushing to get fitter as quickly as possible focus on getting in better shape gradually You'll be able to workout much longer or harder in a couple of months if you rush it you'll probably not be able to work out at all
2070,fitness,Having guided a group of 'Start-to-Runners' ( similar to Couch to 5K ) I'm convinced anyone can get started with running (except those who can't walk properly) A couple of things you should keep in mind Don't overdo yourself because if you get injured you can't run at all and you'll lose any progress you've made Also you'll get less muscle soreness which makes working out out on a regular basis easier to maintain Besides speed is of no relevance when building up your fitness so it's literally a waste of energy Try running at a speed that you can still talk because your breathing frequency (and thus ability to speak) and your VO2Max (your capacity for exercise) are very strongly related The speed at which you can still speak is mostly a speed at which you're performing aerobic exercise If you run any faster you start to create more CO 2 than your body can get rid off which is what you want to avoid Try to run with a buddy because you have somebody to talk to during running and someone who can keep you motivated when things start to get tougher (and vice versa) Especially when running with someone of a similar level can be very motivating Why does this all matter Well you say you can walk for miles with no problem but your lungs start burning when you exercise So you need to do two things improve your general fitness so your lung capacity increases and your body is stressed less during exercise and find a sweet spot at which you're not walking but your lungs aren't burning either Generally people think you have to go fast to get a good workout but they're wrong Even jogging at 8-9 km/h can greatly improve your fitness when done regularly so take it easy and enjoy yourself
2025,fitness,I've had lots of success of RunKeeper Pro It comes with some preset training programs and you can customize them yourself Granted the programs themselves are only for single running sessions as opposed to a long term plan There are also training programs (Fitness Classes) available through the site though you have to buy them However you can set them up yourself as well RunKeeper Pro is available for free on both the iPhone and Android Which uses GPS to track your running speed and gives you feedback during your running session The results are automatically uploaded to their website When you login to their site you can see reports of your progress/activity over time There are also options to add reports for other workouts like swimming or rowing and also upload heart rate reports from your Polar to get a more complete overview of your workouts There's also RunKeeper Elite (19$/year) that offers access to more advanced features
2090,fitness,Morning is better because It's the first thing you do so no excuses as to no time You get it done Your metabolism increases for the rest of the day burning a few more calories You get energized and your muscles are stretched so you're more apt to take stairs walk a little further etc Save an extra shower
2059,fitness,You will find a lot of advice on the Internet about this one most of it is not demonstrated This is what the British Journal of Sports Medicine says about it Recent research suggests that the timing of the intake of protein related to exercise may be more important than the total amount of protein consumed in a day In the case of resistance training an intake of approximately 20–25 g of a high quality protein source in the hour after exercise appears to produce the maximum rate of protein synthesis So the total amount of protein consumed is not important the timing is But this amount is much more smaller what the supplementation guys say (to view the full article linked above you would have to subscribe for 30 days free)
2071,fitness,You should not need to empty your bowels prior to exercise If you are experiencing discomfort during abdominal exercise you should make sure your diet contains enough fiber to allow you to go regularly The Mayo Clinic has a nice list of high-fiber foods you are not taking excessive diuretics you are not taking in foods that cause excess gas or bloating About com has a list of foods known to cause excess gas and bloating If you are following all of those rules and still experiencing this problem regularly then you should consider talking to your doctor about it
2035,fitness,I think this depends on what your ultimate goal is If your goal is to be able to lift as much weight as possible bulk first If your goal is look like an underwear model probably cut first If your goal is general health/something in between I seen no reason why you can't just start working out without making any radical changes first and let your body gain muscle/lose fat as it will
2212,fitness,I prefer running outdoors for several reasons It can keep your mind somewhat more engaged than when using a treadmill (i e taking in the scenery etc ) (and possibly keep it away from the fact that you're exercising) You get to go somewhere and see things other than the inside of a gym As Greg mentioned the natural benefits of fresh air and sunshine are a huge plus and should not be underrated You can establish routes using bike paths neighborhood/city blocks hills etc to track progress as well as to switch things up to keep it interesting You're less likely to quit You can easily hop off a treadmill but once you've jogged in one direction you have to jog back ;)
2043,fitness,Looking in the mirror is a pretty good way I also got a great motivation boost when I received much more attention from ladies after losing 10kg
2085,fitness,You're question lacks a little bit of detail about what your workouts looked like but general causes for exercise related lower leg injuries are Running too fast puts massive strain of the soft tissue around your shin bone The only way to cope with this is slowly built up your exercise to allow the tissue to strengthen Run at a speed that you're still able to speak is a nice rule of thumb for not running too fast Running on hard surfaces if you're like me forces to run in the suburbs your feet hit the pavement hard with each step This causes the same problem as my previous point it will just happen faster on hard undergrounds If you can go to the woods or a park Running with bad shoes while the barefoot running crowd may disagree running on old or plain wrong shoes is bad Old shoes have degraded EVA which has bad or inconsistent cushioning Using your aerobics or tennis shoes for running is also a bad idea because while they may look similar they are really built completely different and often of different material as well Getting a pair of good running shoes will go a long way of reducing your chances of injury If all else fails you could get a pair of orthotics (don't overpay ) or compression stockings The first helps reduce movement of your foot in the shoe or make slight corrections to your feet roll off pattern The latter keeps your calf muscle in place and thereby reduces the stress on the soft tissue a lot The last part I can really recommend get a training plan I'm sure you've tried one before but this time get a training plan and cut your goals by 20% So instead of aiming to run at 10 km/h start out at 8 km/h instead of trying to run for half an hour start with 20 minutes Also I can really recommend a program like Start to Run or your C25K this forces you to alternate between walking and jogging for at least the first few weeks This greatly reduces the total stress on your soft tissue and should give your body time to start adapting But remember even when things start to get better stick to your plan
3045,fitness,It depends on the circuit you do You could alternate a circuit such that you don't do the same muscle group on consecutive days if you like working out every day If you are working your muscles hard you definitely want a 48-hour recovery period for each muscle group I did a pretty intense circuit program 3 days a week (M-W-F) so I could have weekends free And if you do it right you sure appreciate those days off But sometimes due to scheduling I've done circuits consecutively So long as you know your limits and keep good form you can avoid injury You can always do another activity for days in between I would suggest if you worked legs the day prior to not run a really long distance the next day A lower level of intensity like a leisurely walk swim or bike ride is nice but it's all relative to your activity level Listen to your body
2243,fitness,You should get yourself a decent heart-rate monitor and calibrate it yourself (as good as possible) What you do is you take a cycling home trainer and you follow the following protocol Maintain 80 rpm all the time Start cycling for 3-5 minutes at 100 Watt Add 30 Watt every 3 minutes Monitor your heart rate Assuming everything is more or less accurate your heart rate should be increasing more or less linearly until you start get above your anaerobic threshold after which it may start to increase steeper The point is it gives you a nice estimation of how much Watts you burn at what heart rate And since Watts can be roughly translated to kilo-calories (1 kcal = 1 163 Watt) you get an idea of how much calories you've burned Convert your heart rate over time to Watts and convert those to calories Though I'm sure most of them will have one built-in You can use your own 'calibration' to adjust the values If your a heart patient or you have any other diseases your mileage may vary
2110,fitness,The general rule of thumb is to add no more than 10% (measured in either time or distance) per week If you're just getting started running I would recommend the Couch to 5k program I used this last year to start running (after having never done any running in the past) and completed my first marathon in December
2130,fitness,Dr Stuart McGill suggests everybody should be able to hold the plank position for two minutes Once you are comfortable with this I vary my routine with elevated planks single leg planks stability ball planks side planks elevated side planks planks with feet on wall planks with feet on wall raising one leg to chest alternatively (I find this very tough)
2095,fitness,My girl friend and I used Your Shape (Xbox360 with Kinect) and in her case the calories indicated by the game were actually too low compared to what her Polar was indicating Now by no means is this scientific or calibrated but at least it means they're a nice estimate I'd say there's three things that are important Does the game make you keep coming back Because in the end any diet is based on sticking to it so if the workouts are fun and keep you motivated that's a plus Does it make you workout regularly Not only do you have to keep working out you have to do it at least ~three times to be a little bit effective Especially if you want to get better at something you have to regularly stress your body and let it recover so you become more fit Does it make you workout longer Your body needs about 30 minutes to get your fat-burning going so if you worked out for 15 minutes and then quit well then you only burned the sugar you shouldn't be eating ;-) If you can find a workout that keeps you busy for 30-60 minutes then it's becoming really effective Note that you may include warming up and cooling down as long as it keeps you moving And off course Kronos is absolutely right if you eat more than you burn during the workouts you don't loose anything But as your fitness improves so will your metabolism so it's OK-ish to eat a little bit more but then you have to promise me that you'll do an extra two sets of Wii Tennis This image shows different games and their metabolic equivalent task (MET)
2117,fitness,Sadly I don't have access to the journal to verify but Copacabana Runners is citing the journals Medicine and Science in Sports &amp; Exercise and Sports Medicine that reviewed over 60 detraining studies They found the following physiological effects after 2-4 weeks of detraining VO2 max down 4-10% Blood volume down 5-10% Heart rate up 5-10% Stroke volume down 6-12% Flexibility Decreases Lactate threshold Decreases Muscle glycogen levels down 20-30% Aerobic enzyme activity Decreases Running economy Unchanged So basically your body gets hit across the board Of course your mileage may vary depending on your level of fitness (the more you have the more you have to lose) the amount of inactivity you had (bed rest vs just no training) and most likely your age (the older you are the harder it is to regain what is lost)
2075,fitness,The bar should actually sit on the ridge formed by your shoulder blade whenever you grab the bar For instance see the following image for an approximation of where it should sit You really shouldn't have to be pushing forward much with your hands to keep the bar in place Also are you using any kind of padding on the bar Try to put padding on the bar if you aren't or take it off if you are I always preferred no padding as it makes the bar sit oddly on my shoulder blades Unless you are really super concerned with how much you can squat perhaps consider switching to nothing but front squats for your primary lower body lift instead of a back squat Here are some advantages of doing front squats
2078,fitness,This is not true Your body will process the food that you ingest and if the caloric intake of the food is greater than the amount of calories burned during the ensuing exercise you will gain weight You can have some carbohydrates before your workout if you find that your diet doesn't supply you with enough energy to not feel weak or excessively tired during your workout You can also add protein before your workout if you find yourself incredibly hungry in the middle of your exercise Again though if you take in more than you burn you will gain weight
2093,fitness,The most comprehensive answer I've found to this is at BodyBuilding com Essentially standard creatine won't make you retain water like micronized creatine and the standard creatine is used continuously whereas the micronized creatine should be used in a load-maintain-unload cycle The lack of water retention will change the look of the muscle you are building to be less soft if you're using standard creatine as well as lowering your weight/size
2107,fitness,Itching/tingling indicates an increase in bloodflow due to expanded capillaries and arteries This problem should go away as you increase your level of physical fitness If the itching comes with a rash though it's more likely to be exercise-induced anaphylaxis (an allergic reaction)
2106,fitness,Here's a few links to some great body-weight exercises that I use regularly Beginner Body Weight Workout Advanced Body Weight Workout Prison Workout
2941,fitness,It is possible to train the human body into needing much less sleep It is sleep quality and not amount that counts I have heard about people from military elite educations who make their body used to no more than 4 hours of sleep at night And they are still fit and surprisingly energized the next day ready to continue their rough training http //en wikipedia org/wiki/Polyphasic_sleep http //studenttavern com/2008/04/live-on-2-hours-of-sleep-a-night-sleep-method/ http //www physorg com/news76867739 html First of all it all depends on ones own body Some need more some need less sleep to reach the same level of rest Then it also depends on your sleep quality You've got to fall a sleep fast and immediately go to the deep and good sleep and get quickly past the REM sleep (Rapid eye movement sleep) which is the more superficial dream sleep Then your body also has to be able to recover fast This follows your psysical condition level Lastly remember to eat healthy and have the right amounts of protein etc after training for the body to have perfect conditions for recovering fast If your body ever shows signs on burnout you should react This could be simply having sore muscles the day after the exercise if you do this exercise of these muscle groups regularly but with no soreness If you find yourself loosing focus and concentration and feeling tired then of course you need sleep In the end without training to need less sleep some people just appears to need less anyway without any effort It varies from approx 7-9 hours in average If you are the 6-hours kind of guy this could be just you and you should find out by about this Give it ago and keep focus on soreness and pain in the very early stages
2105,fitness,Evidence shows that more than 5 days a week training increases your risk of musculoskeletal injury Rest is physically necessary so that the muscles can repair rebuild and strengthen -- continuous training can actually weaken it Without sufficient time to repair the body will continue to breakdown from intensive exercise Overtraining often occurs from a lack of recovery time Some signs of overtraining are feeling of general malaise staleness depression decreased sports performance and increased risk of injury This image from pponline co uk explains why you need some rest Making sure to workout when you're muscles are slightly stronger than before your workout will allow you to literally built up your muscles
2101,fitness,Work into it slowly If you're experiencing that many problems from it often then you are likely training too hard I've known 70 year olds who could easily out-do me in the martial arts without breaking a sweat or getting injured It has far less to do with your age than with your experience and endurance
2166,fitness,Disclaimer I've worked for a company that makes pressure plates that allow you to analyze running patterns (with or without shoes) I've done research at a specialized running shoe shop and my father is an orthopedic shoe manufacturer So in short my opinions are fairly biased but by no means scientifically proven While md5sum names some great rules of thumb they apply to every shoe and don't help you determine what type of shoe you're looking for First a short note Adidas did a study way back in 1987 and found that most injuries come from Injuries caused by training Wrong intensity of training 12% Too rapid increase in intensity 9% Several others 9% Injuries caused by shoes Bad quality of shoes 17% Wearing your shoes for too long 11% Wrong kind of shoes 44% As you can see picking the right kind of shoes is pretty darn important So how do we pick one When you dissect a shoe it's made up out of two main components the upper which is the mesh leather or synthetic material + the shoe laces; the sole of the shoe which can consist out of several layers The first component is composed the same for nearly every pair of running shoe While the last around which it is made may or may not fit your foot ( see md5sum's answer for some excellent advise there ) you won't find much variety within a certain brand However you should note that this is mostly true for the running shoes Don't even consider running on those cheaper aerobic or tennis models because you'll fall in the trap Adidas found earlier The other component which is the sole of the shoe Now when I say sole I'm actually referring to everything that is below your foot when you're inside the shoe Trying to discuss all the bits and pieces that compose it depends strongly on the brand and their design-department When limiting choices towards endurance running and excluding fancier shoes that are meant for trail running or sprinting on a track You're left with a shoe sole that serves several functions offer stability and control of motion shock absorption and/or reduction protection against the underground and overuse/strain comfort enhancing your performance Shock absorption is mainly determined by your body weight rather than the type of shoe When you're lighter you can get away with having a 'softer' (as in lower shore values) soles than when you're heavier When I say shock absorption don't immediately think Nike Air because most absorption comes from flexing your joints rather than compressing your sole There are several 'special' features for motion control though you can safely assume that unless you have some severe disorder you probably don't need any of the extreme models With extreme I mean shoes with extreme angles at the heel or other gimmicks If you're 'normal' you fall in one of three groups those who pronate too much those who run neutral those who don't pronate enough (or supinate) Don't mistake this for the other three types people like to call out flat normal arched and high arched feet Hate to break it to them but since the shoes all h
2135,fitness,When you're walking you apply about 1 2 times your body weight to the ground in Newtons (Fz = 1500N in your case) When you start running this rapidly increases to two times or more Furthermore when you're walking you have bipedal phases which means your body weight is carried by both legs But jogging is characterized by going from bipedal to unipedal so all the force is being applied to the one leg and thus one knee You're perfectly capable of walking so what you are looking for is a speed at which the forces on your knees are higher than during walking but not as high as during running What's also important is to reduce the moment arm your bodies center of mass has towards your knee Because the muscles around your knee have to stabilize the joint the further you put your foot (and thus knee) away from your center of mass the larger forces they will exert on your knee Now the single most common injury in runners is a jumpers knee or patellofemoral pain syndrome You can bet that the tendons connected to your patella won't like the strains when you start to run faster So we want two things lower speeds and shorter steps Luckily with shorter steps and the same step frequency you automagically get lower speeds Now how do we know we're actually doing this Well one great way is to listen to your body or in other words talk to yourself out loud If you're able to talk normally you're walking at the right speed Because as soon as you start speeding up the words don't come out steady anymore You get what I mean I can't check it with a heart rate monitor but I'd bet it would be around 60% of your maximal heart rate which is ideally for burning fat If you're more serious about it you can get a heart rate monitor yourself and an accelerometer (like a step counter only fancier) and keep track of yourself but really nothing beats listening to your yourself
2123,fitness,Simply put; no Diet is 80% of the battle when it comes to weight loss Focus on this before any other area Also you cannot 'spot-reduce' fat Push-ups are a useful exercise but they should not be your only exercise Too many push-ups (and little else) will lead to posture issues Consider adding squats pull-ups planks etc - these are compound exercises that'll hit more than one area
2127,fitness,Your muscles increase in size during their recovery not during the exercise itself Therefore you should not be working muscle groups on consecutive days I generally find 2-3 times a week is ideal on non-consecutive days
2138,fitness,Reiterating Your body alone determines where fat is taken from or added You cannot target an area for fat loss or gain In 10 days 10 push-ups a day will gain you 100 push-ups worth of tricep and pectoral strength but will have no bearing on where your body decides to take fat from I've seen extremely obese people lose weight only on one leg for a month before starting to even out and lose in other parts of the body It's rare but it happens Eating cereals and milk only will not give your body all the nutrients you need and you'll likely encounter health problems Eating more calories than you burn will cause you to gain weight Likewise eating less calories than you burn is the only (healthy) way to lose weight
2223,fitness,Among many powerlifters the theory is that you should try to warm up very little Warmups are simply tests to spot joint or technique problems that could cause injury at full weight Anything more is needlessly tiring yourself out before you can get to the heavy lifts that really increase strength Intuitively you would expect to lift the most weight in your first set since you are the freshest the next most weight in your second set and so on Also intuitively if you were to apply your lifts to the real world you may not get to warm up If you must combine cardio and strength training into one workout then do the strength training first as you need to be fresh for heavy lifts and not for cardio Note I'm assuming that you are lifting for strength and doing cardio for heart health If you're training to become e g a competitive runner then this may not be the right advice I think I would save stretching for the end of your workout unless you are very tight and don't overdo it
2141,fitness,Yes it can cause lower back pain groin pain abdomen pain and other pains eHow has an article with some very good exercises to loosen the muscle and relieve much of the pain It walks you through the Side Plank Leg Lifts Ball Squats Lunges Hamstring Curls the Ball Raise
2145,fitness,You cannot target an area of the body for fat loss The body will determine where to add or remove fat Just stick with your routine and make sure that you burn more calories than you take in every day and you will be certain to eventually lose the gut It just takes time
2938,fitness,Well it's not outdated to be sure I'm an Exercise Science major at Ball State U and I hear that you should not sit down immediately after intense exercise I'm not exactly sure of the mechanism but if I remember right it is because of the way venous blood returns to the heart Muscle contraction is a large mover of blood back to the heart by pushing blood past one-way valves located throughout your veins (not arteries) For example when you finish working out your legs if you sit down you won't be using your leg muscles so you won't be pumping as much blood from your veins back to the heart If not enough blood gets back to the heart voila your blood pressure drops and you can pass out If you ever feel like you are about to pass out from anything cross your legs and squeeze your thighs and butt muscles It quickly returns blood to your heart allowing you to maintain blood pressure to the brain I promise it works
2143,fitness,Eating carbohydrates (sugars and complex carbs not fibers) will prevent you from getting light-headed weak or tired during exercise Often a single piece of candy or a cookie is enough to restore and maintain energy Play around and determine the amount that works best for you but don't take more than you need or you'll lose the weight-loss/maintenance benefits of the exercise
2147,fitness,With work you can improve your static-passive flexibility at any age However the gains you will see in time will significantly decrease as your age increases the older you get the harder it is to gain flexibility Stating that it's impossible would be entirely incorrect though See the How Aging Effects Flexibility section of Stretching and Flexibility - Flexibility by Brad Appleton for some of the scientific reasons behind this
2173,fitness,For many people full feeling is usually triggered by fat rather than by the mechanical stuffing of the stomach Red meats tend to be higher in fats than poultry and fish adults are advised to consume on average no more than 70g of red meat per day Poultry and fish are not red meat Pork sheep and beef are the most common red meats that you will encounter in a modern western diet If you are concerned about getting enough protein one could add tofu or tempeh to one's diet although tofu can be high in fat
2159,fitness,Try rolling over onto your stomach and doing a superman You can hold tension in the position or do reps similar to doing reverse situps ref site Also you might think about doing slow squats or wall sits where you focus on contracting both your abs and lower back Tense up so as to make the line between anus and belly-button as short as possible during this exercise - maybe you can replace your ab work at the same time
2377,fitness,If you are actually overweight meaning that your body considers itself to have excess fat then you can restrict calories significantly as long as you maintain a healthy lifestyle of exercise and can do so without becoming exhausted or excessively hungry Some people can reduce intake into the low hundreds (300-500) or even stop eating completely for a period of time and stay active and are healthy while others have to stay above 1000-1200 to maintain an active lifestyle Personally when I was on a weight loss plan I was able to go for a period of time eating 3-4 small meals (~500 calories per meal) each week This only lasted for a few weeks and during that time I lost an average of a pound each day During that time I walked 4-5 miles every day and remained almost as active as normal with only very slight changes Your body is intelligent enough to start burning the stuff it needs least first and will not start to burn needed/used muscle until you are down to your lowest healthy body fat percentage If you sit on the couch and don't eat your body will consider muscle fair game since it isn't being used Calorie reduction will also cause dehydration and the further you reduce the more severe the dehydration becomes Be certain to drink all the pure plain unaltered water (I stress pure water) that you need Drink water any time you feel thirsty Talk to your doctor before you decide to do any diet especially any form of extreme calorie reduction Some doctors refuse to recommend extreme calorie reduction for patients while others are fine with it but at least ask about the risk specifically presented to your health as your doctor will know your health history best Be absolutely certain that you do not continue a calorie restrictive diet past the point of reaching your minimum healthy weight or you will begin burning muscle The muscle you will burn includes your internal organs such as your heart Taking muscle from your organs is extremely hazardous to your health and can lead to long term or permanent damage
2167,fitness,Do something that doesn't require strength in the traditional sense (i e moving large weights) For example with yoga I often found myself shaking and occasionally falling flat on my face even though the exercise didn't seem hard or difficult at first Try holding the posture for 10+ seconds and it's a different story I also found that on average in my class women were doing better than men so this is probably a good thing for what you're looking for
2163,fitness,Typically a good carb indicates that the food has nutrients and is absorbed into the bloodstream slowly Whole Fibers husks and other materials provide fiber (which limits an insulin spike) and vitamins (e g Choose brown rice over white rice) Unrefined Refining processes typically remove nutrients In addition they tend to render foods such that they are absorbed faster (e g Avoid regular white flour in favor of stone-ground or better yet avoid flour altogether) Complex Natural foods containing complex carbohydrates instead of sugars or starches have fiber and other material that helps slow the absorption into the bloodstream (e g Choose beans over potatoes) Low-Glycemic The glycemic index (and the related glycemic load) is a measurement of how quickly foods are absorbed into the bloodstream Low-glycemic is typically defined as foods with an index no higher than 30-50 (normalized against table sugar at 100)
2215,fitness,There are many factors that contribute lack of energy but nutrition-wise one of the reasons is fast carbohydrates (especially sugar) Carbonated soft drinks are usually the worst offenders After you consume sugar it immediately ends up in your blood so your blood sugar level rises rapidly Your body counters it by producing more insulin to maintain stable sugar level Since it has to produce a lot of it ( insulin spike ) your blood sugar level plummets leading to loss of energy and the feeling of tiredness Cut out sugar and sweets out of your diet or at least limit it significantly
2171,fitness,I would suggest you look at solving your craving by eating fruits which are naturally sweet and not an unhealthy food You can vary this by eating different types of seasonal fruits or try and find your favourite I love to eat watermelon and rockmelon (canteloupe) after a workout If you like the flavour of chocolate not necessarily the sweetness you can try something like carob bars which have a similar taste
2388,fitness,Any time you perform an activity that causes joint pain or soreness you're doing long-term cumulative damage to your joints Typically soccer (football) should not cause knee joint pain There are a few different things that can cause this Incorrect or badly fitted footwear Try some different shoes Incorrect technique/balance/weight If you're overweight and running or if you're not moving with some fluidity (running smoothly vs pounding your feet) you'll hurt your knees Weak or previously injured joints Some people have weaker joints due to genetics or poor nutrition Others have weakened joints due to prior injury Typically damage done to joints especially to the knees is permanent and cumulative Every minor injury adds up even some of the ones you never feel If the pain persists check with your doctor and see what he recommends There are some great products now for people who have weakened joints to help alleviate any pain from activity
2326,fitness,I think that inherently there's nothing bad about minimal shoes but as the saying goes if the shoe fits wear it The shoes aren't for everyone so while it may be great for some they can be harmful to others A shoe like the Nike Free was designed to be used together with regular running shoes to mimic barefoot walking on a grass field as an additional workout Studies from the university of Cologne showed that the strength in certain muscles (especially the one used for bending the large toe) increased after training with the Nike Free But you can imagine what happens when somebody starts using them all the time rather than complementary to normal running shoes Instead of increasing strength you're far more likely to overuse those muscles instead I agree with a lot of the arguments that are being made in the advantage of these shoes but I would personally only recommend them to more seasoned runners and people without overweight or excessive pronation The first group has the technique and the fitness to cope with additional stresses that might arise from wearing them The other two points mainly exclude two high risk factors for getting injuries in the first place so I wouldn't recommend them to experiment Perhaps everyone could wear them but then I would advise them to start wearing gradually and preferably as much as possible The advantage of this is that you give your body the time to adapt and strengthen the muscles to deal with running without 'support' The main advantage of this off course would be that overall stronger muscles make you more resistant to injuries so that's always a good thing
2307,fitness,I head to NIH if I want non biased supplementation information The problem is that it almost always has to do in the context of disease so their write-up seems to be effective in the context of Osteoarthritis Osteoarthritis Most research on glucosamine sulfate has measured its effectiveness on osteoarthritis of the knee However there is some evidence that it might also help osteoarthritis of the hip or spine Some research suggests that glucosamine reduces pain of osteoarthritis in the knee about as well as the over-the-counter pain reliever acetaminophen (Tylenol) It also seems to reduce pain about as much as the nonsteroidal anti-inflammatory drugs (NSAIDs) ibuprofen (Motrin Advil) and piroxicam (Feldene) But there is a difference between glucosamine sulfate and these drugs in the time it takes to reduce pain The NSAIDs such as Motrin Advil and Feldene relieve symptoms and reduce pain usually within about 2 weeks but the glucosamine sulfate takes about 4-8 weeks Glucosamine sulfate does not seem to decrease pain in everyone who takes it Some people get no benefit Some research shows that glucosamine sulfate might not work very well for people with more severe long-standing osteoarthritis or for people who are older or heavier In addition to relieving pain glucosamine sulfate might also slow the breakdown of joints in people with osteoarthritis who take it long-term Some researchers hope that glucosamine sulfate might keep osteoarthritis from getting worse as quickly as it otherwise might There is some evidence that people who take glucosamine sulfate might be less likely to need total knee replacement surgery When I want to know things related with sports I use the British Journal of Sports Medicine keep in mind most of the content is behind a paywall Fortunately this study on glucosamine is not glucosamine supplementation can provide some degree of pain relief and improved function in persons who experience regular knee pain which may be caused by prior cartilage injury and/or osteoarthritis The trends in the results also suggest that at a dosage of 2000 mg per day the majority of improvements are present after eight weeks
3858,fitness,I'm going to answer your question in the way that you requested in categories not a list of examples Metabolism Boosters Spicy Foods - Jalapeno Habanero and Cayenne Peppers Protein Meat - Lean Beef Pork Chicken and Turkey Protein Dairy - Milk Yogurt Protein Fish - Salmon Tuna and Sardines Protein Eggs - Chicken eggs Caffeinated Beverages - Green Tea and Coffee Whole Grains - Brown Rice Oatmeal and Quinoa Dark Leafy Greens - Spinach Bok Choy and Broccoli Low-sugar Fruits - Apples Pears Berries Oranges Kiwis and Peaches (organic preferred) Resources/Links Fifteen foods that fire up your metabolism 15 fat-burning foods 4 food groups that can boost your metabolism Foods that Speed Metabolism
2564,fitness,HFCS is usually 55% fructose and 42% glucose while sucrose is 50% glucose and 50% fructose molecule - this is why HFCS is slightly sweeter Glucose can be used as energy in every cell in the body while fructose like ethanol must be metabolized in the liver Fructose in nature is fairly rare (small quantities in fruits packaged in lots of fiber) so it is not surprising that the liver is not prepared to process the large quantities of fructose in our current western diet While HFCS is not in itself worse than sugar its low cost (made from subsidized corn instead of highly taxed sugar canes) makes it very attractive for food manufacturers to put HFCS into everything - soda milk sauces bread you name it So instead of ingesting a few grams of fructose many people on a western diet now ingest several ounces and that depending on who you ask is bad very bad or the single leading cause of the obesity epidemic Other sweeteners are not sugars; if they're harmful they're harmful in completely different incomparable ways For more information about the metabolism of sugar see Sugar The Bitter Truth
2238,fitness,1 kilo fat equals 7000 kcal or 1 pound fat equals 3500 kcal so if you keep your diet the same and start working out more that's how it takes to loose a kilo So assuming your 2 workouts burn 300 kcal together you need 7000/300 = 23 3 workouts to loose a kilo That would mean you would have to train nearly every day of the month to loose it Clearly that's not really an optimistic goal to work towards though on the other hand it's easier to keep up since you don't have to eat less (but not more ) and the result is probably more sustained than with some crash diet Off course increasing the intensity or the duration will chance those numbers but adding more 'exercise' is a more effective way of increasing the amount of calories burned in a day cycle to work take the stairs take a stroll during lunch etc Your other option would be to consume 200 kcal's less each day which would also result in a 1 kilo difference So doing them both would probably give you quicker results Funny enough if you add those two you get the 500 kcal that @Campbell mentioned so I guess his estimate seems about right
2939,fitness,This might be tough to hear but you should also consider the fact that not everyone is gifted with a body that can handle pro-athlete levels of activity If your legs are constantly tired/heavy and you see no improvement (and ESPECIALLY if you see backwards progress) I would say your body is telling you it's not ready for that level of activity (at least not yet) Back it off a bit let your legs recover and try easing into things I'm not saying you shouldn't push yourself but at the same time there is such thing as overtraining and at that point all the time you put into your training is counterproductive and wasted Stubbornness can be the enemy of progress sometimes
2217,fitness,If you're trying to build muscle and lose fat I've heard it's about 1 gram per 1 pound of body weight
2240,fitness,You lose weight because you either eat less or exercise more So it will depend on a combination of those two how fast you would lose weight Since 1 kg fat gives your body 7000 kcal of energy 7000kcal/14 days = 500 kcal/day It depends on your activity level your job and lifestyle whether that's a healthy goal The purpose of any diet is not just to lose but also keep off any weight you lose So rushing yourself only to swing back a couple of months later won't do you any good So what can you do Get more exercise but here exercise means working out at 50-60% of your maximal heart rate examples Take the stairs at work cycle to your work or for grocery shopping get a dog and walk it each day take a stroll through the park at lunch anything that will keep you moving On top of that like the other mentioned pick a sport and set yourself a goal I prefer running because you'll be burning lots of calories from the start and it will only get more once you're able to run longer While jogging for 20 minutes may seem like a bad exercise you'll be surprised how long you'll be able to run three or six months from now If you can run 5 km every other day you'll be burning at least 5000 kcal a month so you'll keep losing weight as long as you keep it up My take home message The goal is not to lose a kilo every 2 weeks now but to permanently keep them off for the rest of your life You're working out to improve your fitness so you'll be able to lose weight once you're fit enough And perhaps more importantly there's a strong correlation between your capacity for exercise and your general health So you should be working out regardless of your weight losing weight in the process just makes it more worthwhile
2231,fitness,Alcohol contains 7 calories per gram ~70 per fluid ounce and ~100 per jigger 1 It is converted in the liver to acetate which is given priority in metabolism in the same way that carbs &amp; protein are given priority over fat 2 As long as you properly count the calories in the alcohol you drink you should be able to work it in to your diet
2222,fitness,I would suggest the rule of thumb here is a dangerous guide Why because it may work for one or two iterations but by the third iteration if you have added 60lbs to your squats in 3 months it may be way too much This is something you need to listen to what your body is telling you and also a trainer
2639,fitness,Absolutely yes I used to be incredibly skinny but changed my workout completely on the advice of a personal trainer I used to work hard on biceps with zero results I changed my workout to focus on the major and largest muscles of the body e g legs and chest and my arms grew in size dramatically and that is after doing zero biceps curls or anything directly targeting the arms Exercises like Dead Lifts Machine Leg Press Military Press etc are key to building great arms
2224,fitness,Box Jumping is one type of Plyometrics exercise that is based off high impact jumping routines Plank is a type of AB workout that focus on holding a position with your back skyward elbows on the floor supporting the body while keeping your back straight maintaining a straight line from your shoulders to the hips to the knee while tip toeing While you are at the Plank make sure to keep breathing In fact keep a consistant breathing pattern while doing any exercise Keep your stomach butt and leg muscles tight during the exercise
2235,fitness,Get enough sleep and eat well and healthy Make sure you get enough vitamins whether from diet or supplements Assuming that you are eating enough make sure that the nutrients you are eating don't go to waste Try taking some enzymes for a while see how that works for you Finally train Lift some weights Be patient results will be slow (think years) Don't rush to big weights and creatine and other helpers Start slow and develop your body carefully avoid injury As long as you're motivated and you keep working at it you will see results Always remember that you want to grow muscles not fat
2263,fitness,To lose weight burn more calories than you eat This can be by working out more or eating less To gain muscle you need to challenge your muscles This is most often done by lifting heavy weights You also need a calorie surplus in order to grow To improve stamina do the thing that requires stamina longer or more often If you interpret lose weight as lose fat then there's no reason you can't do all three things at once I don't agree that there are types of workouts just different goals Someone that just wants to lose fat might just run on a treadmill but that doesn't mean that lifting weights wouldn't be equally (or even more) effective at that goal
2239,fitness,Your best bet is to practice doing sprints the 100m shouldn't be too long a distance You can start smaller and practice doing 50m sprints and then build up You should also make sure you get a decent amount of sleep the two nights before the event Because you will be doing sprints during training and before the event I suggest you do a reasonable amount of warming up and stretches There is also another exercise where you sprint about 10 - 15 metres touch the ground then sprint back another 10 - 15 metres and do several repetitions So you are practising taking off from a point
2259,fitness,I haven't seen any scientific proof as to whether you should or shouldn't work out straight out of bed While you might be a little stiff at first simply strolling for five minutes should be more than enough to get everything going Sitting indoors for one hour won't nearly stress your body as much as just walking a little bit So as long as you start the workout a little relaxed there's probably nothing bad that could happen to you As for pulling a muscle or straining your back listen to your body If it's start complaining slow down and take it easy nobody said you should be pushing yourself and there's zero benefit to getting injured I agree with @Eelvex that making sure you drank something before you go (or taking something with you) is important because you probably didn't drink anything for the past 6-8 hours For the rest you're body will adjust itself quickly enough when you start walking I have no experience with weight lifting in the morning but I reckon the same applies there start with something light to get the blood flow going and increase the loads from there
2252,fitness,No keep resting would be the commonest medical answer for this (acute bronchitus) My view is given all the hours of training you've put in it's better to complete the marathon slightly slower than to escalate the condition in the cold air and not complete it at all
2311,fitness,To understand what anaerobic means let me explain a couple of things about where your energy comes from When at rest most of your energy comes from fat But as soon as you start to exercise your metabolism kicks in and increases proportionally to your rate of work But because burning fat happens too slow and requires a lot of oxygen (CO2) to burn you gradually start to burn glycogen (CHO) instead The relation between these two is pretty much inverse which means that the harder you start working the less fat and the more glycogen you start burning All the while your heart rate and oxygen uptake increase pretty much linearly This phase is called the aerobic phase ( with oxygen ) because you have sufficient oxygen to burn both the fat and the glycogen Until you start reaching a point where your cardiovascular (heart + vessels) or respiratory (lungs) system are starting to lag behind the required lung ventilation This point is called your lactate threshold and indicates that you're starting to accumulate too much lactate in your blood Lactate is the by product of anaerobic glycolysis because you burned the glycogen with insufficient oxygen A second thing that happens is that your red blood cells get saturated with carbon dioxide (CO2) it can take up less oxygen to supply the muscles with 'fresh air' The phase above the lactate threshold is the anaerboic phase If you keep increasing the load from this point on your body is starting to saturate and you're rapidly depleting your glycogen resources Unless you're a highly trained athlete these levels of exercise (80-90% of your maximal oxygen uptake) are generally hard to sustain for long periods of time But as you can see a lot of factors are involved in determining your where your aerobic phase starts Your heart has to be able to increase its heart rate and increase the blood pressure your lungs have to ventilate old air out and new air in and transfer O2/CO2 with the blood blood has to transfer O2/CO2 with both the lungs and the muscles and your muscles have to produce energy contract and relax and transfer all the involved products And all these factors together determine your anaerobic capabilities This is also dependent on how well you are trained The more trained you are the higher levels of lactate you can sustain the more glycogen you have in your muscles or even the more blood you have (and many more) So how does this apply to you Assuming your maximal heart rate lies at 200 bpm and you're working out at 170 that's 85% of your maximal capacity My cynical guess is that your heart rate monitor is slightly off One way of determining whether it's correct is trying your own VO2Max-test only without the actual oxygen testing Follow a standard testing protocol to test how your heart rate increases when you incrementally increase the home trainers wattage until you can no longer sustain the load That's a fair guess estimate of your real maximum and will give you an idea whether 170 bpm is around or above your lactate threshold I greatly simplified several aspects if you have any questions or find that I need to explain something just leave a comment
2266,fitness,There are several ways to measure your body fat percentage Sportsmedicine About com has a nice description of the most important ones in order of their reliability Underwater Weighing - Hydrostatic Weighing One method of body composition analysis in which a person is weighed while submerged in a large tank of water is called underwater or hydrostatic weighing This method of determining body composition relies on Archimedes' Principle of displacement which states The density of fat mass and fat-free mass are constant Lean tissue is more dense than water Fat tissue is less dense than water Therefore person with more body fat will weigh less underwater and be more buoyant Skinfold Thickness Measurements Because underwater weighting it is complicated and cumbersome and requires special equipment most exercise physiologists use simple skinfold measurements to determine body fat percent The American College of Sports Medicine says that when performed by a trained skilled tester they are up to 98% accurate Bioelectrical Impedance Bioelectrical Impedance is another method of assessing body fat percentage There are a variety of body composition and body fat analyzers and scales available for home use that provide more than just total weight measurements These devices determine total weight the percent and amount of body fat muscle mass water and even bone mass While the readings can be affected by hydration levels food intake skin temperature and other factors if you follow the directions and take the reading under similar conditions you will obtain the best results
2652,fitness,There are countless possibilities that use no/minimal equipment Jogging sprinting intervals etc Stair climbing Jump rope Burpees (this one will surprise you try to get through 50 as fast as you can) Tuck jumps Squat jumps Shadow boxing Power yoga Best of all any timed circuit/interval workout check out the Crossfit Bodyweight Workouts PDF for a great list
2291,fitness,Yes Your target heart rate is based on a percentage of your maximum heart rate The formula is (((220 - Age) - RHR) * 0 7) + RHR This formula will figure for 70% of your maximum based on the Karvonen Method Your Resting Heart Rate (RHR) should be averaged over a few days (3-5) You should take it upon waking before getting out of bed and count total beats for one minute
2341,fitness,Nose has natural air filters for all sorts of things; temperature regulating filters for example Nasal breathing is very important at cold weather However there is a lot of debate concerning the general advice to breath through the nose during cardio workouts You need a lot of oxygen and maybe nose is too small to deliver enough Many runners breath both through nose and mouth at the same time (I should add some sources later - please edit if anyone has some handy)
2287,fitness,Lifts in no particular order Wrist Curls Reverse Wrist Curls Farmer's Walk Wrist Roller Reverse Curls (these interject the outside of the forearm almost as much as the bicep) Other things that help When running do so with tennis balls in your hand so you force yourself to grip the entire time The gain is nominal but it is another chance to work your forearms while exercising When at work periodically squeeze on a stress or tennis ball when not typing/writing or otherwise using your hands When lifting make sure to squeeze the bar/weight as much as possible no matter what lift you are doing If it's a machine grip the handle as hard as possible This forces you to tighten not only your hands/formarms to work them but also makes you tighten the rest of your body There's really no such thing as a lift that only works parts of your body - if you are doing them correctly
2289,fitness,Nothing will totally prevent or 100% effectively treat Delayed Onset Muscle Soreness (DOMS) To treat DOMS though you can try Ending your workout with aerobic exercise will decrease it Taking aspirin or ibuprofen will reduce the pain You shouldn't take ibuprofen before working out This will only treat the symptoms and will not reduce healing time To prevent DOMS A good warm up will go a long way in reducing the soreness Avoid sudden major changes in workout type or length Work into it For more tips (some with significantly less scientific backing) see the Sports Medicine article on About com
2349,fitness,The standard advice is a good meal 3 hours before race/training refuel (like a small gel+water) 20 minutes before start begin fueling 15 minutes after you start hydrate every 15-20 minutes use bars/gels/drinks every 10-30 minutes Another thing is that you can calculate how many calories you'll burn and be sure to get that much from your gels/drinks in regular intervals Generally you have to try and see what works for you (I don't know how well are all these documented but they get used a lot) Be careful when running for 2+ hours you may refuel too little but you may also do so too much don't drink too much don't refuel too much; it's dangerous
3302,fitness,Shoulder injury is often caused by weakness in or overloading of the rotary cuff It's badly developed by most people since they don't really train it Badly executed Bench Presses seem to be a major cause too by putting too much strain on the shoulders So in this light I think it's important to train the muscles involved with your rotary cuffs Infraspinatus Supraspinatus Subscapularis Teres Minor There are a few very easy to execute exercises that can help in strengthening your cuffs with only a dumbbell Lying Shoulder Internal Rotation http //www youtube com/watch v=Vw8HVQuMpNc Lying Shoulder External Rotation http //www youtube com/watch v=VuHuKq3elzQ&amp;feature=related Of course you can use cables in stead of the dumbbells if you happen to have those EDIT BUT If you ever feel a strain on your shoulders or pain while exercising go see a kinesiologist
2386,fitness,I have been running for about twenty years and always found the logic behind barefoot running quite compelling If you want to read more about it after watching that video you can look at Harvard's Skeletal Biology Lab's Barefoot Running Website or just watch the compelling videos showing ground reaction forces for different types of foot strikes After becoming a father my running slipped to about four months of running a year leading to a big July 4th 10K in Atlanta GA (The Peachtree Road Race) followed by eight months off A few years ago I decided to give barefoot running a try during my normal off season I destroyed the soles of my feet a few times by running with bad form and trying to do too much too soon but they healed up surprisingly quickly -) ONE CANNOT PUT TOO MUCH EMPHASIS ON GOING SLOWLY WITH BAREFOOT RUNNING Eventually I bought a pair of Vibram Fivefinger KSOs and this made a huge difference I would start out running a short distance barefooted to help me with my form then put on the Vibrams and run a mile or so Running in Vibrams made my feet sore for the first few weeks This was muscular soreness most noticeable when I got out of bed each morning and the muscles in my feet were stiff The soreness would go away after about five or ten minutes Eventually I stopped having sore feet and gradually worked my way up to a mile barefooted followed by three more in my VFFs - all on concrete and asphalt I still run this way and really enjoy it Last year I ran the last three miles of the Peachtree Road Race barefooted and it was hilarious I got quite a few comments -) Now I consider myself to be a Natural Runner running as I would run barefooted but using minimalist footwear to protect the soles of my feet from the friction of concrete and asphalt After running in VFFs for 5 years I am sure that I will never go back to traditional running shoes though I also do not think barefooted running is practical if you are running on concrete and asphalt One can do it but it would take me more than a year of committed effort to develop the necessary toughness in the soles of my feet to put in 15 or 20 miles a week this way These surfaces are just soooo abrasive From my current perspective the notion that you would take a complex shock-absorbing structure like your foot and encase it in a shoe that filled-in the arch and presented a flat surface to the ground then add shock-absorbing materials to the shoe and expect that to be better for your feet seems somewhat comical Of course that's just my opinion I could be wrong -) One additional note if you are going to experiment with barefoot running read the advice found here on barefootrunning com It is the best advice I found online While it discusses the foot strike it does not put too much emphasis on it If you really try to land on your forefoot you will blister your foot Focus primarily on the other advice like landing with your feet under your hips and bending your knees more and lifting your whole foot rather than pushing off with your toes I find it helpful to think that I am placing my forefoot on the ground then shifting my weight to it Of course this happens very quickly and the effect feels more like landing on your whole foot but your forefoot contacts the ground first then your foot and ankle absorb much of the impact compared to a heel strike UPDATE 03/20/2012 I just found this video that gi
2306,fitness,Take a dumbell bar and tie one end of a length of rope to it Tie the other end to one of your weight disks Hold the dumbell bar out in front of you one end in each hand and rotate it so the rope wraps around the bar The weight should rise with the rope creating resistance Twist the bar one way until the weight touches it then reverse the direction all the way down and back up again Repeat until your forearms burn like fire
2301,fitness,If you are looking to build mass you should rest 45-60 seconds between sets of 8-12 repetitions for optimal gains This timing builds optimal muscle mass and hypertrophy If instead you are looking to improve your strength or endurance you should be looking at a 3-4 minute break between sets You'll want 4-6 repetitions with heavier resistance for strength or 16-24 repetitions with lower resistance for endurance The Art of Manliness published a great article with an extremely simple strength/endurance training routine See World Fitness Network
2317,fitness,The effectiveness of commercially available sports drinks This study from the University of Tasmania suggests so Our conclusions are 3-fold First because of variations in drink composition and research design much of the sports drinks research from the past cannot be applied directly to the effectiveness of currently available sports drinks Secondly in studies where a practical protocol has been used along with a currently available sports beverage there is evidence to suggest that consuming a sports drinks will improve performance compared with consuming a placebo beverage Finally there is little evidence that any one sports drink is superior to any of the other beverages on the market
2360,fitness,Theoretically you can loose weight with doing just one exercise However if you want to loose more weight you have to raise the number of calories you've burned You can do this by exercising under higher loads or working out longer But fitness machines often fail at mimicking the natural increase in loads on an elliptical trainer when you increase the loads going 'round' get's harder everywhere I've made a small model of the forces during an elliptical workout when you increase the load from the light to the dark blue arrow the forces get proportionally higher everywhere But the problem is when you're leg is stretched away from the center of your body those 'small' forces have a large moment arm (towards the center of your body) So while it's not an issue for the forces to increase under your body it because extra hard when they increase far away from your body This causes it to be pretty hard to go faster and faster on your home trainer even when your fitness increases So that option is basically out as there's almost no home trainer that doesn't make the same mistake Your other option is to workout longer which happens to be a very good choice when you want to burn fat Because burning fat happens mostly at lower intensities so actually when you start to work out harder your body shifts gear and moves away from fat to other 'faster' fuels Therefore it would be better to burn 500 kcal at 60% of your maximal heart rate than at 80% because at least you got rid of some fat There's another reason why you need to work out longer fat is a slow fuel and your body doesn't really start to burn fat until you're 30 minutes into a workout Luckily nobody said you had to workout hard to get to this point so I'd advise you to use a long relaxing 'warming up' just to get your metabolism going and then get going with the real deal So basically no you don't need to go out and buy anything else But if you like some variation I'd really recommend you to start running outdoors instead Running does become heavier 'naturally' when you increase your speed and it's generally more pleasant to workout outdoors as well
2316,fitness,In a groundbreaking 2003 experiment scientists at the Georgia Institute of Technology found that 50 minutes of hard running on a treadmill or riding a stationary bicycle significantly increased blood levels of endocannabinoid molecules in a group of college students The endocannabinoid system was first mapped some years before that when scientists set out to determine just how cannabis a k a marijuana acts upon the body They found that a widespread group of receptors clustered in the brain but also found elsewhere in the body allow the active ingredient in marijuana to bind to the nervous system and set off reactions that reduce pain and anxiety and produce a floaty free-form sense of well-being Even more intriguing the researchers found that with the right stimuli the body creates its own cannabinoids (the endocannabinoids) These cannabinoids are composed of molecules known as lipids which are small enough to cross the blood-brain barrier so cannabinoids found in the blood after exercise could be affecting the brain Phys Ed What Really Causes Runner’s High
2321,fitness,I suggest you read this article from the Australian Institute of Sports About halfway down the page it has a section titled Protein – are vegetarian diets adequate There is too much info to quote there so I'll put it in point form but I suggest you read the whole article Although most vegetarian athletes meet or exceed recommendations for total protein intake diets of vegetarians often provide less protein than those of non-vegetarians Vegetable or plant proteins may be limiting in one or more essential amino acids There is no requirement to include complementary proteins at each meal as long as a variety of protein sources are included As plant proteins are less well digested than animal proteins vegetarian athletes are advised to consume approximately 10% more protein than current athlete protein recommendations The AIS is the organisation that trains Australian olympians
2370,fitness,You should be taking your supplements daily to maintain your load Both of those supplements do good things not only for workouts but for your body in general including the brain I would suggest continuing to take them in the same manner that you take them on on days However most people I've come into contact with suggest taking creatine and L-glutamine twice a day once being before bed or mid evening and once either in the morning or prior to/following your workout When and how you take your supplements really depends on how your body reacts to them I'm with you on the sugar why they have to add so much sugar to our food these days I'll never know
2331,fitness,In my own experience I have regularly attended a gym with a friend when I was just out of high school He was a few steps ahead of me in terms of fitness and strength and he was quite committed This was really helpful as it meant I would try harder to reach my goals I have also joined gyms and made friends by attending regularly You tend to find a few of the same people showing up at the same time if you attend the same time regularly Meeting people this way is less 1 on 1 and means you can do your own thing if you want but also makes the overall experience more enjoyable and encouraging to keep returning
2338,fitness,Plyometric exercises are specialized high intensity training techniques used to develop athletic power (strength and speed) Plyometric training involves high-intensity explosive muscular contractions that invoke the stretch reflex (stretching the muscle before it contracts so that it contracts with greater force) The most common plyometric exercises include hops jumps and bounding movements One popular plyometric exercise is jumping off a box and rebounding off the floor and onto another higher box These exercises typically increase speed and strength and build power If you combine Plyometric sessions while maximizing the cardio side of it with minimal rest time you can achieve high lung capacity The small window of rest with these sessions will increase your heart rate and force your body to adjust to the right breathing pattern (of course done w/ moderation and safety in mind) Plyometric is considered a must in all athletic training Soccer players recovering from ACL surgery participate in Plyometric to get back into competition shape Plyometric will increase your performance in any sports including running if it is done right
2350,fitness,When I was in the Army we did wide-arm push-ups elevated push-ups diamond (close-hand) push-ups push-ups with resistance (someone putting pressure on your back or a sandbag on your back) and push-ups on our knees The variations in resistance targeting different muscle groups and working towards a burn-out all contributed to breaking the muscle down to the point where it would grow back stronger during the recovery period Lastly endurance is about numbers You have to do lots of sets and lots of reps with a rest period of at least 1 day in between For instance we did push-ups Monday Wednesday and Fridays
2357,fitness,Based on my own experience and what I've read Start with warm ups to losen all your muscles especially lower legs thighs etc (anything that will get a workout while you're running) Practice your running technique - pumping your arms and stretching your legs Develop a strategy for running the race; start strong keep a good pace through the middle push yourself to get a little faster and then try and stop thinking about the race so you can keep pushing your body without getting too tired I've found a couple of interesting posts that might help - this one on strategy for running the race and this one on how to run the race and improve your times Hope this helps and wish you all the best for your race
2355,fitness,You can certainly run when it's cold but there are some things you should do to make sure you're comfortable and safe This guide on Cold Weather Safety recommends that you dress in layers As you warm up which you will you can remove layers as needed If you feel cold you can add the layer back on Nylon or Goretex is recommended to reduce moisture and protect you from the wind and a knit cap can keep 40% of your body heat from escaping through your head Gloves and a ski mask are also options if it gets cold enough
2368,fitness,Almost everything you're eating looks like it's fairly high in carbohydrates and low in protein Try a protein shake or drink a few minutes before your workout and see if that helps I've found that drinking a Special K2O before a workout stems my hunger significantly It comes in several flavors and is high in protein and fiber (5g each) and low in carbohydrates (1g net) Protein mixes and things like that will typically be available in the pharmacy section of your local supermarket and the resident pharmacist will be able to assist you in making the right choice That's what they're trained to do
2385,fitness,In general the method I would use would be this Find a sports trainer or other resource for your specific sport When I say trainer I don't mean coach You want the guy that would prescribe an ice bath or massage vs the guy who doles out the workouts Through that resource determine what physical capabilities are needed for your sport Devise a way to test your capabilities to identify any areas of weakness you might have The test could emphasize different physical aspects like muscle strength flexibility endurance stamina range of motion hand-eye coordination etc It all depends on your sport Using the results of the test craft a training plan that will minimize or eliminate those defficencies Retest yourself to see how well you have improved after being consistent with your training plan after enough time has passed ( Enough time can vary depending on what you're doing but a good rule of thumb is six weeks )
2369,fitness,A proper exercise routine including strength training can be extremely beneficial and helpful to anyone including the elderly You should always consult your physician before beginning an exercise routine to see if you are healthy enough to perform it especially if you have heart lung or other ailments Geriatric Times published an interesting article on this in 2005
2367,fitness,It is quite well known that average adult human being needs about 8 hours of sleep a day In terms of feeling well rested It does not matter how long you sleep rather what quality is the sleep You are getting The same goes for muscle building Sleeping 7-8 hours every day would be the optimum More than that and you risk oversleeping which in my experience is much worse than sleeping too little There is a good e-book on this subject How To Sleep Less and Have More Energy Than You Ever Had As to the time question It is never a good idea to go to bed immediately after a workout You should plan the workout to end at least 2 hours before You go to sleep to have the best sleep and the best rest Also if Your goal is weight loss then the best time to workout is in the morning You have just woken up Your energy levels are quite low carbohydrates are not available so the body has to burn fat instead @Louis In answer to Your comment Right after the workout Your energy levels are at their highest It stands to reason that even if You were tired and sleepy before You will most definitely be not after a good workout Furthermore our biological clock is temperature oriented That means that when You go to sleep Your body temperature is lowered That is a prerequisite for a good nights sleep That is why we sleep best in a cool room rather than in a hot room Right after the workout Your body temperature is quite high so there will be no sleep for You This is from a biological standpoint In regards to the 2 hours that is not a specific time limit That is just something that was mentioned in the book and I usually follow it just as a guideline
2382,fitness,There's a nice article on developing mental toughness on Ralph Jean-Paul's blog The main key to developing mental toughness is the same as developing a muscle repetitive use and practice Interestingly the development of mental toughness is also a spiritual battle (Biblically) (see the last paragraph where Paul speaks of beating his body into subjection) and while normally I wouldn't throw a religious link into the mix here I think it's important to recognize on some level that in order to successfully develop a better stronger attitude the spiritual side is certainly an issue I would consider looking into most of what are known as the Spiritual Disciplines as a method for developing mental toughness These include fasting meditation prayer servitude simplicity etc All of them will have a positive effect both on you and on those around you and will do wonders at developing your character and inner strength Note I own the blog referencing Paul's spiritual battle However this same information can be found at many other sources through a quick search on Google Addressing comments about the spiritual aspects of mental toughness take a look at this quote from the Fort Hood Resiliency Campus of the US Army's web site Any human being functions in three modes i e Physical (Body) Mental (Mind) and Spiritual (Soul) These three aspects of our lives are not independent from each other but dependent and usually intertwined They are reflected in our behavior our relationship to others and also seen in the way we respond to challenges and crisis The Stoic philosophy is another spiritual discipline that leads to mental toughness and that is not a religion which you might want to look at One excellent source of information (links on-line books videos training courses etc) is the Stoic Foundation
2408,fitness,Too much protein is excreted so there are no health risks in that respect You will also avoid muscle wasting with a high protein diet however you're likely to have low energy levels from the lack of carbs so maintaining protein-only for more than one day isn't a good idea The tryptophan found in protein also needs carbohydrates to be transported which essentially equates to low serotonin levels for the duration of the diet
2397,fitness,If you're going to use those machines use them after you use weights or perform any sort of skilled exercise You do not want to be fatigued before you use weights I suggest a targeted warm up consisting of bodyweight exercises involving the muscles you will workout (ie bodyweight squats jumping jacks etc) before moving onto weights Get the exercises that required most movements done first e g do deadlifts before 'core' work such as planks stability ball work
2401,fitness,The odor is standard (although possibly more acute) bacterial foot odor Spraying them with Lysol after use will make them go longer without needing washed as will putting them in a (very) mild bleach/water mixture Basically anything to kill the odor-causing bacteria (laundry detergent is typically not anti-bacterial) washing them in the sink with an anti-bacterial soap by hand is more effective than the laundry You'll probably still need to wash your feet (or hit them with the Lysol/bleach as well) Other possible suggestions to kill the bacteria (and thus eliminate the funk) are vinegar (white) UV light air them in sunlight
2410,fitness,The number one thing you can do is train with a coach who knows about good running form Many cities have various track clubs that are friendly to people who are less than lightning fast However that costs money takes time and may not be ideal One activity that I used to learn a lot about my running style was setting up a video camera to record me running on a treadmill I set up the treadmill to have me run in a couple of different ways about the pace I use for my long slow runs and my racing pace both with and without shoes I then stepped through each of the videos frame by frame to get an idea for how my foot was hitting and rolling as I ran Unfortunately because the video was from the side it made it hard judge whether or not my feet were actually hitting straight on and how far left or right they were After watching each video in slow motion I stepped through the shoe and barefoot videos side by side to understand how my kick was working and more importantly what sort of pressure I was putting on my feet while landing It wasn't pretty and made it obvious why my knee hurt so much with shoes I was hitting on my heel with my leg fully extended There was nothing to absorb shock except my heel Using that information I've been paying extensive attention to my running form while on training runs -- regardless of whether or not I'm barefoot Mainly this consists of making sure that I land further up on the ball of my foot with a slight bend in my leg and that my heel hits the ground after the ball of my foot An easy way to tell if you've done this right is by listening to to the noise that your footfall makes Landing on your forefoot is usually much quieter This will however end up hurting your calves It took me about four weeks before I was comfortable running with a forefoot strike without focusing on my food landing My calves still haven't built up all the way but that's coming For what it's worth I got the idea of recording my running from a running analysis video by NJ Sports Medicine on YouTube
2438,fitness,I'd recommend going with a rapid fire style of workout using light dumbbells that is a hybrid of both cardio and strength training For instance see this video from Scott Abel The rationale for why you would want to do this workout is in the zentofitness com article too
2417,fitness,I'm not sure about the origin but there was an interesting article in the local newspaper about the All Blacks (the New Zealand rugby team/world number one team) in which it was stated even the lightest member of the team was overweight (based on BMI) and four of the players were obese As professional sportsman these players are all very well built and very muscular but hardly what you would think of as obese I think measuring your body fat might be more helpful as being X KGs/X cm tall with 5% body fat vs 25% body fat is a big difference You could probably also form your own judgements as to whether or not you are overweight but I'd say while BMI might be a useful guide it's hardly definitive as to what your ideal weight is Hope this helps Good luck
2419,fitness,I suggest you read the wiki article In large amounts and especially over extended periods of time caffeine can lead to a condition known as caffeinism Caffeinism usually combines caffeine dependency with a wide range of unpleasant physical and mental conditions including nervousness irritability anxiety tremulousness muscle twitching (hyperreflexia) insomnia headaches respiratory alkalosis and heart palpitations Furthermore because caffeine increases the production of stomach acid high usage over time can lead to peptic ulcers erosive esophagitis and gastroesophageal reflux disease Caffeine may also increase the toxicity of certain other drugs such as paracetamol There are four caffeine-induced psychiatric disorders recognized by the Diagnostic and Statistical Manual of Mental Disorders Fourth Edition caffeine intoxication caffeine-induced anxiety disorder caffeine-induced sleep disorder and caffeine-related disorder not otherwise specified (NOS) Refer to article for more info and references
2426,fitness,Supermans will work your lower back without the need for ill fitting machines ref site So does the plank
2427,fitness,You don't want to overdo electrolytes so if you're going to get them from fruits or vegetables don't add much of anything An avocado is a great example to get potassium Also bananas are great as well Presumably unprocessed banana chips (dehydrated banana) would be a good start Celery is also a good way to up your sodium levels All of your electrolytes can be gotten from purely natural sources and your body will absorb them slightly easier from these natural sources The basic thing to remember is what your electrolytes are sodium chloride potassium magnesium and calcium Potatoes avocados and bananas are good for potassium leafy green vegetables like lettuce and cabbage for magnesium cheese and yogurt for calcium and celery for sodium chloride Just plain table salt will also work for sodium chloride High fiber isn't a big concern Fiber is regulatory not diuretic As long as you're going normally to start with then having the extra fiber won't cause you to need to go in the middle The big thing is the amount you'll need to consume to balance your electrolytes may actually cause you to need to go so I would start switching over to the natural stuff slowly to see how you perform Example 1oz of cheese on a few leaves of lettuce 1/4 - 1/2 of an avocado and a celery stalk for a salad That will add a load of electrolytes but only be about 1 - 1 1/2 cups of food Start that during practice so that if you do have to stop to go you can get the timing down to where you can eat it long enough before you begin that you'll need to go before your run
2460,fitness,I stumbled onto a great resource of swimming information Swimsmooth com that happened to have a couple of great tips I'd like to share 1 Focus on exhaling rather than inhaling The reason you want to breathe is not because you have a lack of oxygen You only consume a couple percent of the 20% oxygen you breathe in no the reason you breathe is to get rid of the CO2 So focus on exhaling as fast or powerfully as possible With focus I mean practice it regularly while swimming more slowly and really try to get rid of all the breath in your lungs as fast as possible When you've exhaled you're able to inhale again so mastering exhaling gives you more flexibility in when you decide to inhale again (you never have to wait) 2 When you're not breathing keep your head still Your head is like the steering wheel of your entire body so keeping your face still looking down means your body will have a steady posture too If the swimming pool has lines on the bottom it greatly helps to swim straight above because it gives you a point of focus 3 Breathe in your armpit As Swim Smooth puts it When you move through water you create a 'bow wave' with your head and body just like a boat does The shape of the bow wave means the water level drops along the side of the swimmer's face This creates a trough either side of your head and body that is beneath the surface level of the pool - so there's air lower than you might expect there to be Try to limit unnecessary movement so simply rotate your head by moving your chin towards your armpit Normally the trough created by your head should be large enough to allow your mouth to be just above the water so you can breathe in 4 Don't lift your head If you listened to my previous tip you already knew you not supposed to lift your head but simply rotate it One important reason you don't want to lift your head is because it's very straineous on the back/neck muscles but more importantly it causes your legs to sink lower This slows you down and makes the swimming heavier thus increases your need for air 5 Don't over rotate your head A picture says more than a 1000 words so here's another great example from Swim Smooth If you rotate your head too far remember your entire body is rotating with This is bad for several reasons your leg might be kicking near the surface of the water rendering it pretty much useless; your arm dives too deeply making it heavier to lift out of the water again and your increasing your frontal surface within the water which is also very bad 6 A lack of body rotating hurts your breathing The first reason for this one is obvious if you rotate your body you don't have to rotate your neck as much to be able to breathe But there's more so much actually that Swim Smooth has a whole article about it that I highly recommend Other than because you need it for a good technique I think there's one important point Those muscles are much more effective if you rotate slightly because when lying face down it means more strength would have to come from your breast muscles instead of the very strong b
2453,fitness,There are four major strokes in swimming Freestyle Backstroke Breaststroke Butterfly stroke Of these freestyle is the fastest and most efficient swimming style In my own experience the butterfly stroke is by far the hardest to maintain for long distances How you fill in your workout depends on two factors how well can you swim and how hard do you want to be working out How well can you swim If you have a poor technique or a poor condition swimming will become quite heavy to sustain at a decent speed very quickly The poor technique causes you to swim inefficient because having a wrong posture creates a lot of unnecessary drag and inefficient breathing The poor condition means fatigue will have a negative influence on both your speed and your technique A combination of both will lead to a complete downward spiral Why should you care Well the point is you want to be swimming for a long stretch off time enough for some real cardio-exercise You don't want to be 'wasting' energy and end up completely wasted half way through You should only make it harder on yourself once yourself once you have a decent baseline condition Pick the strokes that allow you to swim for as long as possible How hard do you want to be working out Like running swimming is something you can make as hard as you want to If you swim faster it will cost you more energy You can also add tougher strokes (like butterfly stroke) or add resistance (wear a t-shirt or use paddles) to increase the resistance The other option is working out longer though unless your serious about swimming long distances I reckon this is the last option for everyone Why should you care This means that if you always swim for half an hour and you're swimming condition + technique start to improve you can either do more in those 30 minutes or add some minutes to make it harder So say your already capable of swimming half an hour non-stop what should you be doing As with every other workout you can either try to swim faster (more miles/hour) and just swim non-stop or use interval training to push yourself harder during short stretches of time combined with periods of relative rest For pushing yourself faster I'd recommend the latter Though you should always combine it with 'rest' days where you simply swim non-stop and perhaps focus on improving your technique rather than pushing for speed A 'typical' swimming workout with intervals could consist out of a 400m warming up it doesn't really matter what stroke you swim just that you get your heart rate up to a nice and steady level Then you start swimming 100/200m intervals you have two options with how to approach these swim freestyle as fast as possible without going over your lactate treshold and then rest Depending on your condition you could take off every 1 45 or 2 min this means the faster your swim the longer your allowed to rest But don't over do yourself you're not trying to break World Records here swim freestyle with a certain time goal in your mind (not your fastest) then after the 100m swim backstroke or breaststroke for 50m I prefer backstroke because you train your back muscles which helps with some variation and more importantly you can get to breath non-stop which is often the most important thing during your 'rest' You could also try to focus on swimming faster for longer stretches of time An often used method for this is using a 'pyramid' built up It basically goes like 100m --> 200m --> 300m -->
2443,fitness,These things take time don't worry about it If you are consistent you will see results but plan to evaluate at least every few months not weeks If you have difficulty maintaining your motivation join a class that really helps a lot You don't want to give up on holding that position when the person next to you is still doing it (even though they are probably thinking the same about you) Also even if you fail to hold it for the given time don't just give up Let's say you have to do 30 seconds and collapse after 10 No problem relax for a few seconds and then go at it again You will fail in five Relax try again Keep pushing yourself until the proper time expires Of course this goes for resistance exercises Don't try to push yourself on stretches too much that's a guaranteed way to injure yourself You should feel tolerable discomfort not agonizing pain But really the class will help Also some exercises seem simple but there's a subtle catch to them where the instructor can help you a lot
2434,fitness,No you should not exercise with with temporary illness such as a chest cold flu or fever A runny nose or sore throat shouldn't prevent your exercise routine though Once you have an infection or illness that effects your cardiovascular or digestive system you should sit it out and let your body have the time needed to heal itself People with chronic ailments such as asthma heart disease and anything more serious needs to talk to their doctor before even thinking the word exercise Some people can some people can't even with the same symptoms Your doctor will know best
2437,fitness,If you do them on the same day your body has at least one full day of recovery between workouts Running is largely a cardiovascular exercise once you're in good enough shape to do it effectively (which at 5K I would consider you to be) Effectively it will get everything loosened up warmed up and slightly tired for you to have an extremely effective workout So you'll build more strength and tone (or bulk if you work it that way) by doing your other lifting and exercising immediately following your run Having a complete day of rest (or 2) in between workouts enables the body to more effectively and completely repair itself meaning you'll be working everything again from it's peak and not keeping your body and muscles stressed all the time Currently your routine works almost your entire body every day and that's literally just too much; your body never gets a rest
2452,fitness,This is a question only you can answer for yourself Actually all three of your scenarios are correct If you train too much you will stop gaining and it will take longer to heal If you do too much in one session you have a higher risk of injury and you will probably get burnt out Personally I do martial arts on Tuesday Thursday and Friday and weight lift on Monday Wednesday and one day on the weekend So far I haven't injured myself or gotten burnt out but I do take regular breaks from one or the other for a week or more If you're working out this much and especially if you're doing heavy weight lifting you need to take a break This is called de-loading and you'll notice that you perform better when you return from a break Listen to your body If you hurt stop If you find yourself getting sore without recovery take a break If you aren't looking forward to your workouts take a break
2470,fitness,Stretching is clearly a very controversial subject Instead use of stretching as a prevention tool against sports injury has been based on intuition and unsystematic observation rather than scientific evidence ( Source ) A second major reason that many coaches and athletes still view static stretching as an important preactivity ritual is the belief that it reduces the likelihood of subsequent injury This belief is based on the idea that a tight muscle-tendon unit is less extensible without stretching which means that its tolerance for elongation is lower This intuitive concept has resulted in a widespread belief that stretching will prevent muscle and tendon strain ( Source ) Like many sports physicians Dr Bartoli tells her patients that rather than stretching before physical activity they should do the sporting activity at 50 percent of the target intensity ( Source ) Before I continue I should note that I have found no study that focused on stretching after working out But based on the results from other literature reviews I don't think the effect after a workout is much different Also the lack of scientific interest in the topic should make one highly skeptical as well Shrier points out several important points to consider Both the muscle-tendon unit and the joint capsule may limit ROM [range of motion] Flexibility is usually considered the ROM limited by muscle-tendon and mobility is usually considered the ROM limited by capsule/ligament Stretching must be differentiated from ROM There are many individuals who have excellent ROM but never stretch and many individuals who stretch but continue to have limited ROM Therefore different injury rates in people with different ROMs may not be related to the effect of stretching but rather to the underlying interindividual variation in tissue properties anatomy etc Stretching immediately before exercise may have different effects than stretching at other times and should be considered as a separate intervention Whereas there is considerable amount of clinical data on stretching immediately before exercise there is much less data on stretching at other times Some people claim that negative results in some studies are due to improper stretching technique Because the effect of stretching are believed to occur through changes in stiffness and ROM an improper technique implies that the ROM is not increased If ROM is increased without causing an immediate injury then by definition the stretches were done properly Warm-up is not synonymous with stretching In the colloquial sense warm-up means any activity performed before participating in sport Used in this sense stretching is only one component of warm-up and if stretching is included in the pre-exercise activity I explicitily state that stretching was used The other component of warm-up is participating in an activity that requires active muscle contractions This type of warm-up can be divided into general or sport-specific warm-up In a general warm-up the objective is to increase body temperature In sport-specific warmp-up the acitvity is the same but performed at a lower intensity Be aware that the mechanism of action will dictacte whether one type of warm-up is superior to another The term dynamic stretching is currently used differently by different people but in es
2480,fitness,Exercise depends on several factors your muscles for doing the actual work and their local storage of energy; your blood as a transfer system of O2/CO2 and energy; your heart for making the blood flow; your lungs as a transfer system of O2/CO2; your liver as a supplier of energy; your nerves for stimulating your muscles So depending on the intensity of your exercise all of these systems will be taxed After a certain amount of reps you have depleted all your muscles local storage of glycogen and they will need to recharge before they're able to perform work again After the local storage is gone your blood will have to resupply your muscles to deliver you new energy This also means that if you start working on another muscle group that muscle group has to share its new supplies of energy with the muscles that are recovering Even if you were to rest your body has a storage of lactate acid it needs to process and an oxygen deficit to compensate for This requires oxygen but more importantly produces a lot of CO2 your body needs to get rid off This is called having an oxygen debt If you switch muscle groups you don't give your body time to recover from its oxygen deficit If you're trying to work out on your maximum lung capacity you have to subtract a part of your oxygen supply for the oxygen debt so you're not working out maximally anymore you're simply out of breath Furthermore if you can't supply your muscles with sufficient oxygen it will anaerobically burn it's fuels While they're a great fuel it won't last you very long and it will most likely hamper the amount of repetitions you can make with the same fuel Because as soon as you rest then the fuel get's processed again but you already moved on to another muscle group If your body feels tired your central nervous system (CNS) might also inhibit any further exercise It won't stop you from trying but you probably won't be able to perform the amount of reps you could perform with sufficient rest So all in all even if you were to switch muscle groups your body needs some rest between repetitions How much rest your body needs depends on how heavy the exercise is how fast your performing it and how many times you perform it per repetition
2463,fitness,This advice doesn't pass simple common sense Creatinine is a toxin used as a proxy to detect kidney problems It almost certainly would never have made it to store shelves if it broke down to creatinine on contact with acid An article entitled Conquering creatine myths with science simply boils it down simply as follows Myth Creatine is instantly destroyed by acid either in the stomach or in juice Fact Creatine easily survives stomach acid and can be mixed with acidic beverages if you consume them shortly after mixing The myth probably began with some creatine producers differentiating themselves by claiming their formula protects the creatine from stomach acid thereby increasing absorption rates [citation needed] At any rate the idea that citric acid has an affect that stomach acid does not is false
2482,fitness,I would suggest you may be addressing the issue at the wrong point if this is related to your work I suggest you have a look at why you are experiencing pain and strain related to your work rather than how you can reduce the pain during workouts Intentionally trying to avoid this pain during your workout may still exacerbate the problem It is also possible you may be doing the exercises incorrectly in that case I would check with a trainer or someone experienced that your arms wrists and grip are correct I've had someone come up to me before in a gym and say you're doing that wrong this advice is priceless So always check with someone if it doesn't feel right I would then attempt to address the issue at the source and find out why you are straining your wrists at work This is not good I used to have this problem when I started programming and after about six months I was in screaming pain I replaced the keyboard I was using with one I bought myself and I found the problem went away after 3 or 4 days I also learnt to take periodic brakes and how to touch type properly Touch typing (if done properly) will help immensely with strain if you are trying to go as fast as you can I know this doesn't answer your question directly however I think you are avoiding the underlying problem and could put yourself at risk by avoiding it
3234,fitness,Wide pull-ups Source This blog I can't believe nobody has mentioned this yet The muscle that gives the overall appearance of 'broad shoulders' is the 'Latissimus dorsi' or your 'lats' Wide pull-ups are a great exercise for targeting this region as well as your shoulders (deltoids) arms (both biceps and triceps) and forearms (from gripping the bar) For proper form be sure to cross your legs and lock your knees to prevent swinging or cheating by using momentum (no frog kicking for 'just one more' allowed) Raise and lower yourself in a slow/controlled manner I can't stress how important it is to maximize both the raising and lowering during the exercise Forget about the number of reps you can do (this isn't an exercise to impress the 'how much can you bench ' guys at the gym) If you do them 'right' you'll feel it Note If you don't have the strength to pull yourself up to the bar yet grip the bar and start by jumping up the position where your eyes are level with the bar then lower yourself down slowly This will help you build up the strength needed to be able to do the full motion Source Wikipedia If you're going for more of the 'thick neck' or 'no neck' look or what I like to dub the 'meat head' look you'll want to target your 'Trapezius muscle' or 'traps' Source Wikipedia The exercises that exclusively target your traps are 'shoulder shrugs' - done with either dumbbells or a barbell 'upright row' - done with either dumbbells a barbell or using a bar attached to a weighted cable It may surprise you but the 'broad shoulder' look has little to do with your shoulders and more to do with upper/lower body proportions Update Added some more specifics about the wide pull-up exercise including tips on proper form and and easier variation for those who don't have sufficient strength to do the full motion yet
2474,fitness,Sport socks can make a world of difference especially during longer workouts or if you have sweaty feet They help reduce the moisture and friction which becomes more important the longer you run In diabetics they sometimes add patches of low friction material to the insoles because this reduces the shear forces on high risk areas Good sport socks do the same they allow your foot to slip a little bit relative to the insole which greatly reduces the friction and thus reduces the chance of blisters Regular cotton socks tend to get wet after about 20-30 minute workouts and don't transfer the moisture away from your foot sole Therefore any negative effect from the moisture like increasing friction get's worse over time Furthermore wet cotton socks can loose their shape and start to wrinkle or bunch up After several wash-wear cycles cotton socks have been noted to become abrasive which can potentially be irritating on the skin Proper socks are also known to improve the fitting of a shoe because they're generally thicker in some important areas whereas regular socks are more homogeneous in their composition So I'd advice anyone who has to do some serious running for longer stretches of time to wear sport socks
2562,fitness,There are both physiological and neurological reasons to build in rest Even elite athletes who take on volumes of training most of us would not be able to comprehend have smart periods of lighter training and even full rest periods Physiologically training works by stressing the body just enough that the adaptation allows you to recover and the body overshoots a bit so you end up stronger/more coordinated than before Doing this too much however will back fire due to a principle known as GAS (General Adaptation Syndrome) You can read about it here http //www healthnewsnet com/gap html Basically it ultimately leads to exhaustion and dramatically increases risk of injury In training steadily (3 - 5 days per week) for the past 10 years I've had no serious injuries because I build in rest On the flipside I know people who refuse to stop believing more is better and go non stop until an injury has them on their back for months Much better to build in the rest periods when you can control them than to end up being forced to rest because you didn't allow your body the recovery it needs You'll often hear you are less likely to need rest if you alternate muscle groups After all if I work my legs my arms are resting right This isn't entirely true for two reasons Reason one unless you're using totally isolated movements like single arm dumbbells machines etc your other muscles do participate to an extent Your core is always stabilizing your torso your leg muscles work to provide stability while your upper body is moving etc Reason two and more important every workout taxes your neuromuscular system There is a neurological effect This is where the idea of muscle memory comes from and the reason why some people can be extremely strong without having large muscles - it's because their neurological system reacts to the workout by improving coordination of motor units the bundles of nerves that fire to contract muscle (when you lift a heavy weight or a small one motor units are helping you do that and it's just a question of how many are activated at once) Any type of training will tax your central nervous system and too much training can lead to fatigue This fatigue can tax your adrenal system and raise cortisol levels a stress hormone that also makes it more difficult to burn fat Literally you'll stress your body to the point your metabolism slows you burn fat less efficiently and stop making strength gains So yes please build in rest If you train 5 - 6 days per week then a week of light activity is probably fine but even elite athletes I know try to take at least a week off of ALL training at least once per year I've found if I train just 3 - 4 days per week then I can go a lot longer without having to take a longer period for rest
2486,fitness,Many muscle groups are involved including hand arm chest shoulder back strength it really depends on where your weak link happens to be I would suggest first making sure that you can do regular flat-hand pushups with good form Once you can do that then try raising up onto all five fingers Once you have those down you can start taking fingers away If you find that it is too difficult to do many on your fingertips at first try going to your knees and doing pushups from your knees with your fingers The point of these isn't to tax your chest/arm/shoulder assembly but rather to train your fingers to hold up the light load before you do full pushups on them
2489,fitness,Do negative chin ups Jump/hop/use a chair/whatever to get to the top of the bar and make it down as slow as you can Rest for a minute repeat 5 - 8 times 3 times a week You'll be ready in less than 10 days
2502,fitness,Is this for after weights If so swim for a few laps concentrating on your arms/shoulders [You will be swimming to work your muscles not your stroke ] For example swim freestyle but keep your hand open fingers together as you push through the water giving a little resistance to work the muscles You can also do chest fly's standing in the pool and also bicep curls and tricep push downs all with hands open
2498,fitness,Realistically protein is just protein Certainly there are some minor differences in types of protein as there are differences in carbohydrates and fats as well The primary difference in terms of digestion however is what you are getting with your protein Protein in fish contains a very small amount of fat compared red meat It's difficult to take in the fat that the body needs without eating it in meats unless you want to upset another ratio of your diet (e g you could drink milk to increase fat but would jump your carbohydrate intake as well) The primary idea is that you would want to balance your ratios of protein carbohydrates and fats to suit the type of diet you are undertaking but don't worry too much about the various types of protein
2497,fitness,According to Physiology of Sports and Exercise Every endurance exercise session should conclude with a cool-down period Cool-down is best accomplished by slowly reducing the intensity of the endurance activity during the last several minutes of your workout After running for example a slow restful walk for several minutes helps prevent blood from pooling in your extremities Stopping abruptly after an endurance exercise bout causes blood to pool in your legs and can result in dizziness or fainting Also you should remember that intensive exercising effects your entire body your heart is pumping your lungs are ventilating your blood is racing through your body your muscles are contracting your liver is producing energy If you were to suddenly stop your body will slightly lag behind your abrupt change in exercise intensity (or the lack thereof) When you've been working out near the lactate threshold your body has also been piling some amount of lactate If you lower the intensity of your workout such that you get enough oxygen your muscles will start using lactate as a fuel and get rid of it Because burning lactate creates a lot of waste products like CO2 and creatine kinase it's advisable to keep up some level of activity so that your body can get rid of it easily During workouts your body releases hormones such as adrenaline and endorphins if you were to abruptly stop exercising you maintain higher levels of these hormones which can cause a feeling of restlessness or a sleepless night Another good reason is that when you exercise your muscles stretch and shorten a lot If you were to stop abruptly you leave your muscles in a somewhat misaligned state By cooling down you gradually limit the range of motion back to the state it would be in when you're in rest So basically the goal of cooling down is giving your body time to readjust itself to the change in requirements However a study by Law and Herbert from the university of Sydney indicated that cooling down did not reduce delayed-onset muscle soreness Which might indicate that this isn't directly related to residual waste products Furthermore Tanaka an exercise physiologist from the University of Texas claims it's an understudied topic and that there's no science behind the advice I'd like to point out that my while my answer is based on the physiological processes going on during exercise I don't have any publications to back them up Lucky for me neither has he
2496,fitness,According to Physiology of Sports and Exercise Every endurance exercise session should conclude with a cool-down period Cool-down is best accomplished by slowly reducing the intensity of the endurance activity during the last several minutes of your workout After running for example a slow restful walk for several minutes helps prevent blood from pooling in your extremities Stopping abruptly after an endurance exercise bout causes blood to pool in your legs and can result in dizziness or fainting When you've been working out near the lactate threshold your body has also been piling some amount of lactate If you lower the intensity of your workout such that you get enough oxygen your muscles will start using lactate as a fuel and get rid of it Because burning lactate creates by products like CO2 it's advisable to keep up some level of activity so that your body can get rid of it easily I would therefore suggest to reduce the intensity of your workout For example in running start jogging at a speed your still able to talk This ensures you get plenty of oxygen (to pay your oxygen debt) while still maintaining sufficient blood pressure to let your body adjust itself In cycling you could keep up high revolutions but with very low resistance or in swimming you could vary your strokes focusing on getting back to normal breathing As md5sum suggested a recommended cool down for weight lifting is a short cardio workout (stationary bike treadmill etc) Depending on the intensity I'd recommend a bike over a treadmill as this is non-weight bearing which can be pleasant after heavy squats/leg exercises
2505,fitness,[I will treat this from a purely mechanical point of view as this is my area of expertise -- the actual body mechanics are much more complex but the effect is similar ] From a mechanical point of view the most efficient way to propel yourself on the bike is to step on the pedal so that the distance between your heel and the pedal is maximized Ideally this place would be your toes; however due to the toe joints your ability to do so is rather limited as the the flexible joint will change the way power is transmitted through your foot Thus the optimal place to step is the ball of the foot -- it has a good surface area for good contact and the muscular mass which reduces the reaction from the pedal and dampens somewhat vibrations from the bumpy road As to why the most efficient place is as far from your heel as possible See this handy force diagram below that I found when writing my answer (image license unknown) Although not exactly what we need here it will come in handy in the explanation How this whole biking deal works You step on the pedal with force PF which creates a torque Tc with arm `CL' and which is transmitted through the drive train system to the real wheel which due to its traction with the ground pushes you forward Mechanically the greater the arm CL the greater torque you get for a given force PF They are related linearly as follows Tc=PF CL Simple enough You might be tempted to think that if you make your pedals long enough you can exert enormous amount of force and indeed this is so Archimedes famously said Give me a place to stand and I shall move the earth with a lever The thing is your feet will need to travel much farther to cover the same distance as with shorter pedals -- but requiring a negligible force There ain't no such thing as a free lunch in mechanics as well But wait this only takes into account the length of the pedal not the foot Indeed Your foot only serves as an extension of the pedal effectively increasing the torque you produce by increasing the lever length If the length of your foot is Lf then the torque will become Tc=PF(CL + Lf) effectively increasing the torque you produce
2558,fitness,A proper squat involves the hip joint ending up below the knee joint as seen from the side (see the image above) This is called squatting below parallel Many studies indicate that squats when performed correctly and with appropriate supervision are not only safe but may be a significant deterrent to knee injuries A look at weight training injury rates and using common sense when thinking about the third world squat how you sit on any low surface (e g toilet) and the fact that olympic weightlifters - who routinely squat crazy loads WAY below parallel - can still walk should also be fairly convincing
4003,hemija,As you move from left to right across a period the number of protons in the nucleus increases The electrons are thus attracted to the nucleus more strongly and the atomic radius is smaller (this attraction is much stronger than the relatively weak repulsion between electrons) As you move down a column there are more protons but there are also more complete energy levels below the valence electrons These lower energy levels shield the valence electrons from the attractive effects of the atom's nucleus so the atomic radius gets larger
4056,hemija,Crystals have inspired a great many chemists because they are fascinating for a good reason Not only are they aesthetically pleasing but they serve as an excellent subject to tour a variety of theoretical subjects important for understanding high-level chemistry Crystalline materials are made up of periodic structures We're only going to primarily focus on binary compounds where there is not a high degree of covalency There are several ways to think about this problem but let's start with the melting of a crystal We say that at some definite temperature a highly ordered crystal will melt into a liquid Those of us familiar with the language of equilibrium thermodynamics might recognize that the change in free energy for this phase change can be written at constant temperature as $$ G_{liquid} - G_{crystal} = H_{liquid} - H_{crystal} - T ( S_{liquid} - S_{crystal} ) $$ $$ \Delta G = \Delta H - T \Delta S $$ If we suppose that this process is spontaneous then we would say that the change in Gibbs' free energy is negative i e $\Delta G &lt; 0$ This is true if and only if $$\Delta H &lt; T \Delta S$$ Traditionally we interpret this as saying that there is a thermally-driven increase in entropy when we melt a highly ordered crystal into a liquid which more than offsets the energy cost associated with the enthalpies of the interactions holding that crystal together A chemist tends to learn early on that the reverse is not necessarily true at some definite temperature a perfect crystal rarely forms from the liquid This inability to just heat up any substance and always produce a perfect crystal by cooling illustrates how crystal formation is a case of kinetic- rather than thermodynamic- control So the process by which you form your crystal could possibly result in a different crystal structure Sometimes crystal structures change just by altering the temperature of the chamber you're measuring the crystal structure in Now neither of these cases apply to sodium chloride to the best of my knowledge The formation of an ionic crystal such as sodium chloride is a delicate balance between electrostatic attraction and Pauli repulsion Electrostatic attraction says that between two different charges $q_+$ and $q_-$ there is a Coulomb force given by $$F= \frac{k q_+ q_-}{r^2}$$ where $r$ is the distance between the two charges If one plays with the numbers then it's easy to see that at short distances the force is strongest but there is a limit to how close they may come together Eventually a repulsive force due to a quantum mechanical principle called the Pauli Exclusion Principle overpowers the attraction An equilibrium results in which the atoms sit a certain distance from one another so that if you will humor me the forces between them balance out This is why we traditionally represent crystal packing using marbles with a unique radii The radii of the hard marble represents where the Pauli repulsion overpowers the attraction You might say Sure we have these kinetic electrostatic and quantum mechanical factors to consider but how do these help with the final crystal structure Hold your horses we're getting there A famous mathematician and scientist thought about the most efficient ways to pack spheres of the same size together By most efficient I mean this in terms of what FedEx considers efficient fitting things together into the smallest possible volume This is also what electrostatics want Kepler sug
4013,hemija,I think your question really arises from some confusion about what $\Delta G$ represents In general $\Delta X$ for a thermodynamic quantity $X$ is the change of $X$ along some process You could make it clear by actually writing $\Delta G(\text{A}\rightarrow\text{B})$ where A and B are before and after states (We'll note that in the general case $\Delta X$ depends on the path take from A to B making this notation improper If $X$ is a function of state though you're good to go ) However in the equation you quote $$\Delta G = \Delta G^0 + RT \ln Q$$ the $\Delta G$ is a free energy of reaction and should thus be denoted $\Delta_r G$ with the correct equation being $$\Delta_r G = \Delta_r G^0 + RT \ln Q$$ The free energy of reaction is defined as $\Delta_r G = G_{\text{products}} - G_{\text{reactants}}$ Thus this $\Delta_r G$ is not the variation of $G$ over the entire reaction which would be the $\Delta G$ of the system between the start of the reaction and the equilibrium PS I think this link is the online resource I found with the clearer use and explanation of notations Notations are important in thermodynamics
4012,hemija,In the presence of these strong acids the NMe 2 group is partially protonated and the protonated form is meta-directing Under the conditions I know for that experiment you get a mixture of para- and meta-product but no ortho-product due to steric hindrance
4019,hemija,Generally speaking the best solvent will be dependent on the impurity that you are trying to remove The solvent must dissolve both the desired compound and the impurity at a high temperature but only the desired compound at lower temperatures The solubility product of the impurity as well as the common ion effect should both be taken into consideration
4022,hemija,Silver is not as inert as gold Tarnish is the name we give to the phenomenon when silver metal is oxidized and becomes a salt Surfaces made of silver tend to disinfect themselves pretty quickly As for disinfecting water poured into a silver cup I imagine that would take a little longer since you have to wait for silver to diffuse away from the surface and into the solution But even very trace levels of silver can have strong antimicrobial effects
4090,hemija,Setting the scene for your question In Environmental Chemistry there are several pathways a chemical might travel through including being subjected to the biochemistry of an animal photodegradation chemical degradation (perhaps it slowly reacts with water etc ) uptake as a metabolite in plants degradation by soil microorganisms and this is ignoring aquatic and air pathways Say within a mouse or a fly a possible pathway for degradation for some insecticide is cleavage of the P-O bond How they have determined this in the past is noteworthy; a synthetic chemist would isotope label an atom in the molecule say $^{33}$ P -labeled or $^{14}$ C -labeled and he purified it before finally determining its specific activity Then a biochemist would dose a mouse with a solution of the compound dissolved in an appropriate solvent such as olive oil Urine and feces were collected after administration An aliquot of the collected urine for example would have its radioactivity measured using scintillation counting The organo-soluble species would be extracted from the rest of the urine and the distribution of radioactivity could then be determined between the aqueous and organic layers Then one could investigate what were present by TLC if you're old school -- they would actually run via cochromatography all the possible metabolite products they could independently think of and synthesize to identify a TLC spot (That is a lot of work ) These days we can cheat with HPLC-MS So you might ask are results from this generalizable I have to tell you that they generalize in a very rough way Enzymes are responsible for all the nifty biochemistry and they tend to be rather selective towards a given substrate and in many cases chirality even matters What they are picky about is a tough problem that is tailored to each individual enzyme One can make all kinds of generalizations about the chemical nature of the bonds and some possible predictions from it but those won't necessarily hold up in an animal model with a different molecule So nitty-gritty mechanisms might not be what we want to look at Instead we'll step back and take a broader view of this landscape We need to look at bigger items in terms of physical properties namely 1) Persistence what is its half-life in soil etc This is a matter of degradation whereas the next is all about movement of an intact molecule A simple chemical way to test this is how they resist hydrolysis 2) Mobility over time does the compound tend to sit or spread itself around For example factors that influence this are sorption how much does our chemical bind to our soil water solubility how easy it is to move in water for our chemical vaporizability how likely is the material to evaporate away from its source A famous organophosphate is malathion The partition coefficient according to the wiki page is 2 36 This implies its solubility is greater in organics than water but some of i
4065,hemija,While nomenclature is of particular interest to organic chemists to specify an exact compound the classification of X into broad category Y or Z isn't a precise science and not really of practical use The article cites a textbook by Seager to this effect stating The distinction between organic and inorganic carbon compounds while useful in organizing the vast subject of chemistry is somewhat arbitrary Even if you find a source that says charcoal is (in)organic you may just as well find one stating the opposite Just like the coal from which it may have been produced it was once biomass and decidedly organic but so was graphite and diamond or CO 2 and CO 3 2&minus; I think it's overly pedantic and unproductive to try to come up with definitive judgements for these decidedly edge case scenarios After all it's just a chemical on the shelf what one does with it is far more relevant I don't use it on a daily basis but it seems more like a tool than a reagent The fact it contains carbon seems beside the point; it's value isn't in the chemical composition but rather its extraordinary adsorptive properties
4057,hemija,ChemEQL is pretty nice in some instances and free but it looks like it has been mostly abandoned I'm aware of Titrator but it only runs on Windows and I've never had a chance to run it It's also free I've heard good things about Visual MINTEQ but it is Windows only and I've never had a chance to run it Supposedly well maintained There is also PHREEQC which sort of works on stuff other than Windows I hear it is harder to use but probably worth it
4093,hemija,The obvious order is via the stability of the carbocation of the group I think you meant the migrating groups stability This is not what textbooks say Reactions are ruled by the delta G of the activated complex This may be close to educts or to products or some halfway state or a real intermediate minimum is on the reaction path So in some cases the model of a separate carbenium ion (in a ionic pair maybe) is useful sometimes You need to think of a three-center bond state
4233,hemija,Here's two links ( 1 2 ) that show the simple scheme on forming monolayers The essential figure is also shown here For the second part of your question I can only point out this article that describes which properties change if you make the molecules more bulky or add chloride etc I would guess that APTES has well known properties and they are neutral regarding common applications
4067,hemija,By rapid switching they technically mean Ligand-to-Metal Charge Transfer (LMCT) A more modern framework is Ligand Field Theory I would have to teach a class to fully explain it in those terms but I'll try to explain it in terms of this hybridization you brought up A chemical bond implies a higher probability of finding electrons between two bound nuclei Atomic orbitals describe the electron density for individual nuclei A bond between two nuclei necessitates some physical overlap of the relevant atomic orbitals It is in this way that we say that the linear combination of atomic orbitals say two consisting of a $2s$ and $2p$ orbital results in a molecular orbital that describes the molecule We would write the wave function for the complex as $$\Psi = C_1 \psi(2s) \pm C_2 \psi(2p)$$ The amount of mixing is just a matter of adjusting the coefficients $C_n$ And so we might say that the bond in this made-up molecule is $sp$ hybridized $sp^3$ hybridized would mean the following kind of wave function $$\Psi = C_1 \psi(2s) \pm C_2 \psi(2p) \pm C_3 \psi(2p) \pm C_4 \psi(2p)$$ Similarly $sd^3$ could mean a wave function of the following nature $$\Psi = C_1 \psi( (n+1) s) \pm C_2 \psi(n d) \pm C_3 \psi(nd) \pm C_4 \psi(nd) $$ where $n=3$ in the case of Mn or Cr If one performs the proper molecular orbital calculations on the valence $3d$ $4s$ and $4p$ for manganese or chromium and the $2s$ and $2p$ for oxygen in tetrahedral symmetry then you can draw the molecular orbital energy diagram for your complex once you match your eigenvalues to the correct trace of the appropriate transformation matrix and you can hear a bunch more jargon on it or I can explain the main point of our models why we think MnO$_4$ $^-$ is purple The bonds in specifically permanganate (I have never actually done the calculations on dichromate although they should be similar in principle) are between Mn$^{+7}$ and four O$^{-2}$ in the geometry shown here The electronegativity of oxygen clearly dictates most of the electron density and as a result has them sitting on ligands (oxygen) instead of the metal An electronically excited state can be reached with light absorption in the range of 500-600 nm light due to the relative weakness of the interactions between the ligand and metal If they were stronger interactions then it would take more energy to promote electrons into a higher energy state and the color would be shifted into the UV A colorwheel tells you that light absorption in the 500-600nm range should be approximately violet which is what we see for the color of this particular complex Anyway this excited state means some electrons temporarily move from the ligand to the metal resulting in a LMCT band
4777,hemija,Almost certainly yes Applications of modern organic synthesis methods have drastically shortened the synthetic routes for such previously daunting targets such as strychnine see Vanderwal tetracyclines Myers and pyrroloindole alkaloids Movassaghi to name but a few - and recent efforts by Baran and White on C-H activation of alkanes have already yielded more efficient syntheses of many complex polycyclic terpenes There is no reason to think that significant improvement could not occur for chlorophyll or B12 In fact the absence of chlorophyll or Vitamin B12 from modern synthetic efforts represents a bit of a blind spot; it could well be that new chemistry could be discovered in the process Although that 1 16 hydride shift by Eschenmoser will be hard to beat -)
4034,hemija,It's a very general statement but it's not always true I'll explain why it's often true and give a counter-example at the end Your majority component B and the impurity (let's call it A) form a binary system In most cases such binary mixtures exhibit a solid–liquid phase diagram as follows (image taken from these lecture notes ) This binary phase diagram has pure A on the left pure B on the right A and B form somewhere a eutectic It is the point here at concentration e and temperature y Because the existence of a eutectic point is guaranteed for any A/B binary system and because the eutectic corresponds to a lower temperature your liquidus curve decreases with increasing impurity concentration and the impurity thus lowers the melting point However not all binary mixtures form a eutectic In the words of Wikipedia Not all binary alloys have a eutectic point; for example in the silver-gold system the melt temperature (liquidus) and freeze temperature (solidus) both increase monotonically as the mix changes from pure silver to pure gold The corresponding phase diagram is as follows
4038,hemija,EDIT It is clear now that NickT was looking for an experimental solution My post deals with a computational solution We can delete my response if need be until a more relevant question arises How would I quantify how significant the interaction is Determining the interaction energy between two defined monomers such as your aromatic Triazole and amide is a rather straightforward process This process is referred to as the supramolecular approach I'll point you to a paper that analyzes the benzene dimer This method is strictly a computational one so a bit of knowledge in computational chemistry is necessary The Supramolecular Approach The What The supramolecular approach boils down to this $E_{int} = E_{dimer} - (E_{mon1} + E_{mon2})$ Here we have some interaction energy ($E_{int}$) determined from the difference of a dimer energy ($E_{dimer}$) and the sum of the two monomers ($E_{monomer}$) If both monomers were equivalent (say you were interested in the interaction energy of the benzene dimer where each monomer was a benzene ring) you could simplify the summation to two times the energy of one monomer ($2E_{mon}$) In your particular case you have two different monomers The whole idea is if I have two interacting molecules I can determine the energy of each molecule individually (as if they were separated at infinite distance) as well as their complex So as you bring these molecules closer and closer together the energy starts to go down (the interaction energy) The How NOTE Your geometry is from a crystal structure Do NOT modify this geometry You will want to keep everything exactly as is This means you don't want to optimize your system You do not want to eyeball monomer placement Take everything from your known structure and be careful not to change it otherwise it can ruin this process We are modifying the structure by truncating and capping but intermolecular parameters must stay the same for whatever it is you are trying to model You will want to optimize your cap meaning run an optimization on your capped-monomer but freeze everything but the thing you are using to cap Define your monomers You will need to determine what part of your 'dimer' system is important for describing this weak interaction I recommend keeping the aromatic ring and truncating the ring with something similar to what is being truncated You could cap your monomer with a hydrogen or a methyl group for example If the piece you've cut out is highly polarizable cap with something with a similar property If your truncated piece is neutral in charge cap with something that is neutral You get the idea Define your dimer Your dimer is simply a combination of your two defined monomers Determine the method you want to implement Post-Hartree-Fock methods are essential for this Note that if you use the widely-implemented MP2 method your answer may be way off (can over-estimate pi-pi interactions by as much as 200% ) The CCSD(T) method is recommended Define your basis set For aromatic systems your best bet would be to use Dunning-Hunzaga's correlation consistent family of basis sets I recommend using aug-cc-pVTZ for good results Whatever you decide be sure your basis set includes polarization and diffuse functions The suggested basis set does this (augmented means diffuse on all atoms whereas pVTZ means 'polarized-valence triple zeta') Determine the energies of your monomers and dimer You will want to run a single-point energy ca
4051,hemija,An $\alpha \beta$-unsaturated ketone is electrodeficient at the $\beta$ position This can be seen if you draw the resonance structures of such a molecule The $\beta$ carbon is thus a good site for nucleophilic attack But as you know carbonyls are also prone to nucleophilic attack To discriminate between the two you need to look at how the reaction is controlled either thermodynamically or kinetically In a kinetically controlled reaction the product that is formed fastest predominates In a thermodynamically controlled reaction the predominant product is the energetically favored one A Michael addition is a 1-4 addition where a nucleophile attacks the $\beta$ carbon and produces the thermodynamically favored product On the other hand a 1-2 reaction (on the carbonyl) gives the kinetic product and is obtained at low temperatures Why is the 1-4 product thermodynamically more stable Because the resulting product benefits from keto-enol tautomerism which results in lowering the energy of the system Usually the more resonance forms a compound has the more its electrons are delocalized the more stable it is Draw the resonance forms of the 1-4 and 1-2 products and see You asked for specific affinities of different organometallics in 1-4/1-2 additions My knowledge is that organocuprates ($\mathrm{R-CuLi}$) will perform Michael additions and that organolithians seem to prefer 1-2 addition Also according to this source Grignard reagents do not seem to have a preference My take on this is that the cuprate is less reactive and therefore can form the thermodynamic product whereas the lithium reagent is so destabilized that it reacts right away
4362,hemija,Ok I will try my hand on the mechanism In my opinion the reaction should work with a catalytic amount of $\ce{H+}$ since the key step for the reaction is the hydrolysis of the orthoester This is an entropically very favourable reaction since from one reactant molecule you get two product molecules The resulting molecule is extremely reactive So it can easily react with the enolized malonate The enolization might be facilitated by the $\ce{ZnCl2}$ and/or the cat $\ce{H+}$ (I'm not entirely sure whether the $\ce{ZnCl2}$ really plays the role I've given it below in this reaction So if anyone knows good arguments against it please let me know ) The protonated product can then take part in an $\mathrm{E}1$ elimination reaction to yield ethyl ethoxymethylenemalonate The carbocation which results when $\ce{EtOH}$ leaves the molecule is stabilized by the neighboring ethoxy group The hydrogen on the central carbon atom is very acidic since it is placed between three carboxylate groups so it can even be removed by rather non-basic molecules e g by an $\ce{EtOH}$ a malonate or an othoester molecule So much for the acid catalysed mechanism Now for the question why one doesn't simply use a base to facilitate the addition of the activated methylene compound Well this is due to the fact that orthoesters are quite stable against basic hydrolysis So under basic conditions you don't get the carbonyl component of the addition reaction
4055,hemija,The difficulty with creating an ionic bond glass is that creating true glasses depends on a combination of directional bonding between atoms (or molecules) and a rate of cooling that doesn't give those bonds time to move into optimal positions The result is a bit like piling together sticky spheres that become locked before they can find an optimal stacking arrangement Pure ionic bonds in contrast are more like slippery spheres that are attracted towards each others' centers but don't have any particular preferences about which parts of their surfaces make contact Given that the most likely result by far from very rapid cooling of a molten ionic solid will be to produce micro or nanocrystalline ionic solids that may superficially resemble glasses but which under X-ray or neutron diffraction crystallography would prove actually to have crystal structure at a very fine scale With that said you nonetheless can come up with a recipe for trying to make a truly glassy ionic solid especially since there's no such thing as a pure ionic bond You would want to cool a very thin layer of molten salt at at an incredibly high rate at least millions of degrees per second if I recall glassy metal cooling rates rightly For ionic you would also want the result to be pretty close to absolute zero since whatever degree of directional stickiness you can get out of ionic solids is likely to be less even than metallic bonds Finally if you really want to try to make such a thing as a serious project you of course must check the literature in detail My rule of thumb is that if you dig deep enough there are almost no decently plausible ideas that you can think of in the physical sciences that someone has not at least written a paper on in some journal and maybe even done some experiments Still what you are asking is an intriguing idea for a real experiment and it wasn't that many years ago that both metallic glasses and Penrose-tiled quasicrystals were new ideas
4435,hemija,Nitrile gloves are made of nitrile rubber or poly(butadiene/acrylonitrile) This polymer is highly soluble in chloroform with some papers I found indicating that one can dissolve up to 18% in mass of nitrile butadiene rubber in chloroform Moreover it permeates easily through NBR meaning we can expect the dissolution to be fast in addition to thermodynamically favourable Finally I would not expect the mechanism here to be any different from that of any polymer dissolution by a good solvent The solvent will permeate through the polymer intercalate between polymer chains and solvate them (inducing swelling) Once solvated the network of polymer chains looses its mechanical properties and they can fully separate (I wish I could find a good existing illustration for that part but I can't right now… If anyone can feel free to edit )
4104,hemija,The currently accepted hydrogenation mechanism for metal like palladium is the Horiuti-Polanyi mechanism ( image ) This mechanism works with good yield for alkenes and alkynes For an aromatic compound the activation energy is much higher because the aromaticity of the target molecule is being broken by the first hydrogenation Indeed 1 3-cyclohexadiene is very unstable and will revert to benzene and hydrogen instead of continuing to cyclohexane Addition of a Lewis Acid makes palladium an acceptable catalyst for the hydrogenation of benzene however An intermediate compound with the benzene and the LA is first formed and hydrogen is added to this intermediate This paper discusses the mechanism of hydrogenation in the presence of a Lewis Acid Rhodium catalysts work slightly differently They generally have ligands attached that help stabilize the intermediates You can see a simplified mechanism here and there are papers discussing specific catalysts here and here Basically in the case of benzene the relatively unstable 1 2-cyclohexadiene is held in place on the catalyst until it has been completely hydrogenated
4075,hemija,It has to be so common a question that the answer is actually given in various places on Dupont's own website (Dupont are the makers of Teflon) “If nothing sticks to Teflon® then how does Teflon® stick to a pan Nonstick coatings are applied in layers just like paint The first layer is the primer—and it's the special chemistry in the primer that makes it adhere to the metal surface of a pan And from this other webpage of theirs The primer (or primers if you include the “mid coat” in the picture above) adheres to the roughened surface often obtained by sandblasting very strongly it's chemisorption and the primer chemical nature is chosen as to obtain strong bonding to both the metal surface Then the PTFE chain extremities create bonds with the primer And thus it stays put
4066,hemija,Well shaking a sep-funnel is a great way of creating a horrible intractable emulsion with no clear phase boundary so don't do that (also the stopper might pop off as soon as you set it down) As for breaking an emulsion if you have the time just leave the sep-funnel somewhere and wait a few days I seem to recall the first and last time I shook a sep-funnel the lab tech added some salt to the water/diethyl ether emulsion however I'm not clear on this Finally consider sacrificing part of your product by drawing the funnel off to the very base of where the emulsion begins draining the emulsion layer entirely and then taking the remainder If you have blobs of one phase in the other you may be able to remove them with a pipette
4078,hemija,Which nuclei you should consider as neighboring atoms for the multiplet splitting depends on the size of the coupling constant The coupling constant are usually named after the number of bonds between the coupling nuclei so a coupling between two hydrogens that are three bonds apart from each other would be a $\mathrm{^3J_{HH}}$-coupling In an alkyl chain like CH 3 CH 2 the coupling between the CH2 and the CH3 hydrogens would be a $\mathrm{^3J}$-coupling that is about 7 Hz large Couplings over more than three bonds are usually not observed because they are smaller than the linewidth of the signals in your NMR spectrum The exception to that you're likely to encounter are couplings along C=C double bonds where you can even see $\mathrm{^4J}$-couplings or more So the simplified rule would be that couplings along three bonds are visible couplings along more than three bonds are only visible when there is at least one C=C double bond along the way The concepts of chemical and magnetic equivalence are essential to understanding how different multiplicities arise Chemical equivalence means there exists a symmetry operation that exchanges those nuclei Magnetically equivalent nuclei additionally need to have identical couplings to all other spins in the molecule Magnetically equivalent nuclei don't couple to each other In propane the hydrogens of both CH 3 groups are three bonds away from the hydrogens of the CH 2 group so the coupling between those is visible in the NMR spectrum The hydrogens of each CH 3 group are magnetically equivalent due to the fast rotation along the C–C bond and the two CH 3 groups should also be magnetically equivalant So you have 6 magnetically equivalent hydrogens that couple to your CH 2 hydrogens The result of that is a splitting into a septet (7) The OH-group is an interesting exception as you would expect it to lead to a visible coupling on hydrogens connected to the same carbon but you don't observe that under most conditions The reason is that the OH is acidic enough that the hydrogen exchanges quickly with the solvent so the hydrogen dissasociates and associates quickly This happens too fast for NMR so the other nuclei only see the average OH-hydrogen This eliminates the coupling to the OH and it is also the reason why the OH-signal is often very broad or even completely gone in NMR spectra
21319,hemija,Permeation of protons potassium ions and small polar molecules through phospholipid bilayers as a function of membrane thickness Reported values for proton permeability coefficients vary over a wide range from $10^{-2} \ \mathrm{cm/s}$ to $10^{-7} \ \mathrm{cm/s}$ (Nichols and Deamer 1980; Nichols et al 1980; Biegel and Gould 1981; Elamrani and Blume 1983; Grzesiek and Dencher 1986; Perkins and Cafiso 1986; Norris and Powell 1990) This variation relates to the use of different techniques lipids and bilayer systems (liposomes and planar bilayers) or different pH ranges (Perkins and Cafiso 1986) The proton data of this study agree with the higher reported values As revealed by Fig 4 proton permeability coefficients strongly depend on membrane thickness starting at a maximum value of $1 3 \cdot 10^{-2} \ \mathrm{cm/s}$ for the shortest lipid and steadily decreasing to a value of $4 9 \cdot 10^{-5} \mathrm{cm/s}$ for the longest lipid It follows that proton permeability decreases by a factor of approximately 250 as the thickness of the hydrophobic region is increased from $20 \ \mathring{\mathrm{A}}$ to $37 \ \mathring{\mathrm{A}}$
4318,hemija,I'm not sure why you're wanting to exert so much effort to find the charge of compounds If you really want to know the charge determine it more directly Electrophoresis can allow you to do so A charged species is put in a capillary tube that is in between a cathode an an anode When a charged species is subjected to an electric field it will experience a force proportional to its charge and the field strength Drag forces present in the solvent are also a consideration in this setup and they depend on the size of your ion as well as viscosity of your solvent One also has to worry about Joule heating as an increase in temperature will cause poor reproducibility Picking a suitable detector is fun but hopefully you can get by with the common UV-vis detector So if you can find a suitable setup know the radius of your ion and know the time between injection of your sample and its arrival at a detector you can at least determine its effective charge
4077,hemija,The molecular origin of the Hofmeister series is not well understood and still a very active area of research For example see the upcoming Faraday discussion conference The syllabus for the conference says Although it is now clear that the Hofmeister series is intimately connected with ion hydration in homogeneous and heterogeneous environments and with ion pairing the molecular origin of these effects has been poorly understood Biochemists and physical chemists have been typically using the term Hofmeister series to put a label on ion specific behaviour in various environments rather than to reach a molecular level understanding and consequently an ability to predict a particular effect of a specific salt ion (So this is not an answer but rather too long for a comment…)
4086,hemija,Some information on Side chain oxidation in alkylbenzenes is available here http //www chemguide co uk/organicprops/arenes/other html An alkylbenzene is simply a benzene ring with an alkyl group attached to it Methylbenzene is the simplest alkylbenzene Alkyl groups are usually fairly resistant to oxidation However when they are attached to a benzene ring they are easily oxidised by an alkaline solution of potassium manganate(VII) (potassium permanganate) Methylbenzene is heated under reflux with a solution of potassium manganate(VII) made alkaline with sodium carbonate The purple colour of the potassium manganate(VII) is eventually replaced by a dark brown precipitate of manganese(IV) oxide The mixture is finally acidified with dilute sulphuric acid Overall the methylbenzene is oxidised to benzoic acid Interestingly any alkyl group is oxidised back to a -COOH group on the ring under these conditions So for example propylbenzene is also oxidised to benzoic acid A Chemistry student at UBC did his doctorate on the mechanism https //circle ubc ca/handle/2429/32304 and for the pdf see https //circle ubc ca/bitstream/handle/2429/32304/UBC_1973_A1%20S65 pdf sequence=1 It was found that the most vigorous oxidant was permanganyl ion (MnO??) with some contributing oxidation by both permanganic acid (HMnO?) and permanganate ion (MnO??) in the case of easily oxidized compounds such as alcohols aldehydes or enols The mechanism of the acidic permanganate oxidation of alkanes (ethane to n-tridecane) was found to proceed via rate-determining homolytic carbon-hydrogen bond scission as depicted below [See Thesis for Diagrams] The -mechanism of arene oxidation was shown to proceed via rate-determining electrophilic attack by permanganyl ion on the aromatic ring to yield ring degradation products Phenols are believed to be intermediates in this process as depicted below [See Thesis for Diagrams] The mechanisms of the oxidation of alcohols ketones aldehydes and formic acid were determined and shown to be consistent with mechanisms previously established under other conditions
4101,hemija,Copper (II) is almost always blue so I'm not sure how much luck you'll have trying to change the color CuCl$_2$ is one of the few solutions I can think of off the top of my head that isn't blue - it's more greenish-blue CuCl$_2$ can be made via electrolysis of NaCl and copper plates You could also make CuCl$_2$ if you add hydrochloric acid and some NaCl
4173,hemija,This question raises more important issues than just the technical why methyl ester so I'll address those too The easiest explanation for their focus on the methyl ester is that the ethyl ester just isn't nearly as sweet This report says it is approximately 10x less sweet (see Table VI on page 2689 and the entry 'Asp-Phe-OEt' with ++ being defined on pg 2684) It's an easy economic choice between the two assuming the only difference is sweetness/g but you're also worried about the perceived safety concerns of the methyl derivative A toxicology class I once took can mostly be summed up by a cliche phrase the dose makes the poison Even too much of a good thing is by definition a bad thing for example too much oxygen water or even the sun can lead to death We also say that too little of any compound won't cause measurable harm It's correct to say that aspartame will be metabolized into amino acids and methanol Methanol itself is toxic in high doses in the same manner ethanol is by acting as a CNS depressant This acute poisoning can cause respiratory failure or other medical emergencies such as kidney failure Ethanol poisoning occurs in microorganisms too it has been proposed that some yeast produce ethanol as a means of eliminating competition This would have been potentially deadly for a prehistoric mammal too if it were not for enzymes which metabolize ethanol called alcohol dehydrogenase ADH lead to biochemical pathways that allow the safe removal of ethanol but methanol is a different case ADH instead make formaldehyde which is then converted to formic acid by ALDH Formic acid in large quantities is used in the animal kingdom as a weapon most notably in bee venom and from the bites of a fire ant In low quantities it is fairly benign Formaldehyde is just a nasty chemical Both are produced in the body as a natural response to methanol and it's easy to see how too much of either couldn't possibly be desirable The body also has ways to eliminate these toxic metabolites assuming you don't overwhelm these systems Hence why a sufficiently small amount of methanol won't make you blind Researchers have measured the amount of these undesirable metabolites in the blood after having people like me ingest nearly 1 8 g of aspartame in one sitting Then they looked at urine and blood concentrations and they couldn't detect any formic acid in the blood but they could clearly see it in their urine for up to 8 hours after a dose This evidence suggests that acute methanol poisoning isn't going to happen after a ~ 200 lbs person ingests 1 8 g of aspartame Let's stop for a second and put this in terms of something we can imagine I don't have any reliable numbers but let's just say that for a 12 oz diet pop there are 200 mg of aspartame That means for every liter of pop there are approximately 564 mg of aspartame That means 3 liters of pop would get me to 1 8 g of aspartame That means I would need to drink more than 3 L in one sitting to overwhelm the excretion systems This suggests an acute poisoning isn't very plausible There is however long-term toxicity to consider as well This is a much more difficult que
4095,hemija,I am not an expert in the field but a quick literature check turns up a good amount of papers on the topic In particular I found this paper [1] which nicely answers the question for guanine So in the gas phase the most stable enol tautomer has a 0 9 kcal/mol free energy difference with the most stable keto tautomer That free energy difference is very exacerbated in aqueous solution with now a ?G° = 8 7 kcal/mol We also notice that the most stable keto tautomer is not the same in the gas phase and in solution and that both keto and enol have many tautomers close in free energy showing the limits of the simple keto vs enol line of thought Regarding uracil the first reference that comes up in a bibliographic search is this paper [2] I only quote the abstract The effects of the solvent on the tautomeric equilibria of cytosine and uracil are studied using Onsager's reaction field model in the framework of density functional theory […] Our results are in good agreement with available experimental results and confirm that the polarization of the solute by the continuum has important effects on the absolute and relative solvation energies pKa Values of Guanine in Water Density Functional Theory Calculations Combined with Poisson-Boltzmann Continuum-Solvation Model Y H Jang et al J Phys Chem B 2003 DOI 10 1021/jp020774x Solvent effects in density functional calculations of uracil and cytosine tautomerism L Paglieri et al Quantum Chem 2004 DOI 10 1002/qua 560560517
4097,hemija,Other than the excellent answer by F'x there is also actual spectroscopy specifically UV/Vis spectroscopy This'll only work if the metal has transitions in the right range though -- transition metals lanthanides and actinides are usually fine s or p -block metals and metalloids probably not Then you're looking for the absorption/transmission spectrum of the ion and the metal separately and how much of each there is in a combined spectrum It's not strictly quantitative but you can get a good idea of the ratios
4098,hemija,Questions about why we represent things in one way or another are hard to answer in general because they appeal to one’s sense of taste… de gustibus non est disputandum Here I want to state the position and the reasoning of IUPAC on this matter The relevant publication on this are the IUPAC 2008 Recommandations titled Graphical representation standards for chemical structure diagrams (IUPAC Recommendations 2008) [1] Section GR-6 of the report deals with “Aromatic rings and other types of electron delocalization” I quote part of it (emphasis is mine) GR-6 5 Curves should only be used when delocalization is being represented Curves represent delocalization yet lots of structures have some delocalized elements Although all such elements could be depicted with curves there is little gained by doing so On the contrary the arbitrary use of curves can draw the viewer’s attention to insignificant portions of a structural diagram and away from areas that are chemically more important such as an active site or a reactive group Accordingly curves should only be used when the delocalization is specifically being highlighted as an important feature of the structure When curves are not used any alternating configuration of double bonds is acceptable within the further constraints discussed in GR-3 5 The text then goes further to note It is generally not acceptable to use curves in two adjacent fused rings since such diagrams are at best ambiguous in terms of the character of the shared fusion bond between the two rings Graphical representation standards for chemical structure diagrams (IUPAC Recommendations 2008) Pure Appl Chem 2008 DOI 10 1351/pac200880020277 PS hmm the leader for the IUPAC report works at CambridgeSoft…
4106,hemija,Although I can't think of any drug examples other than thalidomide here's information on thalidomide's mechanism The chiral carbon of thalidomide can tautomerize in basic conditions into an enol which is achiral A reversal back to the ketone results in a mix of (R) and (S) enantiomers In the body this tautomerization is generally catalyzed by basic amino acids Specifically albumin is the main catalyst in humans While this is beyond the scope of your question the reason that only (S) -thalidomide causes birth defects is that it can insert itself into DNA and suppresses certain genes necessary for embryonic development Reddy - Chirality in Drug Design and Development http //www ncbi nlm nih gov/pubmed/9860497 http //www ncbi nlm nih gov/pubmed/9499573
4338,hemija,Some factors were hinted but let me put them in an order of importance and mention some more metals generally have a high melting point because metallic interatomar bonding by delocalized electrons (Li having only few electrons for this electron sea ) between core atoms is pretty effective in those pure element solids compared to alternative bonding types (ionic 6-20 eV/atom bond energy covalent 1-7 metallic 1-5 van-der-waals much lower) Also ionic lattices like NaCl have a higher lattice and bonding energy they have weak interatomar long-range bonding unlike most metals They break apart or are easily solvable metals are malleable but don't break the electron sea is the reason for their welding ability the crystal structure and mass plays a inferior role among your filtered elements (just look up the crystal structure of those elements) as metallic bonding is not directional unlike covalent bonding (orbital symmetry) Metals often have half filled s and p bands (stronger delocalized than d and f) at the fermi-edge (meaning high conductivity) and therefore many delocalised electrons which can move into unoccupied energy states yielding the biggest electron sea with half or less fill bands noble metals like Au Ag have a full d orbital therefore low reactivity/electronnegativity and are often used as contact materials (high conductivity because of very fluid electron sea consisting only of s-orbital electrons Unlike Tungsten with half or less occupied d-orbitals they show no interatomic d-d bonding by delocalized d-electrons and more importantly a half filled d-orbital contributes 5 electrons to the energy band while a s only 1 p only 3 the electron sea is bigger among the d-group The packaging of core atoms in the lattice (interatomic distance) among the high Z atoms (compared to e g Li) is more dense (more protons stronger attraction of shell electrons smaller interatomic radius) means stronger interatomar bonding transmitted by the electron sea You can see here that in each series (Li Na Ka) the melting points rise to a maximum and then decrease with increasing atomic number (lacking unoccupied energy states for delocalized d-electrons) bigger electron sea being here a stronger factor than a bit more dense packaging Boron as a semi-metal shows metallic and covalent bonding Carbon strong directional covalent bonding and is able to build network of bonds unlike other non-metal elements showing covalent intramolecular bonding e g in diatomic molecules but not strong intermolecular bonding in macromolecules because of lacking unpaired electrons So there are some bigger trends for melting points explaining the high melting points of d-metals but also some minor exceptions to the rule like Mn
4186,hemija,In the definition of an s shell you will find that its l (letter ell ) number is zero In classical terms that corresponds to an orbit with zero orbital or angular momentum — which for a large object is a clear impossibility For an electron it gives the peculiar result that any electron in any s shell is classically speaking moving back and forth through the nucleus rather than around it So in one curious sense what you just asked is exactly what happens the classical analogy is that the electrons do go through the nucleus which is why they have such a lovely spherical symmetry The second part of the answer however is that electrons can't go through the nucleus unless they are hugely more energetic than the ones found in a typical small-nucleus atom Clearly there's a bit of a paradox going on there The resolution of the paradox is that very-low-mass charged particles must be treated by quantum rules So for example rather than the electron behaving like a well-defined particle it behaves like a standing wave That standing wave can in turn be thought of as two simultaneous versions of the electron one going (for example) clockwise and the other counterclockwise around (The real situation has an infinite number of such components; I'm just picking out a single pair of them that demonstrate the principle ) Each of these components can furthermore be thought of as being refracted by the powerful spherical charge field of the nucleus curving around it without every striking it This refraction is not the same as an attraction In fact it is this refraction effect that prevents the density of the electron cloud from reaching infinity at the nucleus — that is from striking the nucleus If you think of how a tank of water can cause a beam of light to bounce off the surface instead of entering the tank — and that's a terrible analogy I know I know — you can at least get some idea of how an increasing optical density towards a central point might keep light away rather than bringing it closer So for an electron that behaves like two waves going both clockwise and counterclockwise the combined waves curve around the nucleus rather than striking it This is a very quantum sort of event since for a classical object such splitting of the object is simply not possible and the object simply dives straight into the source of attraction But if objects are light enough that kind of particle-like behavior simply ceases to be available to the object Instead you get waves that neatly and with perfect spherical symmetry curve around the nucleus never obtaining enough energy (which makes it more particle-like) to connect directly with that nucleus Finally notice that electrons in s (and other) shells necessarily combine multiple paths all at the same time For each image of the electron that is traveling clockwise there must also be an exactly balancing image of the same electron traveling counterclockwise so that the two images always balance out to zero orbital momentum What an amazing thing that is And an important thing too since it's what makes chemistry possible So good question even if it really is more of a physics question per se than a chemistry question But it's such an important chemistry question It's like asking how the engine that powers the car works You can accept it as a given that all cars and vehicles have engines and that they all work in a certain way However sometimes it's nice to dive in a little deeper and try to understand why these peculiar things do the things that make chemistry possible — that is how the engine really works
4150,hemija,In addition to the general rules of how electronic configurations of atoms and ions are calculated the elements from the $d$ block (aka the transition metals ) obey one special rule In general electrons are removed from the valence-shell s orbitals before they are removed from valence d orbitals when transition metals are ionized (I took this formulation from this online lecture notes but you will find equivalent statements in your textbooks ) So what that does mean is that if you remove electrons from vanadium (0) you will remove the 4 s electrons before you remove the 3 d electrons So you have the following electronic configurations V is [Ar] 4 s 2 3 d 3 V 2+ is [Ar] 4 s 0 3 d 3 V 3+ is [Ar] 4 s 0 3 d 2 V 4+ is [Ar] 4 s 0 3 d 1 V 5+ is [Ar] 4 s 0 3 d 0 And thus V 3+ is paramagnetic because it has two unpaired 3 d electrons In fact all the ions above are paramagnetic except V 5+
4160,hemija,As I understand this there are basically two effects at work here When you populate an s orbital you add a significant amount of electron density close to the nucleus This screens the attractive charge of the nucleus from the d orbitals making them higher in energy (and more radially diffuse) The difference in energy between putting all the electrons in d orbitals and putting one in an s orbital increases as you fill the d orbitals Additionally pairing electrons in one orbital (so adding the second s electron) carries a significant energy cost in terms of Coulombic repulsion because you're adding an electron essentially in exactly the same space as there's already an electron I'm assuming that the effect isn't strong enough to avert fluorine having a $2s^2$ occupation and if you look at gadolinium the effect there isn't strong enough to stop the s from filling (large nuclear charge and orbital extent at the nucleus is a good combination energy-wise) it does manage to make it more favourable to add the electron into the 5d instead of the 4f orbitals Also if you take a look at tungsten vs gold there the effect isn't strong enough for tungsten to avoid a $6s^2$ occupation but is for gold - more d electrons making the screening effect overcome the strong nuclear charge and enhanced nuclear penetration of an s orbital
4376,hemija,I think your question implicates another question (which is also mentioned in some comments here) namely Why are all energy eigenvalues of states with a different angular momentum quantum number $\ell$ but with the same principal quantum number $n$ (e g $3s$ $3p$ $3d$) degenerate in the hydrogen atom but non-degenerate in multi-electron atoms Although AcidFlask already gave a good answer (mostly on the non-degeneracy part) I will try to eleborate on it from my point of view and give some additional information I will split my answer in three parts The first will address the $\ell$-degeneracy in the hydrogen atom in the second I will try to explain why this degeneracy is lifted and in the third I will try to reason why $3s$ states are lower in energy than $3p$ states (which are in turn lower in energy than $3d$ states) $\ell$-degeneracy of the hydrogen atoms energy eigenvalues The non-relativistic electron in a hydrogen atom experiences a potential that is analogous to the Kepler problem known from classical mechanics This potential (aka Kepler potential) has the form $\frac{\kappa}{r}$ where $r$ is the distance between the nucleus and the electron and $\kappa$ is a proportionality constant Now it is known from physics that symmetries of a system lead to conserved quantities ( Noether Theorem ) For example from the rotational symmetry of the Kepler potential follows the conservation of the angular momentum which is characterized by $\ell$ But while the length of the angular momentum vector is fixed by $\ell$ there are still different possibilities for the orientation of its $z$-component characterized by the magnetic quantum number $m$ which are all energetically equivalent as long as the system maintains its rotational symmetry So the rotational symmetry leads to the $m$-degeneracy of the energy eigenvalues for the hydrogen atom Analogously the $\ell$-degeneracy of the hydrogen atoms energy eigenvalues can also be traced back to a symmetry the $SO(4)$ symmetry The system's $SO(4)$ symmetry is not a geometric symmetry like the one explored before but a so called dynamical symmetry which follows from the form of the Schroedinger equation for the Kepler potential (It corresponds to rotations in a four-dimensional cartesian space Note that these rotations do not operate in some physical space ) This dynamical symmetry conserves the Laplace-Runge-Lenz vector $\hat{\vec{M}}$ and it can be shown that this conserved quantity leads to the $\ell$-independent energy spectrum with $E \propto \frac{1}{n^2}$ (A detailed derivation though in German can be found here ) Why is the $\ell$-degeneracy of the energy eigenvalues lifted in multi-electron atoms As the $m$-degeneracy of the hydrogen atom's energy eigenvalues can be broken by destroying the system's spherical symmetry e g by applying a magnetic field the $\ell$ degeneracy is lifted as soon as the potential appearing in the Hamilton operator deviates from the pure $\frac{\kappa}{r}$ form This is certainly the case for multielectron atoms since the outer electrons are screened from the nuclear Coulomb attraction by the inner electrons and the strength of the screening depends on their distance from the nucleus (Other factors like spin and relativistic effects also lead to a lifting of the $\ell$-degeneracy even in the hydrogen atom ) Why do states with the same $n$ but lower $\ell$ values have lower energy eigenvalues Two effects are important here The <stron
4155,hemija,There are several ways that enantiomers can be separated but none of them are particularly simple The first way to separate them is chiral chromatography In chiral chromatography silical get is bonded to chiral molecules to form what is called a chiral stationary phase The enantiomers will then separate as they run down the column because one of the enantiomers will interact more strongly with the column and stick in place Chiral sugars (ex cellulose) are frequently used in chiral chromatography The second common method is to react the enantiomers with another chemical to form diastereomers While enantiomers are identical in terms of chemical properties diastereomers are not Diastereomers can be created by reacting a mixture of both the enantiomers with another chiral molecule For example s -brucine is commonly used because it is cheap Diastereomers have different chemical properties (for example melting points) so it is much easier to separate them Then after separation the enantiomers can be recovered from the single diastereomer
4322,hemija,It's not exactly clear from your question what distances you consider but I have taken snapshots of a methylcyclohexane with the methyl group in the equatorial and axial positions       (equatorial)    (axial) The distance of the methyl H atoms to the nearest cyclohexane H atoms are displayed and you'll see that indeed the axial position has a shorted H–H distance of ~ 2 Å while equatorial position has longer H–H distances Also as others have remarked in the comments only the groups (and H atoms) directly on the cyclohexane ring are said to be in axial or equatorial position Atoms of side chains do not follow this terminology in particular for side chains that can rotate freely as it means nothing there
8681,hemija,If you're still interested in this fluorescence may be a good method to test the phosphate dependence on magnesium activity First record a fluorescence spectrum of a solution of magnesium and fluorescent dye then add small volumes of concentrated phosphate and note any changes in the fluorescence Of course you would have to do a control experiment to see if the phosphate is directly perturbing the fluorescence This write-up describes some magnesium-chelating dyes that you could use though if you don't want to spend a lot of money on a dye you may already have some chemicals in your lab that will work Phenol looks like promising alternative as it fluoresces is decently water soluble (83 g/L) and forms a complex with magnesium which breaks planarity which would certainly change the fluorescence spectrum It would only take a day to get accurate quantitative results if you have access to a fluorometer
4166,hemija,This exception rule is actually orbital filling rule For two electrons to be in same orbital they need to have different spins (Pauli exclusion principal) This electron pairing requires additional energy and thus it is easier to add electrons if there are free orbitals When element has a half-filled p sublevel all 3 orbitals have one electron and pairing takes place (difference between energy levels of 2p and 3s is greater than electron pairing energy) Electron pairing effects have significant impact to physical properties of coordination complexes (like color and magnetic properties)
4165,hemija,The two hydrogens are the same but some periodic tables show hydrogen in both places to emphasize that hydrogen isn't really a member of the first group or the seventh group Hydrogen is a diatomic gas in it's elemental state which is different from the other group one metals (and similar to the group seven elements) At the same time hydrogen usually loses its electron similar to sodium and the other group one metals
4171,hemija,Well you have to ask how stable and compared to what Epoxides including ethylene oxide are generally considered relatively unstable molecules with a high chemical activity and involvement in numerous reactions including polymerization and thermal decomposition However ethylene oxide does exist as a molecule as does cyclopropane which also contains a three-membered ring So let's compare for example the ring strain in ethylene oxide and cyclopropane I found some computation data on their relative stability from these lecture notes which indicate that “ethylene oxide has less ring strain than cyclopropane” I would attribute this to the fact that the C–O–C angle has less strain because its “relaxed” value would be 104 5° compared to the “relaxed” C–C–C angle of 109 4°
4183,hemija,In short I think the answer is that although the overall symmetry of the buckminsterfullerene is almost spherical the quadrupole–quadrupole interaction with other molecules with aromatic cycles are local (in addition to the ever-present van der Waals interactions) I tried to think of a way to exemplify that and drew the 2D picture below In this 2D model the “double bonds” of the circular structure represent the individual aromatic rings of your buckminsterfullerene ( which isn't superaromatic i e electrons are not delocalized over its entire structure ; think may be a key point) bearing each a local quadrupole moment (symbolized in the red/blue thingy) The other particle with a quadrupole is close to one of the ball's quadrupoles and this local interaction dominates In fact other interactions maybe be stabilizing or not depending on the orientation of the individual quadrupoles but they are weaker that the local quadrupole–quadrupole interaction
4451,hemija,Having embarrassed myself with my first answer let me try this again Ammonia and water hydrogen bond exceptionally well In fact the $\ce{O-H N}$ value is 29 kJ/mol while the $\ce{O-H O}$ value is 21 kJ/mol ( Ref ) So if there is any water around ammonia would be happy to become solvated as they recondense Also ammonia is a weak base and reacts with water (which is not usually the case when you're doing a distillation) $$\ce{NH3 + H2O &lt;=&gt; NH4+ + OH-}$$ So if there's any water around as the ammonia is distilling and they recombine they would also react which would make it difficult to remove the last of the water (Some reaction would occur because the lowest energy point of the equilibrium is not at reactants or products ) So as noted in the comments distillations to prepare dry ammonia use a scrubber to remove the water
4181,hemija,The boiling point of a liquid depends on the intermolecular forces present between the atoms or molecules in the liquid since you must disrupt those forces to change from a liquid to a gas The stronger the intermolecular forces the higher the boiling point Two oxygen molecules are attracted to each other through London dispersion forces (induced temporary dipoles between the molecules) while water molecules are attracted to each other by hydrogen bonding (attraction of the + dipole on H in one molecule to the – dipole on an oxygen in an adjacent molecule) that is relatively strong (Hydrogen bonding is an important intermolecular force for molecules where H is directly covalently bonded to F O or N which are quite electronegative and thus form bond with H with a relatively strong dipole ) London dispersion forces become more important for atoms and molecules with more electrons Dipole–dipole attractions are also important in some molecules
4188,hemija,It should be noted that ammonia is a mildly polar solvent and that (for instance) you can prepare $\ce{[Cu(NH3)6]^{2+}}$ by reaction with liquid ammonia 1 The description of the geometry (two pages earlier) of $\ce{CuSO4\cdot{}5H2O}$ (namely containing a square planar $\ce{[Cu(H2O)4]^{2+}}$ unit with two additional $\ce{[SO4]^{2-}}$ above and below the plane) suggests that ammine ligands could fulfill a similar role to water Bear in mind that passing ammonia gas over a solid will probably mean that if the reaction happens it will only happen near grain surfaces To answer your second suggestion Housecroft and Sharpe 1 identifies a reaction producing $\ce{[Ag(NH3)_4]^{+}}$ from $\ce{Ag2O}$ in liquid $\ce{NH3}$ Hope this is informative [1] Housecroft C E and Sharpe A G ; Inorganic Chemistry 2e; Pearson Prentice Hall; 2005 pp 635-637 693
4192,hemija,Cartesian Space In Cartesian space three variables (XYZ) are used to describe the position of a point in space typically an atomic nucleus or a basis function To describe the locations of two atomic nuclei a total of 6 variables must be written down and kept track of The general ruling is that for Cartesian space 3N variables must be accounted for (where N is the number of points in space you wish to index) Internal Coordinates Z-matrices use a different approach When dealing with Z-matrices we keep track of the relative positions of points in space Cartesian space is 'absolute' so to speak A point located at (0 0 1) is an absolute location for a coordinate space that extends to infinity However consider a two atom system The translation of the molecule through space (assuming a vacuum) will have no affect on the properties of the molecule An H2 molecule centered around the origin (0 0 0) is no different from the same H2 molecule being centered around (1 1 1) However say we increase the distance between the hydrogen atoms We now have altered the molecule in such a way that the properties of that molecule has changed What did we change We simply changed the bond length one variable We increased the distance between the two atoms by some length R With Z-matrices we keep tabs on internal coordinates bond length (R) bond angle (A) and torsional/dihedral angle (T/D) Using internal coordinates reduces our 3N requirement set by the Cartesian space down to a 3N-6 requirement (for non-linear molecules) For linear molecules we keep tabs on 3N-5 coordinates When performing complex computations the less you have to keep track of the less expensive the computation Symmetry Consider the following molecule H2O We know from experience that this molecule has C2V symmetry The OH bond lengths should be equivalent When using some sort of optimizing routine you may want to specify symmetry in your system With a Z-matrix the process is very straightforward You would construct your Z-matrix to define the OH(1) bond as being equivalent to the OH(2) bond Whatever program you use should automatically recognize the constraint and will optimize your molecule accordingly giving you an answer based off a structure that is constrained to C2v symmetry With Cartesian space this is not guaranteed Rounding errors can cause your program to break symmetry or your program may not be very good at guessing the point group of your molecule based on the Cartesian coordinates alone Picking the Right One As a preface programs like Gaussian convert your Cartesian coordinate space (or your pre-defined Z-matrix) into redundant internal coordinates before proceeding with an optimization routine unless you specify it to stick with Cartesians or your Z-matrix I warn you that specifying your program to optimize using Cartesian coordinates makes your calculation much more expensive I find that I will explicitly specify 'Z-matrix' when I know I'm dealing with high symmetry and when I know my Z-matrix is perfect You will want to use Z-matrices on systems that are rather small If dealing with systems with high symmetry Z-matrices are almost essential They can be rather tricky to implement and you will likely spend some time figuring out the proper form of your Z-matrix through trial-and-error If you wish to scan a particular coordinate Z-matrices are also very helpful as you can tell a program to scan across a bond length angle or torsion with ease (as long as you've properly defined that coordinate in your Z-matrix) I use Cartesian coordinates for large systems systems with very little or no symmetry or when I'm in a hurry
4193,hemija,What you're describing is thermoluminescence which is used as a paleochronology method (aka Trapped Electron (TE) dating afaik) The wikipedia article has a good synopsis Simply electrons get bound to potential wells generated in the radiation-defected structure When you heat the crystal up you allow it to anneal and return to a more regular structure The defected crystal is stuck in an energetic local minimum You can also do this trick with X-rays Calcite (or was it aragonite ) exposed to X-rays will turn light blue for a while
4524,hemija,This difference between living and non-living polymerizations is small in words but large in effects In principle cationic and anionic polymerizations can be living In practice it is not as easy as it looks on paper See below A living polymerization is any chain (or addition) polymerization that is prevented from terminating i e the ends of the chains are still reactive In a living polymerization chain initiation has one initiation and propagation continues as long as there is monomer Termination (almost) never happens If your polymerization stops because your monomer runs out adding more monomer restarts the process and the existing chains get longer The number of chains formed matches very closely with the number of equivalents of initiator and the length of the chains depends on the ratio of monomer to initiator Most chain polymerizations (radical anionic cationic transition-metal insertion and ring-opening) do not behave this way There is a mechanism for termination that becomes increasingly kinetically favored as the concentration of free monomer decreases Sadly the wikipedia article for chain termination is pretty sad Most chain polymerizations propagate and terminate quickly with termination usually initiating the next chain Thus in a non-living polymerization new chains are constantly being initiated and old chains are constantly terminating until their is no more monomer In these cases adding additional monomer later would not accomplish anything because the chain ends are dead Experimental evidence for this phenomenon includes the formations of more chains than there were equivalents of initiator The length of the chain is more dependent on the relative rates of propagation and termination than on the initial ratio of monomer to initiator For radical polymerization termination mechanisms include fragmentation by &beta;-hydrogen abstraction or by recombination Both of these mechanisms are inherent in the radical nature of the polymerization and so are not preventable by exclusion of other species Radical polymerizations are made living by the addition of species that protect the radical at the end of the chain to prevent it from terminating Examples include ATRP and RAFT The wikipedia articles for both are pretty good For cationic polymerization termination mechanisms include &beta;-hydrogen abstraction chain transfer and attack by nucleophiles like water In principle a cationic polymerization could be a living polymerization if great pains were taking to exclude all species which might induce termination In practice this is doable but difficult as termination by chain transfer is always possible See the wikipedia article on living cationic polymerization The article is not great but suggests that cationic polymerizations done in nonpolar solutions (not in bulk) with nonnucleophilic Br&oslash;nsted or Lewis acids at low temperatures can become living polymerization For anionic polymerization termination mechanisms include protonation by protic molecules chain transfer and reaction with electrophiles which can include O 2 in some reactions Like cationic polymerizations anionic polymerizations can be made living by rigorous exclusion of terminating species Again this is difficult due to the possibility of chain transfer termination However with the correct choice of conditions it is doable as suggested by the wikipedia page for living anionic polymerizations Another way to accomplish some anionic polymerizations in a living fa
4218,hemija,Intermediate is a middle step in the chemical process reactants -> intermediate(s) -> products Benzyne is formed from benzene and is highly reactive Thus it is never the first or last step of the process To my knowledge benzyne is rarely observed as a product because its reacting fast with other molecules because its triple bond What makes a molecule an intermediate It is a highly reactive middle product of a reaction See here and there
4219,hemija,A general rule is crap goes in crap comes out A large-sample low-field 1D NMR at room temperature is usually only minimally affected by using a cheap NMR tube There are important differences though and I'll highlight a few The first distinction between prices is what the tube is constructed from quartz obviously costs more than borosilicate Why would a chemist ever use the more expensive quartz You can heat/cool quartz faster (nice for thermal studies) the UV cutoff is lower (think 190 nm opposed to 320nm) which is important for photolysis you can work with quartz at higher temperatures (around 1300 deg C instead of 250 C) and the purity of quartz is better controlled than your typical pyrex There are different grades of quartz fused and synthetic and there are different grades of borosilicate such as the high-quality pyrex or the lower-quality Class B each comes with its own limitations as far as purities are concerned and so forth Three more important parameters have to do with the manufacturing of your tube are concentricity camber and wall thickness Lower quality tubes will tend to have less precision &amp; accuracy over each of these parameters and as a result your sample may wobble while spinning (introducing problems such as modulation sidebands) A particularly bad tube can hit your RF coils and cause damage to your probe over time slowly or quickly if it is ignoring any reasonable standard -- even more apparent for a tube at this level of quality is that it may be easier to break while acquiring your sample and we all should be aware of how much fun that is for everybody involved Shimming can deal with impurities present in the glass (such as ferric oxide) and increased impurities in the glass/inhomogeneities will result in taking longer to get a good shim Time is money A lot of these things have lower tolerances in more complex experiments and at higher fields It really does depend on your particular experiment and what you're hoping to get out of it
4227,hemija,This reference indicates that primary bonds in a polymer are the intramolecular bonds (the covalent bonds) that hold the polymer together Cross-linking different chains of polymer together does occur through a covalent bond such as a disulfide bond The secondary bonds that help to give the polymer its physical properties are intermolecular forces such as hydrogen bonding for polymers that contain hydroxyl or primary or secondary amine groups These secondary bonds can be disrupted by changing solvent conditions or heating for example which wouldn't disrupt the primary bonds So I suppose that you could say loosely that there are two kinds of bonds between polymers in solid material if you mean between the polymer chains before they have an opportunity to cross-link (by a change in pH for example) After they cross-link the physical properties of the bulk sample of the polymer will still be affected by secondary bonds the intermolecular interactions between the cross-linked chains The primary bonds will be stronger and can only be affected by chemical changes the secondary bonds (intermolecular forces) can be affected by changing the physical conditions
4223,hemija,This answer to this question applies to your query Many physical properties depend on the intermolecular forces experienced in the liquid or solid These intermolecular forces are hydrogen bonding (strongest) dipole-dipole attractions (in polar molecules) and London dispersion forces (weakest of the forces when comparing molecules of the same size but dependent on the number of electrons in the substance) Stronger intermolecular forces would make the substance less volatile As noted in the first response methanol is more volatile than ethanol Methanol and ethanol would both have both hydrogen bonding (a relatively strong type of dipole-dipole attraction) and London dispersion forces Because ethanol has more electrons (because it is a bigger molecule but not necessarily because it is a heavier molecule) it would have more London dispersion forces in comparison to methanol so with stronger intermolecular forces it would be less volatile So this example follows the general guidelines--figure out the intermolecular forces present in the molecules you want to compare and the molecule with the weakest cumulative intermolecular forces will be the most volatile Intramolecular bonding (covalent bonds when you're talking about molecules) does not affect physical properties
4224,hemija,The 2 refers to DOT (Department of Transportation) hazard class 2 This reference describes the different DOT hazard classes As summarized in the comments Class 2 (compressed gas) is divided into three subclasses (2 1 - flammable 2 2 - non-flammable non-toxic and 2 3 - toxic) I imagine the diamonds for Class 2 don't contain the subclass number because there is a different diamond colour/symbology for each subclass (as opposed to for instance 5 1 (oxidising agent) and 5 2 (organic peroxide) which have the same symbol and colours)
4229,hemija,Look carefully it's (distorted) tetrahedral--four groups at nearly symmetrically positions in 3D space{*} So the hybridization is $sp^3$ As you can see the shape is distorted but it's tetrahedral Technically the banana bonds can be said to be made up of orbitals similar to $sp^3$ but not exactly (like two $sp^{3 1}$ and two $sp^{2 9}$ orbitals--since hybridization is just addition of wavefunctions we can always change the coefficients to give proper geometry) I'm not too sure of this though $\ce{B}$ has an $2s^22p^1$ valence shell so three covalent bonds gives it an incomplete octet $\ce{BH3}$ has an empty $2p$ orbital This orbital overlaps the existing $\ce{B-H}$ $\sigma$ bond cloud (in a nearby $\ce{BH3}$) and forms a 3c2e bond It seems that there are a lot more compounds with 3c2e geometry I'd completely forgotten that there were entire homologous series' under 'boranes' which all have 3c2e bonds (though not the same structure) And there are Indium and Gallium compounds as well Still group IIIA though these are metals I guess they like $\ce{Al}$ still form covalent bonds So the basic reason for this happening is due to an incomplete octet wanting to fill itself Note that banana is not necessarily only for 3c2e bonds Any bent bond is called a banana bond Regarding similar structures $\ce{BeCl2}$ and $\ce{AlCl3}$ come to mind but both of them have the structure via dative(coordinate) bonds Additionally $\ce{BeCl2}$ is planar Sneaks off and checks Wikipedia Wikipedia says $\ce{Al2(CH3)6}$ is similar in structure and bond type I guess we have less such compounds because there are comparatively few elements ($\ce{B}$ group pretty much) with $\leq3$ valence electrons which form covalent bonds(criteria for the empty orbital) Additionally $\ce{Al}$ is an iffy case--it like both covalent and ionic bonds Also for this geometry (either by banana bonds or by dative bonds) I suppose the relative sizes matter as well--since $\ce{BCl3}$ is a monomer even though $\ce{Cl}$ has a lone pair and can form a dative bond *Maybe you're used to the view of tetrahedral structure with an atom at the top Mentally tilt the boron atom till a hydrogen is up top You should realize that this is tetrahedral as well
4234,hemija,In short the definition of a chemical bond is not unique and a clearly-drawn line The simplest and most common definition is the sharing of electrons between two or more nuclei In contrast other interactions are often said to be intermolecular (which is somewhat more specific than the term “physical” In a longer commentary I see can have five different types of definition of the chemical bond (vs intermolecular interactions) Let's start from the beginning in this case using the words of Linus Pauling winner of the 1954 Nobel Prize for “determining the nature of the chemical bond linking atoms into molecules” In The Nature of the Chemical Bond (1960) he gives the following definition            A bond is what links atoms into molecules and molecules are defined at the discretion of the chemist You can find the same definition still in use in some high-school textbooks but it isn't very helpful… The complete opposite consider all interactions as chemical bonds whose strenght can vary I actually hadn't heard that one before I researched textbooks to write this answer but you can find it in some textbooks like this one              This view has some grounding because all interatomic interactions stem from the behaviour of the system's electrons (in addition to nuclei–nuclei Coulombic forces) However it does not allow to make a strong distinction between interactions whose energies differ by orders of magnitude Chemists like molecules and they like categorizing things between intra- and inter-molecular as it's a nice model (making it easier for our mind to handle) You can classify interactions by energy decide that chemical bonds are those interactions that have an energy higher than a certain threshold let's say 50 kJ/mol This makes things clean and makes sure that you can easily classify interactions However the choice of a threshold is problematic Finally what I believe is the most common description is to look at the nature of the interaction and classify it following a certain convention The two other answers so far have focused on this part and listed the various “usual” types of bonds and intermolecular interactions so I won't say more on that I said five types right Well the fifth is mine of course Not only mine but that of the New Oxford American Dictionary as well which I quite like chemical bond a strong force of attraction holding atoms together in a molecule or crystal resulting from the sharing or transfer of electrons Short and powerful What I like in that is that it gives a general prescription allowing one to argue individual cases and not based too much on convention What are the features of a chemical bond Well it has to be an attractive force between atoms sure… but I think the most relevant criterion of all is sharing (or transfer) of electrons That is after all what chemistry is about description of electronic clouds around two or more atoms And I think when this criterion is applied to the list of interaction types commonly classified it works quite well (whithout being dogmatic) Also what I like in it is that a given interaction type can be considered one way or another depending on its strength The best
4237,hemija,The answer appears to be right there in the linked synthesis details - the nickel is non-catalytic and is only there to generate a passivated surface that won't degrade in the high-temperature fluorine atmosphere [Edit] The plot thickens this paper suggests that xenon may in fact abstract fluorine off the passivated nickel fluoride surface due to observed zeroth order kinetics which lends some serious credence to jonsca's original inference of Ni being catalytic This will take some more investigation
4339,hemija,Classical hybridization theory does not allow for noninteger hybridizations However ab initio calculations can be interpreted using a bond order analysis method such as NBO where the MO coefficients are used to provide the closest analogue possible to a classical hybridization picture For example one of the pure $sp$ orbitals in Pauling's valence bond theory (where hybridization was first introduced) has wavefunction $$ \phi_{sp} = \frac{1}{\sqrt 2} \phi_{s} + \frac{1}{\sqrt{2}} \phi_{p_x} $$ Taking the square of the coefficients this orbital is $\frac 1 2$ s character and $\frac 1 2$ p character i e it is an $sp$ orbital The basic idea of a bond order analysis method is to reexpress a molecular orbital into a form similar to $$ \phi = c_1 \phi_{As} + c_2 \phi_{Ap_x} + c_3 \phi_{Ap_y} + c_4 \phi_{Ap_z} + \dots$$ If the other coefficients are very small the ratio $$ n = \frac {c_2^2 + c_3^2 + c_4^2}{c_1^2} $$ would yield a number that could be used interpret $\phi$ as a $sp^n$ orbital on atom $A$ Edit As for inferring hybridization states from a direct inspection of nuclear geometries there is in principle no such direct relationship in electronic structure theories that are more sophisticated than VSEPR The former is an electronic property whereas the spatial arrangement of atomic nuclei are not and the relationship between the two becomes much more complicated
4334,hemija,What you describe is unfortunately a very common misconception Defining orbitals does not break spatial symmetry It is still completely arbitrary which directions you define as $x$ $y$ and $z$ and thus it is entirely arbitrary how you orient the $2p$ orbitals in space Therefore rotational invariance is still preserved The other thing to remember is that all the $p$ orbitals are (essentially) degenerate and so you can take any linear superposition of them you want For example you can write down an orbital that looks like $$\frac{1}{\sqrt{3}}\left(\phi_{2p_x}+\phi_{2p_y}+\phi_{2p_z}\right) $$ and placing an electron in this orbital does not break rotational symmetry
4241,hemija,If your compound has been synthesized before you can measure its optical activity and compare it with the data in the literature If your compound hasn't been synthesized before it can be possible to differentiate between two enantiomers with NMR by measuring the spectra of your molecule in a chiral environment Here's a review article on how to resolve enantiomers with NMR Otherwise diastereomers can be differentiated easily By looking at your molecule you can (sort of) predict which spectrum it should give rise to by using the concepts of enantiotopic homotopic heterotopic and diastereotopic protons We had to analyse and solve 2D-NMR spectra ( 1 H and 13 C) of diastereomers last year and to a beginner I have to say it was a challenge As always it boils down to forming diastereomers EDIT Take a look at these diasteromers Without getting into much detail (I've got finals coming up) it's easy to see that the hydroxyl will shift the ethyl's CH 2 peaks towards lower field (higher ppm) in the second isomer The interpretation of chiral media NMR spectra would have to be similar My impression is that the acquisition of chiral media spectra is more involved but the interpretation isn't I might be totally off track though Someone might want to share a more thorough answer
8555,hemija,So I ask what are the trends leading to the brittleness of a metal Brittle materials absorb very little energy before fracturing The only trend that can be linked to this is the strength of bonding in the metal (Other factors like crystal structure and purity do not have any trends and so there are a lot of exceptions) This also means that the trend in brittleness is not very regular since it is complex and depends on too many other factors why are these trends there (usually trends can be explained in terms of simpler things like atomic size/sheilding/etc) Since brittleness cannot be explained by a very simple trend I will explain what reasons I have found The stronger the bonding in the metal the more difficult it is to change its structure and thus it tends to break instead of deforming Here's a graph (melting point vs Group number) that gives a general measure of strength of metallic bonding in all the transition elements of the first three series According to the graph we predict that Tungsten (Period 6 Group VIB) should the most brittle and this is true for impure tungsten Again this successfully predicts that Silver Gold and Copper (Group IB) are least brittle (and so most malleable and ductile) Also Lanthanum (Period 6 Group IIIB) is predicted to be quite malleable and this is also true Note that this also explains the high malleability of alkali metals (the graph is not very clear at that point) There may be much more exceptions to this trend when examined closely so take this only as an approximation Crystal structures also play an important role in brittleness but they cannot be explained by a trend Purity of a metal tends to increase malleability and decrease brittleness because there are less points at which a fracture can take place
4244,hemija,This can be done by first reating the phenol with $\ce{HNO2}$ Which gives us this Then reacting with dil $\ce{HNO3}$ we get 90% yield of para nitro phenol This happens because $\ce{N=O}$ has a very strong $- I$ effect thus it concentrates the electrons more towards the para position so on reacting with dil $\ce{HNO3}$ we get max yield of para P S - Another way of getting only para is using a protecting agent but i cant recall the reaction now I will post it later
4247,hemija,I'd really reccomend the free Marvin suite from ChemAxon MarvinSketch (the actual drawing package) hasn't disappointed me in the past and has a wide feature set In addition to just being able to draw compounds in Lewis format the software includes plugins to name what you have drawn (systematic/traditional) predict properties change atom and bond properties generate stereoisomers and much more that I haven't got the experience to do anything more than experiment with You can view what you have drawn in Lewis form and also as a 3D structure should you wish to It supports many chemical drawing filetypes Here is the product page including brochure and technical details It is Java based so works on Mac Windows Linux and is even available on the web (but is quite slow to generate properties etc - I'd reccomend doing this locally)
4371,hemija,The signals you get in a simple 1D proton spectrum are in most cases roughly quantitative though there are some exceptions and some aspects you need to consider if you need high accuracy Acquisition There are several things you need to keep in mind while setting up the experiment to ensure that the resulting NMR spectrum will be quantitative The spectrum should be of high quality any artifacts and noise will affect the quantification Relaxation delay The relaxation delay is the time between experiments if it is too short some protons won't relax completely back to the equilibrium state This affects the intensity of the NMR signal and will cause the integrals of those signals to be off The relaxation delay (including the acquisition time) should be around $5 \cdot \mathrm{T_1}$ You can either estimate the $\mathrm{T_1}$ or measure it 13 C satellites The 13 C satellites are around 1% of the total signal if you need high accuracy you should use decoupling to get rid of those Other aspects You also need to ensure that the pulses used in the experiment have a reasonably flat excitation profile for the area you're looking at This is usually no problem for standard 1D proton experiments but might be a problem for other kinds of experiments or if you have an unusually long pulse and/or and unusually wide distribution of your chemical shifts Processing Baseline The baseline should be absolutely flat and at exactly zero any error there will lead to large errors in the quantification Using a digital filter during acquisition is a good idea for that ( baseopt option for Bruker spectrometers) this will ensure a very flat baseline at zero as long as no other problems distort the baseline Phasing The phase-correction should be exact it might be necessary to perform a manual phase-correction
4251,hemija,Well your question is equivalent to “what is it about ?-diazoketones that makes them so much more stable ” which is easier to see Compared to an alkyldiazo the ?-diazoketone has a resonance structure in which the negative charge goes to the ketone’s oxygen (and far away from the positively-charged nitrogen atom) Because the oxygen is a quite electronegative element the resonance form is quite stable and explains the extra stability of ?-diazoketones It is for the same reason that the protons in position ? to the ketones are always more acidic than alky chain protons Coming back to alkyldiazo compounds you have to realize that merely being able to write a resonance structure does not intrinsically imply stabilization the resonance structure has to have some intrinsic stability factor In the alkyldiazo the resonance form you wrote is a carbanion which is considered quite unfavourable unless it has a further stabilizing factor Moreover the most common reactions gives N 2 which is a very stable compound… the reaction is thermodynamically very favourable
4253,hemija,According to the MSDS for Jet Dry (a common brand name) it contains Sodium 2-phenylpropane-2-sulfonate (Sodium cumene sulfonate) Propane-1 2-diol Citric acid I think it may be safe to assume that the citric acid is in there for a clean smell and also to soften the water (thanks to the commenter) The sulfonate is presumably a surfactant to help break the surface tension of the water and promote evaporation and the propylene glycol may help serve as a coolant (and is favored due to low toxicity)
4265,hemija,Determining the rate of the kinetics of a reaction isn't done with only one measurement or looking at only one graph What zero order kinetics means is that the rate of the reaction is independent of the concentration of chemical species To determine the kinetics of a process you need to take a series of measurements while altering one variable namely the concentration of one of the chemical species under investigation In zero order kinetics you should observe that as you change the concentration of reactant for example the rate of the reaction remains constant
4268,hemija,Here's a draft First read this paper Considering we're under alkaline conditions (blood pH between 7 35-7 45) we'll take their alkaline mechanism proposal Mechanism This is obviously incomplete with good reason Without knowing what the active site of the enzyme looks like (except for Ser70) it's difficult to devise of a mechanism I could imagine another alkoxy attacking the sp 3 carbon between S and N and shifting two electrons to the sulfur thus generating your SO 2 - in one step but that would require a proper active site The geometry of the molecule inside the active site of the enzyme is obviously optimal for this reaction to proceed I'd like to know more For some reason my school doesn't give me access to this paper
4274,hemija,I'm not sure if this is what you're looking for (it may well still be too abstract) but oscillations can occur in the Gray-Scott system which is $$&#xD; A + 2B \to 3B\\&#xD; B \to P &#xD; $$ where $P$ is an inert product and the reaction is assumed to take place in a flow reactor that provides a supply of A giving rise to the dynamics $$&#xD; \frac{da}{dt} = f(1-a) - ab^2;\\&#xD; \frac{db}{dt} = ab^2 - (f+k)b \\&#xD; $$ where $f$ is a rate determined by the flow reactor $k$ is the rate of the $B\to P$ reaction and the rate constant of the autocatalytic reaction has been set to 1 without loss of generality by scaling $f$ and $k$ relative to it With the appropriate choice of the parameters $f$ and $k$ oscillation can happen because $B$'s concentration increases autocatalytically but then it overshoots its food source (i e the concentration of A) which then builds up again allowing the cycle to repeat You might not like the trimolecular step but I've found you generally get similar behaviour if you split it up into something like $$&#xD; A + B \to C + B\\&#xD; B + C \to 2B\\&#xD; B \to P &#xD; $$ (Simply using $A+B\to 2B$ doesn't work because its kinetics don't have the right sort of nonlinearity ) I would say that this has an advantage over the Ball model that you posted in that it obeys the laws of thermodynamics (At least from what you showed in your question it looks to me that Ball's model only oscillates because the reverse reactions have been neglected and if they weren't then it would have to go to equilibrium because it's a closed system ) It makes explicit that you need a power source (the supply of $A$) for oscillation and illustrates the connection between oscillatory behaviour and autocatalytic kinetics
4516,hemija,I think your problem with this reaction lies in the easily confused term syn and cis Both appear to mean that two functional groups are oriented in the same direction On a cyclic molecule syn and cis are the same However cis is a stereochemical descriptor and syn is a conformational descriptor Simplifying this reaction to an acyclic case may help Consider the meso isomer of 2 3-butanediol shown This isomer is drawn in the anti conformation which is probably the lowest energy for steric reasons This isomer has a syn conformation which looks like the following Note that the two -OH groups are not both oriented forward with solid wedges That drawing implies a high energy eclipsed conformation Now back to the reaction with HIO 4 In order to form the cyclic intermediate the -OH groups need to be close in space On an acyclic molecule this requires a syn conformation which most acyclic molecules can achieve even if it is not the lowest energy conformation In cyclic molecules particularly cyclohexanes the syn conformation might not have the -OH groups close together For cis -1 2-cyclohexanediol which is also syn respective to the plane of the ring to move the two -OH groups into the correct arrangement it would need to wiggle into the higher energy boat or twist-boat conformations However trans -1 2-cyclohexanediol which is anti has the two -OH groups oriented close together In a sense the trans isomer has the two -OH groups oriented syn with respect to the C-C bond that they share Thus trans -1 2-cyclohexanediol should react more readily with HIO 4 that cis -1 2-cyclohexanediol because the lowest energy conformer of trans -1 2-cyclohexanediol has the close arrangement of two -OH groups The cis isomer would have to change conformation which increases the energy input required increasing the activation energy and slowing the reaction rate The diaxial case (whether boat or chair) would definitely be unreactive
4273,hemija,Partition Coefficient P = [Organic] / [Aqueous] Where [] = concentration and Log P= log10 (Partition Coefficient) if log P value is higher then its more lipophilic ;those drugs were more absorbed by cells and elimination of those drugs from body will be less The partition coefficient is a ratio of concentrations of un-ionized compound between the two solutions so for a chemical which is having both hydrophobic and hydrophilic ends also have log P values Eg Oleic acid Log P = 7 64 chemicals which are hydrophopic(lipophilic) tends to have more Log P values and hydrophilic(lipophobic) chemicals have less log P eg pentane(log P=3 45)[lipohilic] 1-pentanol (log P=1 51)[lipophobic]
4277,hemija,Well sulfur in general and thiolates in particular are known for bonding very strongly to gold lead and mercury In the particular case of mercury which was known very early in the history of chemistry (indeed since the times of alchemy) this is the reason thiols are often called mercaptans from the Latin mercurium captans (capturing mercury) In Pearson's HSAB theory (hard and soft acids and bases) the reason the S-Hg bond is be stronger than the O-Hg can be explained because S 2- is a soft basis and Hg(II) is a soft acid making a good fit… while O 2- is harder Finally regarding the historical use of thiols to bind mercury (and lead to) let's just say a chelating agent for mercury and other toxic compounds was quite useful as an antidote You can read a very nice book on the historical use of mercury and lead in John Emsley's The Elements of Murder
4337,hemija,Yes you need additional quantities beyond the minimum necessary for calculating energies and the Hellmann-Feynman piece of the force when the wavefunction is not variational Here is a very rough sketch of why Puláy forces arise from applying the chain rule for calculating forces and was first discussed in the context of applying the Hellman-Feynman theorem Recall that the force is the change of energy with changes in (nuclear) coordinate and the energy is the expectation value of the (electronic) Hamiltonian $E = \left&lt;\psi\left|H\right|\psi\right&gt;$ Applying the chain rule to this $$-F = \nabla E = \left&lt;\psi \left\vert \nabla H \right\vert \psi\right&gt; + 2\left&lt; \nabla\psi \left\vert H \right\vert \psi\right&gt;$$ The first term is what you get from Hellman-Feynman and is the expectation of a one-electron operator of the first you have listed The second term goes away in Hellmann-Feynman only because it assumes the wavefunction is variational If you expand out the orbitals in the Slater determinant in some AO basis $\chi$ $$ \psi(r_1 r_2 r_N) = \phi(r_1) \wedge \phi(r_2) \wedge \dots \wedge \phi(r_N) $$ $$ \phi(r_1) = \sum_i c_i \chi_i(r_i) $$ then it is clear that $\nabla\psi$ generates terms which are MO coefficient derivatives (usually denoted $c_i^x$) and also AO derivatives ($\chi_i^x$) If the wavefunction were variational then by definition all these derivatives are zero If you work through the algebra you will find that there are some new quantities that need to be calculated in order to work out the non-Hellmann-Feynman theorem most notably the one-sided AO overlap derivative matrices $\left&lt;\chi_i^x\vert\chi_j\right&gt;$ which do not normally show up otherwise These terms turn out to be critical for getting correct ab initio molecular dynamics since the wavefunction is rarely (if ever) variational in time There are tricks one can do to reduce the cost of calculating MO coefficient derivatives but they cannot be obtained for free in general And there's really no way around calculating the overlap derivative terms
4291,hemija,This can arise due to ion-ion interactions brought on by Coulombic fields A proper explanation is rooted in transport phenomena Unfortunately chemists are largely not taught this but chemical engineers do get the opportunity Under standard conditions ions in a solution will take a random walk during diffusion This results in no net movement of our ions over some span of time Such a system perturbed by an applied electric field will cause ions to preferentially move in one direction we say the ions will drift We say they have a drift velocity These velocities are affected by a number of parameters as there is still continual collision but we can say that the mobilities will be unequal for ions of unequal sizes Different mobilities mean concentration gradients form and charge separation too Typically we use an electroneutrality field as a reasonable approximation to allow analytic solutions for various equations This field depends on all the ionic fluxes present and as such will affect the total measurable current The result is simple the introduction of any ion will cause a perturbation in the field that all the ions encounter in solution
4288,hemija,I don't have time to do the calculations manually but inputing your problem data into a chemical equilibrium software (here Dozzaqueux) reveals that the precipitate you observe is Pb(OH) 2 while no PbI 2 should form The former is white while the later is yellow; can you tell us what color was your precipitate
4292,hemija,Yes along with a family of related methods I honestly don't fully understand the mechanics behind it but the orbitals obtained with the NBO method can be used for a treatment called Natural Resonance Theory which gives you an idea of what sort of Lewis-structure resonance components there are and in what weightings as well as bond indices and a measure of covalency There are three papers from the developers of the technique covering theory usage and some more examples (I'd probably start with the usage paper ) Also along with NBO there are the Natural Atomic Orbitals which are often lumped in because they're also computed by the NBO program and Natural Population Analysis which you can use the Natural Atomic Orbitals to do These give you an idea of what sort of atomic orbital character is contributing to your molecular orbitals and also atomic charges and net spins
4287,hemija,Yes a 2D-periodic space can be mapped to a torus but that's more a question for the math SE … Regarding your bonus question why would there be What would you do with it Molecular structures are intrinsically 3D so I don't see what you would do in a 2D (periodic or not) space Even when we talk about planar or pseudo-2D structures (buckyball nanotube etc ) they are 3D objects with 3D electronic densities and wavefunctions Edit 3D structures that are periodic in two dimensions and finite in the other one can be studied by many computational chemistry codes They are often referred to as slab calculations or surface calculations The most common issue is that of Coulombic interaction (or Poisson equation solver) which typically requires special treatment in the 2D case
4323,hemija,Is this even how GC/MS results work As cbeleites said the method you described is a proper technique but not likely to be appropriate given the information you cited In GC/MS you should have two sets of information The first is the GC Total Ion Chromatograph (TIC) which will have time as the x-axis and response (abundance) as the y-axis For each retention time on the TIC there will be a corresponding Mass Spectra (MS) In the MS the x-axis is m/z (ion) and y-axis is also response (abundance) Different compounds have different responses so if you inject the exact same amount of two different compounds you could get a much larger response from one compared to the other For example tramadol gives much higher response compared to hydrocodone This is why you want your internal standard to be structurally similar to your analyte Quantification of a compound is often done by running 3 - 5 calibrators at known concentrations in order to make a calibration curve Once an acceptable calibration curve is made the sample along with controls can be fit onto the curve to calculate the amount of the compound of interest in the sample and controls If the controls are correct then the sample value can be used To make a calibration curve you need an internal standard in each calibrator control and sample You can then make the curve either with ion ratios between paired internal standard and analyte ions OR if your internal standard response is consistent in all the calibrators controls and sample you can use the GC peak response In conclusion if you only have one GC/MS run with the two dissimilar compounds in it you will not be able to easily calculate the quantity of the second compound based on the GC response of the first unless you have additional information that was not listed in this question
4805,hemija,I can't tell if the modifier formal applies only to definition or to algorithm also in the first sentence of your question's body In case any algorithm would do this looks like a pretty standard instance of the cycle-detection problem that one learns in undergraduate computer science If you only want to detect the presence of cycles then the algorithm is essentially just to attempt a topological sorting of the graph; if the attempt fails you have a cycle and I guess that would mean that you have some sort of catalysis going on (I didn't major in chemistry; please yell at me if that's non sequitur ) Okay so mere cycles are not enough - it seems you want cycles that have gain How about a tortoise and hare and have these multiply a state variable by the ratio of outgoing to incoming edges to reaction nodes (denoted by squares in your diagram) If the hare catches up to the tortoise that implies a cycle hence catalysis; then if you compare their state if the hare has accumulated a larger gain than the tortoise I would say that that shows the presence of autocatalysis I'm not sure how this works if there are multiple linked cycles You could have one cycle in your graph that's autocatalytic and another cycle linked to it that uses up its catalyst (something of a contradiction) For example B + C -&gt; D + E A + D + H -&gt; 2C + F 2F + G -&gt; H Is that still an autocatalytic system
4352,hemija,Suffice to say nobody really knows and a Nobel prize surely awaits the first people who can explain the relationship between chemical composition and high-temperature superconductivity
4296,hemija,You cannot have bond angles of 0 or 180 degrees in a Z-matrix This is because the dihedral becomes degenerate To solve this you can add ' dummy atoms ' which provide an auxiliary point of reference to remove the 180 degree angle These atoms are usually denoted $\ce{X}$ or $\ce{Xx}$ in computational chemistry suites Usually dummy atoms are placed at 90 degrees to a pair of atoms and the third collinear atom is then defined with a bond angle of 90 degrees and a dihedral of 0 or 180 degrees with respect to the previous 3 atoms Example To illustrate what I'm talking about here's a schematic of your pair of molecules with three dummy atoms This could have a Z-matrix like O 1 C 1 a Xx 2 1 0 1 90 O 2 b 3 90 1 180 Xx 4 1 0 2 90 3 0 H 4 c 5 90 2 180 Xx 6 1 0 4 90 5 0 O 6 d 7 90 4 180 H 8 e 6 104 5 7 180 Notes There are no hard and fast rules about how many you should use (obviously we could simply define subsequent dihedrals in terms of the leftmost dummy atom) however it's best to strive for locality so that small changes in some variable don't correspond to large displacements elsewhere in which case rounding errors or large gradients can ruin your day You could also have defined all of the dihedrals in your molecule with respect to that convenient non-collinear hydrogen however this is the Z-matrix equivalent of spaghetti code You will note that I use different letters for each variable This is because forcing the $\ce{H-O}$ and $\ce{C=O}$ bonds to be of equal length is probably an unrealistic constraint for a geometry optimisation of this system Finally note that dummy atoms do not take part in the electronic structure of a molecule so feel free to put them in anywhere and set the dummy atom 'bond' length to whatever you want 1 0 Ångström is traditional as far as I know P S Molden has a complete if idiosyncratic graphical Z-matrix editor
4588,hemija,I've searched for this same data My interest regards electronic proporties of zeolite-class media I've written this same kind of question on Quora I think the reason this is rarely answered and why there are no 'catalogs' is several-fold There exist a wide range of applications based on the composition of the water / etc The treatment media usually varies in composition meaning that no directive for the use of a zeolite product can be given by a sales rep (only on-staff engineers who know how to use it) Some uses create by-products that are regulated creating possible liabilities for the seller by indicating a fitness-for-use Regulations change; per year per locale etc I think the groups that know about these are environmental engineering firms whom have a stable of engineers that provide end-to-end services for their clientele Many of of these firms have Chemists on-staff or a consultancy relationship that suits the same They know the chemical properties of their media and need a zeolite / catalysts that can effect something very specific As a result they're out their hunting the journals for something that targets their needs Hard to out-sell that without already having a dog in the game
4305,hemija,For those who have never had the pleasure of personally doing this see this video It has been known since 1807 that dissolving sodium in liquid ammonia results in a beautiful color It was originally thought the color was due to some familiar complex instead of a solvated electron A similar phenomenon happens with other alkali metals in ammonia Research in the field says that it takes at least forty some ammonia molecules to solvate a given electron Metstable cavities form and their stability could depend highly on electrostatic interactions to solvate our electron Ammonia will slowly react by evolving hydrogen gas $$\ce{ 2NH_3 + 2e^- \rightarrow H_2 + 2NH_2 ^-}$$ Differences in solvent can play a huge role in the stability of our solvated electron An analogous decomposition occurs in water $$\ce{ 2H_2O + 2e^- \rightarrow H_2 + 2 HO^- }$$ Famously the latter occurs faster than the former see here in comparison to my earlier link We could say that these differences in rate reflect different stabilities of our solvated electron Although the addition of an appropriate catalyst to our ammonia will result in rapid evolution of hydrogen A more quantitative description for the difference in energies is obtained by measuring the UV-vis spectrum for a solvated electron in both water and ammonia One will quickly notice that the band appears at higher energies in water than it does in ammonia and there are considerable differences in the band shapes (the band is wider in ammonia) So Why Do They Differ So here I have to make the disclaimer that no current theories quantitatively reproduce the observed phenomena such as the absorption spectrum for a solvated electron This question has no established/accepted answer as it is an ongoing area of research The self-ionization of water has an equilibrium constant on the order of K = $10^{-14}$ and that of ammonia is on the order of $10^{-30}$ Hydronium formation occurs in the case of water and ammonium in the case of ammonia Hydronium has a considerably lower pKa than ammonium and so it's reasonable to see why a reaction in water would be more likely with an electron (Ammonia rarely produces a weakly acidic species but water often produces a strongly acidic complex ) A slightly deeper reason may be that water forms more ordered local domains/structures in solution than ammonia and this influences the rate by resulting in a larger entropy of activation when the hydrolyzed electron breaks up these larger structures recall that $k \propto \exp (\Delta S ^\ddagger /R) $ Just speculation though
4302,hemija,To understand why the exclusion principle isn't violated in this system you really need to shift from valence bond theory to molecular orbital theory The $p_{z}$ orbitals in benzene combine according to the $D_{6h}$ symmetry of the molecule to generate a set of bonding molecular orbitals (MOs) (which are lower in energy than the isolated $p_{z}$ orbitals) and a set of antibonding molecular orbitals (which are higher in energy) Bonding orbitals are generated from sets of $p_{z}$ orbitals which are mostly or totally in phase whereas the highest energy antibonding orbital of the set is composed of $p_{z}$ orbitals that are totally out of phase with respect to their adjacent neighbors Here for instance is the MO that is generated from the totally in-phase set (I calculated this at a spin-restricted RI-BP86/6-311G* level of theory in ORCA (1) and visualised the isosurfaces in VMD (2) ) Now this MO is shared equally across all of the carbons i e it is not localised However it only contains 2 electrons and thus satisfies the exclusion principle (F'x mentions spin-orbitals - this is where the orbitals are split on the basis of their spin which entails a single electron per orbital Benzene is a closed-shell singlet so we would expect the $\alpha$ and $\beta$ spin orbitals to be spatially indistinguishable ) Not to sound like a walking advertisment for Housecroft and Sharpe but H&amp;S has a great visual introduction to MO theory P S It was suggested in chat that I expand upon the generated orbitals - here's one of the two highest occupied molecular orbitals from the same calculation As you can see there's a big node along one of the mirror planes This orbital is higher in energy than the earlier example because it's composed of two antiphase sets of 3 $p_{z}$ orbitals so this orbital has antibonding character for two of the carbon-carbon pairs It should be easy to extrapolate to the highest lying MO of this type - antibonding all the way around (1) Neese F ORCA – an ab initio Density Functional and Semiempirical program package Version 2 6 University of Bonn 2008 (2) Humphrey W Dalke A and Schulten K 'VMD - Visual Molecular Dynamics' J Molec Graphics 1996 14 1 33-38
4301,hemija,First Wikipedia doesn't really say what is the extent of this change Here are superimposed a benzene and a benzyne molecule (own HF calculation don't ask for the details) The carbon atoms involved in the triple bond are displaced by less than 0 2 Å In benzene we have with this level of calculation C–C distance = 1 39 Å C–H distance = 1 08 Å C–C–C angle = 120° while in benzyne we have C–C triple bond = 1 22 Å other C–C bonds = 1 38 Å (almost all equal) So merely because the triple bond is stronger the distance will be shorted and the benzyne structure is distorted Regarding orbitals well the p orbitals of the carbon atoms involved in the triple bond are “naturally” directed towards where the hydrogens would be Slightly distortion of the system is possible and indeed evidence above on the structure but they will not be parallel as that would disrupt the structure too much and diminish the $\sigma$ bond So it's a competition between making the new $p$ orbitals “more parallel” increases overlap but it weakens the $\sigma$ bonding pattern which is much stronger As such while some distortion happens you stay mostly close to the benzene-type orbitals
4311,hemija,That diagram is misleading because it's technically in reference to bis(hexamethylbenzene)ruthenium(0) This is a famous example by the late and great E O Fischer This Ru(0) complex is 18e three methyl resonances were observed for the bent ring at -10 C$^{\text{o}}$ Crystallography later confirmed this This structure is surprising to the new student perhaps because the classic 18e bis(benzene)chromium is sandwiched between two planar arenes This is not an organic chemistry question It is deep in the realm of organometallic chemistry with a proper explanation requiring a molecular orbital energy diagram and then one has to invoke a Jahn Teller instability argument that requires lowering symmetry from $D_{6d}$ or $D_{6h}$ Suffice it to say stability is increased by changing the hapticity
4309,hemija,I am an Australian English speaker and yes this distinction is made in practice Possibly the only reasonable opportunity to use the p?? pronunciation in the name of a compound is in the case of the entertaining molecule periodane which is actually named after the periodic table This molecule (and later a number of different plausible isomers) was identified computationally by a methodology called 'mindless chemistry' which optimises randomly generated molecular graphs Periodane is a stable configuration of each atom on the second row of the periodic table with the exception of neon (although some people are working on that)
13153,hemija,High-temperature phase transformation and low friction behaviour in highly disordered turbostratic graphite doi 10 1088/0022-3727/46/39/395305 Phase Transformation Mechanism of Graphite-Turbostratic Graphite in the Course of Mechanical Grinding CHEM RES CHINESE U 19(2) 216 (2003) http //www onxlti com/product-divisions/contract-manufacturing-products/on-x-pyrolytic-carbon/
4370,hemija,Juha invited me to write a summary (see comment on my previous answer ) of the differences between melting and dissolving I’ll try to outline this roughly in the same order as his I'm giving this as a new answer since my last answer was quite long as it was Differences Melting and dissolving are completely different processes on the molecular/atomic level that could not be mistaken for each other if you could observe what was happening at that scale If you cool the liquid that arose from melting the solid to a temperature below it’s melting point you would see the entire sample solidify If you cool the solution (dissolved solid + solute) to below the melting point of the solid (solute) in it you would see no change (Unless you had a saturated or nearly saturated solution ) Melting requires only a single substance and energy input while dissolving requires a solvent and a solute that are compatible (“like forces”) (This is actually a pretty huge difference and would potentially affect all of the physical and chemical properties ) Dissolving a solid can be either endo- or exo-thermic Phase change (wording from the previous summary) In each case you end up with a liquid Melting caused a phase change (composition of the substance didn’t change) while dissolving a solid in a liquid is not considered a phase change since a change in composition occurred Similarities In each case forces between the particles that comprise the solid are disrupted and that takes energy (Whether it’s chemical bonds or intermolecular forces depends on the process and on the solid and on your definitions ( See this question ) But melting (rare exception noted in previous comments) is endothermic and dissolving can be either endo- or exo-thermic In each case you end up with a liquid Macroscopically if you walked into a room and saw the liquid on the table it would be difficult to say whether this liquid came from a solid that had melted or a solid that had dissolved in a solute and made a solution But it would be very easy to determine which you had experimentally in a “dozen” different ways Both melting and dissolving require interaction among groups of atoms molecules or ions There are probably more differences that could be given (how to handle thermodynamic calculations complexity of the system etc ) and possibly more similarities but that's enough for me on this topic
4490,hemija,I am not an expert in ICP-MS but an ICP-MS should be considered an ICP ion source coupled with a mass spectrometer There are some key issues raised by the coupling between both instruments due to the high temperature of the ICP source as well as due to the fact that one wants to analyse atomic ions and remove any interfering molecular ions (mostly oxides but other molecules and clusters are possible) Therefore to attempt an answer to your question the standard output of an ICP-MS will be presented as a mass spectrum (intensity versus m/z or intensity versus amu) There are a couple of sample spectra in the brochures published by instrument makers see for instance the Bruker brochure or a Thermo application note How this mass spectrum is obtained from the detector will lead to the type of data that is acquired For a quadrupole mass filter as well as for magnetic sector instruments two modes of operation are possible Either a single mass is selected and therefore ont will only get a trace of the abundance of a given mass as a function of time (chromatography or on-line sample analysis) Otherwise the mass filter can be scanned leading to a mass spectrum For time of flight detectors the mass spectrum is built by the measurement of a flight time for an ion to reach the detector Therefore the mass spectrum is converted from a time dependent signal to a spectrum Finally there have been some attempts to couple an ICP ion source with a Fourier transform mass spectrometer As far as I know these are not commercial instruments In such instruments the ions oscillate at m/z dependent frequencies and the recorded image current is converted to a spectrum through the use of Fourier transform
4377,hemija,Toxicities are often greatly influenced by how you are exposed and in what dose Mercury is a fascinating metal You can find it as the pure metal a salt or in an organic form In its standard state it is a liquid and is rather volatile Large amounts are naturally released into the atmosphere but humans contribute as well for example burning fossil fuels such as coal will release it into the atmosphere The forms of mercury are not set in stone it is quite able to switch forms from the free metal to the salt to the organic by processes in nature Orally consuming some liquid metal isn't the worst possible route of exposure; mercury was at one time even used as a laxative The only absorption in the GI tact is thought to be due to mercury vapor Skin exposure is undesirable but we know that inhaling the vapor is more toxic because it is soluble in lipids Upon entering the bloodstream some makes it way past the blood/brain barrier (hence its neurotoxicity) and some is converted to Hg(II) by catalase Mercury's inorganic salts have a long useful history where some were used to treat syphilis some for lights and some for hat making and so on Mad as a hatter won its name due to chronic exposure to Hg(NO$_3$)$_2$ which was used in hat making This salt isn't very lipid soluble but more absorption does occur when swallowed than the free metal These salts tend to build up in the kidneys after absorption and high exposure will cause renal failure Not much will pass through the blood/brain barrier but chronic exposure will eventually allow some across and cause neurotoxicity They also tend to bind with proteins containing sulfur and this can cause some cell death As a general rule Hg(I) salts are less toxic than Hg(II) presumably due to the decreased solubility of Hg(I) This does not mean they cannot cause harm mercuric chloride was used in a factory and the wastes were discharged into a nearby sea anaerobic bacteria methylated the mercury this methylmercury was absorbed by fish that were later eaten and hurt many people As an organic mercury compounds tend to be considerably more lipid soluble and therefore can readily be absorbed or cross the blood/brain barrier in comparison to the free metal A terrible example is dimethylmercury which claimed the life of an inorganic chemist who was accidentally exposed to some she was using in an NMR experiment I do not know much about its amalgams aside from gold/silver I believe most of the toxicity was seen in gold refineries due to the fact that the amalgam is heated to remove the mercury which leaves behind gold and results in the release of mercury vapor
4315,hemija,I'm not sure I completely understand what exactly your problem is but You want to resolve to signals at approx m/z = 1060 2 and m/z = 1061 2 So $\Delta m = 1$ The required resolution is $\frac{m}{\Delta m} = \frac{1061}{1} \approx 1060$ The spectrometer in (a) has only 1000 which is too low The spectrometer in (b) has 5000 which is quite more than needed Maybe it is easier to think of the reciprocal $\frac{\Delta m}{m}$ which tells you how far two signal need to be apart so your spectrometer can resolve them In (a) this is $\frac{\Delta m}{m} = \frac{1}{1000} = \frac{1 06}{1060}$ two signals can be resolved if they are at least 1 06 m/z apart at m = 1060 I e if they were slightly further apart than they are The spectrometer in (b) can resolve signals that are at least $\Delta m/z = 0 21$ apart at $m/z = 1060$ FWHM and resolution The FWHM is basically part of the resolution The $\Delta m$ is the m/z difference between two signals that are just resolved Have a look at the IUPAC's definitions of resolution in mass spec There are several approaches to specify what just resolved means The FWHM can be used directly as the $\Delta m$ of the peak width definition If instrument A has $5\times$ the FWHM than instrument B then resolution of A $\approx \frac{1}{5}$ resolution of B I'm more used to the 10 % valley which roughly equals the full with at 5 % height you specify how much signal is allowed at the minimum between the two peaks e g 10 % It is important to realize that these two definitions differ by almost a factor 2 Here are some results of simulated peaks at different resolutions and $\Delta m/z$ (all gaussian which of course doesn't need to be the case in reality) I use the full width at 5% = 10 % valley definitions for the resolution The first row is resolution 5000 with the second row is resolution 1000 You see that the two signals are much better resolved than in your example picture above Which means that above another resolution definition was used possibly $\Delta m$ = FWHM In any case a resolution of 1000 should be almost sufficient to resolve m/z 1060 2 from 1061 2 The third row shows two signals of equal height with $\Delta m/z $ = 1 = FWHM According to the resolution definitions above this is a resolution of ca 520 Note that while you can say that there are at least two peaks you cannot take the two maxima as the m/z of the two underlying signals (grey) The fourth row is almost the same but I multiplied the second signal by 56% to account for the frequency of having bradykinin with one $^{13}$C Looks quite similar to your picture Also have a look at M P Balogh Debating Resolution and Mass Accuracy LC•GC Europe 17(3) 152–159 (2004)
4336,hemija,When people say that Kohn-Sham orbitals bear no physical meaning they mean it in the sense that nobody has proved mathematically that they mean anything However it has been empirically observed that many times Kohn-Sham orbitals often do look very much like Hartree-Fock orbitals which do have accepted physical interpretations in molecular orbital theory In fact the reference in the OP lends evidence to precisely this latter viewpoint To say that orbitals are good or bad is not really that meaningful in the first place A basic fact that can be found in any electronic structure textbook is that in theories that use determinantal wavefunctions such as Hartree-Fock theory or Kohn-Sham DFT the occupied orbitals form an invariant subspace in that any (unitary) rotation can be applied to the collection of occupied orbitals while leaving the overall density matrix unchanged Since any observable you would care to construct is a functional of the density matrix in SCF theories this means that individuals orbitals themselves aren't physical observables and therefore interpretations of any orbitals should always be undertaken with caution Even the premise of this question is not quite true The energies of Kohn-Sham orbitals are known to correspond to ionization energies and electron affinities of the true electronic system due to Janak's theorem which is the DFT analogue of Koopmans' theorem It would be exceedingly strange if the eigenvalues were meaningful while their corresponding eigenvectors were completely meaningless
4319,hemija,You will find the answer in probably any textbook on chemical thermodynamics or in a thermochemistry chapter in most physical chemistry textbooks or in Wikipedia's enthalpy of dissolution page Dissolution can be viewed as occurring in three steps Breaking solute-solute attractions (endothermic) see for instance lattice energy in salts Breaking solvent-solvent attractions (endothermic) for instance that of hydrogen bonding Forming solvent-solute attractions (exothermic) in solvation The value of the enthalpy of solution is the sum of these individual steps Dissolving ammonium nitrate in water is endothermic The energy released by solvation of the ammonium ions and nitrate ions is less that the energy absorbed in breaking up the ammonium nitrate ionic lattice and the attractions between water molecules Dissolving potassium hydroxide is exothermic as more energy is released during solvation than is used in breaking up the solute and solvent So it can have any sign The linked page also gives data for common compounds in water some of them endothermic some of them exothermic Regarding why the substance is soluble why its dissolution is endothermic you have to remember that the reaction takes place if $\Delta_r G$ is favourable (i e negative) and $\Delta_r G = \Delta_r H - T \Delta_r S$ Overall the free energy must be negative for dissolution to occur (on a thermodynamic basis; kinetics are another issue) not the enthalpy
4325,hemija,I'm not familiar with the computational packages -- I'm an experimentalist not a theoretician As an example for why both might be present we can turn to gaseous infrared spectroscopy one will quickly find that the line-widths in vibration-rotation spectra depend on pressure At lower pressures say less than a torr Doppler-broadening is the main mechanism This is Gaussian in nature While at higher pressures the broadening is instead due to collisions which are Lorentzian You'll find combinations of the two and so forth depending on the specifics of your case
4327,hemija,You can detect $O_{h}$ $I_{h}$ and $T_{d}$ symmetry by checking that a molecule has all of the subgroup symmetries of these point groups According to this untitled document which I presume 1 is by W C Trogler the elements are as follows $T_{d}$ $E$ $4C_{3}$ $3C_{2}$ $3S_{4}$ $6\sigma{_d}$ $O_{h}$ $E$ $3C_4$ $4C_3$ $6C_2$ $4S_6$ $3S_4$ $i$ $3\sigma{_h}$ $6\sigma{_d}$ $I_{h}$ $E$ $6C_{5}$ $10C_{3}$ $15C_{2}$ $i$ $6S_{10}$ $10S_6$ $15\sigma$ Obviously you don't need to check for the identity symmetry $E$ If you're a visual sort of person a symmetry spot check is to mentally superimpose a tetrahedron cube or dodecahedron over the molecule and see whether the view down the surface normal of each face is identical Cubes and octahedra are duals of each other as are dodecahedra and icosahedra The tetrahedron is self-dual Interestingly H&amp;S does not list chiral forms of these point groups probably because they are so rarely encountered however researchers have come up with molecules that obligately satisfy $T$ $I$ and $O$ symmetry 2 (I have not yet read the paper) (1) I would be indebted to anyone who can furnish me with a full citation to this work and confirm the authorship (2) Narasimhan S K Lu X and Luk Y -Y (2008) Chiral molecules with polyhedral T O or I symmetry Theoretical solution to a difficult problem in stereochemistry Chirality 20 878–884
4331,hemija,Soda contains $\ce{CO2}$ Over time the gaseous $\ce{CO2}$ comes to the surface The ice cream increases the rate of this $\ce{CO2}$ from the soda causing increased fizzing This increased rate is a result of the solvation of ice cream particles in the liquid The particles serve as nucleation sites for the growth of gas bubbles The bubbles get large quiet fast thus you observe fizzing Edit After a bit of experimentation i found the following- Effect of adding milk - There is a bit of extra bubbling and forms a curdy precipitate(couldnt dare to drink it ) Effect of adding salt ( yes ice cream does contain salt ) - Lots of bubbling around the salt After the salt settles to the bottom bubbles collect around it become big and then come to the surface (only thing left to try is sugar and vanilla essence but i ran out of soda) So we come to know that the fizzing effect may be due to milk or salt or a combo of both
5024,hemija,Toilet bowls are made of ceramics which can contain feldspar Toilet bowls also often have a feldspar glaze There is a study that disolved Cr(VI) can be removed by feldspar Studies on the removal of Cr(VI) from waste-water by feldspar http //onlinelibrary wiley com/doi/10 1002/jctb 280530204/abstract The IMA currently recognizes 90 Cr bearing minerals 16 minerals if you only count those that contain also Si Feldspar can be either K Na or Ca-Feldspar Of those 16 minerals 6 contain Na 2 K and 5 Ca Reading those papers might even narrow the choices down to a couple of minerals that can form in standard-toilet-bowl-conditions Mineral database that lets you select minerals by their chemistry http //rruff info/ima/ I think that unless you are willing to donate your toilet bowl to science that the answer won't get any more precise than this Edit Cu would be the favorite choice for a blue color mineral (Azurite ) There is also a study about the removal of copper by feldspar Experimental studies of the interaction of aqueous metal cations with mineral substrates Lead cadmium and copper with perthitic feldspar muscovite and biotite http //www sciencedirect com/science/article/pii/S0016703797001178 Again there are 603 Cu-minerals Cu+Si=32 minerals With the common Feldspar cations your left with about 10 each I don't think the minerals could be recognized visually The favorite method would be x-ray diffraction of a powder sample of the blue stuff
4353,hemija,This answer applies to carbon filtration of water According to most of the sources I found activated carbon binds to most substances through London dispersion forces (from the Wikipedia article) This should mean that it adsorbs larger molecules and non-polar molecules preferentially since they would have larger dispersion forces As your quote indicates it does not bind polar molecules like alcohols glycols strong acids and bases well This source also lists neutral non-polar compounds along with organics and compounds with low water solubility as being effectively removed I think that you may have misread or misinterpreted the sentence about binding chlorine and iodine as saying that it traps chloride iodide and (by extension) other anions well I can't find data to support this statement It traps molecular iodine well and as a large (in terms of electrons) non-polar molecule that makes sense But it removes molecular chlorine $\ce{Cl2}$ through a chemical reaction by reducing it to the chloride ion $\ce{Cl-}$ which is then soluble in water and flows through the filter I did find a couple of sources ( 1 2 ) that said activated charcoal has a slight electropositive charge that helps it to attract negatively charged species which would fit with your statement that it preferentially attracts anions (However they don't list what kind of negative species they're talking about and nitrate sulfate and fluoride ions were specifically excluded) The idea that it will remove ionic compounds seems to be contradicted by the statement in at least a couple of sources ( one given here ) that carbon filters don't remove minerals salts and dissolved inorganic compounds
4350,hemija,It's been known since 1941 that the answer to your question is in the negative i e that there will never be a closed form equation of state for a nonideal gas In 1941 Mayer and Montroll developed what is now known as the cluster expansion for the partition function of a nonideal gas whose particles have pairwise interactions This cluster expansion provides a 1-1 correspondence between various integrals over the interaction potential and virial coefficients in the Kamerlingh Ohnes equation of state which has in principle an infinite number of terms in it Therefore it would not be considered a closed form equation of state In practice the virial coefficients are known to decay so it is usually safe to truncate the expansion for practical calculations One could extend the cluster expansion to three-body and higher-order interaction potentials but this will not change the fundamental argument above
4345,hemija,Those are $sp^2$ carbons and should have a triangular planar 120&deg; geometry The angles you've drawn are clearly not 120&deg; I've not exactly come across the terminology before but I believe this is the distinctions In a linear formula you disregard the actual bond angles and just try to keep everything in a line In an angular formula you draw everything according to the bond angles Not sure of this though
4364,hemija,Basic dishwashing agents Some dishwashing agents contain substantial amounts of NaOH see below Sometimes NaClO solutions are used as disinfectant for dishwashing $\require{mhchem}$ As $\ce{Cl- + ClO- + 2 H+ &lt;=&gt;&gt; Cl2 ^ + H2O}$ they are kept basic (and $\ce{Cl-}$free but $\ce{3 ClO- -&gt; 2 Cl- + ClO3-}$ and $\ce{2 ClO^- -> 2 Cl^- + O2 ^}$) for stabilization @Ashu asked whether I'm really sure so here's an example with 15 - 30 % KOH + 15 - 30 % NaOCl pH 14 Liquid intensive cleaner for dishwashing machines ( technical data saftety sheet both German though) Soaps (chemical meaning) are salts of fatty acids Na- and K-soaps are good for cleaning and as salts of weak acids and strong bases they are basic Dishwashing liquid chemistry If we approximate the food rests you want to clean away during the dishwashing as mixture of carbohydrate proteins and lipids then your dishwashing liquid needs that take care of those 3 substance classes in water carbohydrates are hydrophilic - no problem here amphiphils are needed to clean lipophilic substances like lipids oils greases as Ashu explained already They can take care of quite a bit of protein as well basic solutions usually cause a faster hydrolysis of the amide bonds in proteins than acids Usually quite a bit faster NaOH-solutions immediately give a soapy feeling while e g HCl doesn't As far as I know here (Germany) basic liquids for household dishwashing machines are uncommon/seldom/not used at all (I'm not so sure about the tablets) The actually used solutions are less corrosive (for both the machine and your fingers if you more or less accidentally decide to use it for dishwashing by hand) This page from a New Zealand poison centre talks about the dishwasher [ ] powder or tablets [ ] are often highly alkaline For dishwashing liquids where you are thought/supposed to put your hands in the pH is usually neutral to slightly acidic so you need to scrub a bit more but don't dissolve your skin Acids for cleaning Acids (usually acetic acid or citric acid maybe HCl) are used in household chemistry to clean limescale (CaCO$_3$) But that usually isn't the problem of your dirty dishes Dishwashing machines use ion exchangers to soften the water (or phosphate in the dishwashing agent) so you don't get limescale or lime soaps inside the machine
4368,hemija,I have to stretch my memory to remember how this goes (and read a bit from here ) First how do you define burgers vector You take the end point of the dislocation and make a circle around it as it were a perfect lattice the extra (or missing step) from the full loop is the burgers vector Now is the burgers vector also a lattice vector If yes then it is (edge)dislocation If no then it is only a partial (edge)dislocation (partial dislocations have their own name) Now how does this work on a fcc-lattice case On quick read for link I pasted it seems that fcc-lattice cannot have edge dislocations because the burgers vector is not a lattice vector You may want to go this through by yourself and check that I didn't make any mistakes
4360,hemija,Hammett wanted to find a way to quantify the effects electron-withdrawing (EWG) and - donating (EDG) groups have on the transition state or intermediate during the course of a reaction Initially he took the $pK_{\mathrm{a}}$-values of benzoic acids carrying the respective functional group in para or meta position (ortho acids and aliphatic acids weren't used because steric effects would overlay with the electronic effects) as a guide and plotted them against the logarithm of the reaction rates So he based his scale on the following reaction reaction Later Hammett decided not to use the $pK_{\mathrm{a}}$s themselves for his correlation but defined a new parameter which he called $\sigma$ This $\sigma$ shows how electron-donating or -withdrawing a group is relative to $\ce{H}$ as a ratio of the $\log K_a$s or the difference of the $pK_{\mathrm{a}}$ between the substituted benzoate and benzoic acid itself If the acid required to determine $\sigma$ for a new substituent was not available $\sigma$ could be determined by correlation with other reactions A different value of $\sigma$ for any given substituent was needed for the meta and para positions (sometimes called $\sigma_{\mathrm{m}}$ and $\sigma_{\mathrm{p}}$ respectively) The equation for $\sigma$ is \begin{equation} \sigma_{\ce{X}} = \log \left( \frac{ K_{\mathrm{a}}(\ce{X-C6H4COOH}) }{ K_{\mathrm{a}}(\ce{H-C6H4COOH}) } \right) = pK_{\mathrm{a}}(\ce{X-C6H4COOH}) - pK_{\mathrm{a}}(\ce{H-C6H4COOH}) \end{equation} or in short notation \begin{equation} \sigma_{\ce{X}} = \log \left( \frac{ K_{a} (\ce{X}) }{ K_{a} (\ce{H}) } \right) = pK_{\mathrm{a}}(\ce{X}) - pK_{\mathrm{a}}(\ce{H}) \end{equation} where $\ce{X}$ is the functional group i e the EWG or EDG whose effect on the reaction rate shall be evaluated It is also possible to determine the $\sigma$ values via the logarithm of the ratio of the rate constants for the aforementioned reaction with substituent $\ce{X}$ (in short notation $k_{\ce{X}}$) and with $\ce{X}=\ce{H}$ (in short notation $k_{\ce{H}}$) \begin{equation} \sigma_{\ce{X}} = \log \left( \frac{ k_{\ce{X}} }{ k_{\ce{H}} } \right) \end{equation} Update AcidFlask 's comment reminded me that I forgot to say something about $\sigma^{+}$ and $\sigma^{-}$ values As mentioned above the normal $\sigma$ values are based upon the difference of the $pK_{\mathrm{a}}$ between a substituted benzoate and benzoic acid itself But for benzoates one cannot draw resonance structures that delocalize the negative charge onto the benzene ring via the $\pi$ electron system Yet many reations of interest create negative or positive charges that can be stabilized by delocalization via resonance with the substituent For these reactions one finds that Hammett plots using $\sigma$ values have considerable scatter Therefore two new substituent effect scales were produced one for groups that stabilize negative charges via resonance ($\sigma^{-}$) and one for groups that stabilized positive charges via resonance ($\sigma^{+}$) The $\sigma^{-}$ scale is based upon the ionization of para-substituted phenols (I've found different accounts for this According to the German Wikipedia the $\sigma^{-}$ scale is based upon the ionization of para-substituted anilins ) for which groups like nitro can stabilize the negative charge via resonance The $\sigma^{+}$ scale is based upon the heterolysis ($\mathrm{S}_{\mathrm{N}}1$) reaction of para-substituted cumyl chlorides (phenyldimethyl chloromethanes)
4365,hemija,How flattering that someone asked this question ;) The short answer is yes I've tried QTPIE on series of conjugated molecules like the oligoacetylenes and oligoacenes The results however are fairly dismal as can be seen in the preprint The problem is that in these empirical charge models the polarizability grows faster than the size of the molecule which means that these models predict infinite polarizabilities and charge transport behavior for bulk materials which is obviously nonsense The problem is quite intimately related to that of self-interaction error in approximate density functional theories which also exhibit similar problems
18076,hemija,You can functionalize silica to add polymer brushes to the surface You can also buy pre-made functionalized silica although the downside to that is it isn't always easy to get the manufacturer to tell you exactly what is on the surface This isn't really going to increase the attractive interactions between the silica and the polystyrene however Polystyrene has a bulky aromatic ring hanging off of each monomer and no sites that can participate in hydrogen bonding When you add hydrophobic brushes to the surface of the silica what you are doing is limiting the ability of the silica to aggregate due to hydrophilic interactions with itself As a result given enough kinetic energy (temperature for melt casting or sonication for solvent casting) you can get the particles to disperse Then if you solidify the polymer quickly enough they will be kinetically trapped in a dispersed state Depending on what your goal is this could be enough for your application However if you want to get the particles to strongly attract to the polystyrene you aren't going to be able to get much farther than van der Waals interactions That's a limitation of the polystyrene not the silica Other particles (gold or carbon nanotubes for example) will be more hydrophobic and easier to disperse without functionalizing but you still are not going to get very strong attractive forces between the particles and the polymer
8371,hemija,Some density functional theory packages such as Quantum Espresso can calculate XPS core level shifts using the pseudopotential formalism See Phys Rev B 74 045430 (2006)
4373,hemija,We don't have a homework policy yet but I'm quite sure that it will be against providing full answers Hints You're supposed to take into account molar masses The ratios in a formula are molar ratios so you need to use those not mass ratios Read up a bit on emperical formulae as Janice suggested
16389,hemija,According Kinetics Mechanism and Spectroscopy of the Reversible Binding of Nitric Oxide to Aquated Iron(II) An Undergraduate Text Book Reaction Revisited The correct structure is $\ce{ [Fe^{III}(H_2O)_5(NO^{-})]^{2+} }$ For many years it was thought that iron was reduced to Fe(I) and NO oxidized to NO+ based upon an observed magnetic moment suggestive of three unpaired electrons however the current thinking is that high spin Fe(III) (S=5/2) antiferromagnetically couples with NO- (S=1) for an observed spin of S=3/2
4391,hemija,Alas the problem of course is that you need both position q and momentum p to get an accurate picture of the overall electron state This is not true in quantum mechanics; it is sufficient to characterize the wavefunction $\left\langle x |\psi\right\rangle$ in position space or $\left\langle k|\psi\right\rangle$ in momentum space Which is chosen is entirely a matter of convenience most molecules have the most compact description in position space while most crystals have compact descriptions in momentum space It can be proven that position alone or momentum alone form a complete basis separately In fact position-momentum uncertainly says that an electron state cannot be characterized completely in both position and momentum simultaneously (This by the way has nothing to do with fermionic symmetries and Pauli exclusion which is another quantum-mechanical axiom unto itself The same would have held for looking at bosonic particles also ) It is however true that a probability density in phase space $(q p)$ is needed to completely characterize a classical system Wigner's function was introduced to address this quantum-classical correspondence and is regarded as the quantum analogue of a classical probability distribution in phase space However it is also well known that it can take on negative values and therefore cannot be interpreted in the usual way as a probability distribution This affects all methods using the Wigner function in the Springborg paper in the OP there are regions of negative probability density in the lithium hydride molecule being analyzed Suffice to say the question of how to deal with negative probabilities is an extremely controversial subject Having said that the Wigner function has found uses in ab initio molecular dynamics when a classical phase space density is required to sample dynamical trajectories from the potential energy surface generated by quantum electronic states The regions of negative probability are usually neglected in such procedures with seemingly no loss of accuracy (to my knowledge) I am not aware of other common usages in electronic structure theory (The other problem brought up in the OP about the inaccuracies in DFT are conventionally attributed to incorrect modeling of electron correlation effects which is a matter of incorrect treatments of fermion symmetries and its consequences when projected down into coordinate or momentum space )
4381,hemija,FWIW here are some Hückel calculations on the 5775 PAH I'm showing the first 7 energy levels Each molecular graph presents the contribution of each atom to the electron density ($\mathrm{Abs}[c_i]^2$) This is not normalized I do not use wavefunctions only the coefficients multiplying them assuming $$\Psi_{\mathrm{mol}} = \sum_i^n c_i\psi_i$$ In this approximation (HMO) the first 7 energy levels will be occupied (14 pi electrons) I will first give you pictures of each energy level and then a linear combination of each Take it for what it is as there is much information lost in the process To me this simply suggests that indeed the system will behave as if composed of tropylium and cyclopentadienyl ions But this is not publication quality calculation either =) Here are compared the linear combination of the first 7 energy levels of the 5775 and 6666 systems It is clear that electron distribution is not the same in both PAHs which hints that my calculations are not so far fetched 5775 6666
4382,hemija,I would be surprised if there was reticulation of the gum's polymers upon contact with your saliva According to this manufacturer plasticizers are used and the plasticizers appear to be fats This would mean that upon chewing the plasticizers would be removed from the gum rendering it unpleasantly harder Plasticizers are used to render a polymer more pliable more elastic than it naturally is This could get its own question imo
4387,hemija,Le Châtelier's principle only states that a system previously at equilibrium will want to stay at equilibrium - that is if we perturb it it will try to go back to equilibrium This system is initially not at equilibrium and therefore we don't need his principle I think the reaction proceeds because the formation of AgCl (s) is enthalpically favored Molecular cohesion (enthalpy) is greater in AgCl than in AgNO 3 thus promoting its formation How do I know that Well it seems AgCl is less ionic than AgNO 3 since it is insoluble in water This reaction is a type of metathesis reaction It can be called double displacement reaction or salt metathesis reaction depending on who you're talking with EDIT The other answers make an error I think Both suggest that the solubility product of AgCl is of importance in determining why the reaction takes place My opinion is that the argument as presented is circular in the following manner The solubility product of AgCl in water is very low therefore we observe formation of a precipitate vs AgCl is insoluble in water therefore its solubility product is low It's easy to see that the fact AgCl is insoluble in water implies that it has a low K sp in water If someone can predict and quantify the solubility product of AgCl in water from first principles then that's another story but the explanations offered only rely on observation (experiment) to quantify the K sp My argument goes down the ladder If you can answer why AgCl is insoluble in water then you can definitely predict that Ag + and Cl - ions from different sources (i e in different solutions) put into contact will spontaneously form a precipitate Now you maybe thinking this argument is also circular but it's not We have access to experiment which clearly informs us that AgCl is insoluble in water We can confidently assume (but we might be wrong ) that the AgCl bond is less ionic than the AgNO 3 bond since it practically does not dissociate/has a low K sp in water A bond with a greater covalent character is more stable since the electrons are shared and not transfered Perhaps the crystal structure of AgCl makes it especially stable to solubilization from H 2 O I just think it's obvious that the enthalpy of formation of AgCl is favorable since it has to counter an unfavorable entropy loss in the formation of an ordered crystal lattice As you can see I cannot provide you with a thorough answer it would be extremely time consuming (for me) and I'd have to conduct many many calculations I really hope someone can clarify the why Maybe you can do your own research from the answers provided here and come back with a more elaborate answer
4386,hemija,The salt's crystal lattice's repeating unit is constituted of n molecules of salt and m molecules of water Such salts are called hydrates Wikipedia even has a nice picture of a hydrated vs non hydrated salt You can have anhydrous ferrous chloride as well as ferrous chloride tetrahydrate The number indicates how many water molecules are present in the crystal lattice's unit cell EDIT This has just crossed my mind be careful when preparing solutions of metal complexes to make sure you know what compound you're working with One gram of anhydrous salt contains more equivalents than a gram of hydrated salt This information should be obvious on the bottle If not ask a lab tech if they can identify the hydrated/anhydrous salt by memory
4628,hemija,One of way to answer this question is to put it in some QM software The system is small enough and has higher symmetry so computations shouldn't take to long even on an older computer For co-planar geometry the energy calculated at B3LYP/6-31G(d p) is –492 132206748 a u The geometry with dihedral angle of 90° between $\ce{CO2}$ and $\ce{C3N2}$ is –492 116719625 a u That makes roughly 41 kJ/mol (10 kcal/mol) Now if you really want to be sure if that's the right energy you should make frequency calculations on both structures to find out if they are minima or transition states This can take a bit longer (I haven't done it) and I would expect that the better structure (with lower energy) should have no imaginary frequencies and the higher one should have one (TS) If by some accident both of them are minima than TS has probably a dihedral angel somewhere around 45° Of course this is only computation It gives as clues but not the exact answer You can go for higher level of theory add solvent modelling and or you can take your compound and try to make temperature depending NMR spectra With a bit of luck you can freeze your TS and find out how high is your barrier p s It would be good if you mention in your post what tools (in this case computational) are available to you I really would prefer to explain how to get the number instead of giving just dry answer
4396,hemija,Other answers have emphasized the fact that oxygen is not a fuel but an oxidizing agent I'll try to underline another point there is no uniquely defined “combustion temperature” for a chemical compound This is unlike phase transitions like melting and ebullition which happen at fixed temperature for a given pressure The temperature of a combustion process depends on many parameters including the heat of combustion the ratio of air to fuel the heat capacity of both and the initial temperature The adiabatic flame temperature is one of such combustion temperatures under certain conditions The autoignition temperature and flash point on the other hand are characteristics of a given chemical compound; they are not however combustion temperatures
4406,hemija,strong Bases in general are not suitable as they will react with $\ce{CO2}$ from the air This is actually more problematic than hygroscopicity as it means that you also cannot store diluted NaOH solution (openly) as standard Instead $\ce{Na2CO3}$ or $\ce{KHCO3}$ are much better alternatives (for strong acids) The acids are either a gas (HCl) or are produced from gas I don't see how you can produce a conentration reliably enough for use as primary standard even if you would manage to produce saturated HCl it's much more difficult and less reliable than known alternatives (see below) but that wouldn't work e g for $\ce{H2SO4}$ as $\ce{SO3}$ is soluble in $\ce{H2SO4}$ (Oleum) $\ce{HNO3}$ redox reactions are not only not particularly useful but actually harmful as they use up acid equivalents $\ce{NO3- + 2e- + \mathbf{2 H+} -&gt; NO2- + H2O}$ also here a solid alternative (KH-Phthalate) is available If you don't have that you may get away with oxalic acid which is still better than the acids suggested here
4398,hemija,My understanding is that turbidity is an adjective describing the presence or degree of particulate matter suspended in a liquid (see turbidimetry ) whereas a precipitate is a particulate that can result in turbidity The turbidity generated by a precipitate is a function of its ability to form a stable suspension If the precipitate simply drops to the bottom of a container or readily flocculates the resultant turbidity will be low The turbidity of a suspension is also going to be a function of the solvent and the nature and concentration of dissolved ions - a dramatic example of this is the rapid clarification of muddy freshwater at river deltas mixing with salt water If the word turbidity is being used as a noun it is probably referring to a light-occluding suspension
