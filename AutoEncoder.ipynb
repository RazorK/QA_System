{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.utils.data as Data\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "from preprocess import generate_count_vectorizer, cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper Parameters\n",
    "EPOCH = 3\n",
    "BATCH_SIZE = 60\n",
    "LR = 0.0005         # learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, cv, answers, questions, word_ratio, answer_mapping = generate_count_vectorizer()\n",
    "X = X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vol_sz = X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = torch.from_numpy(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([600, 7574])\n"
     ]
    }
   ],
   "source": [
    "print(train.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(x, batch_size=100):\n",
    "    n_batches = len(x)//batch_size\n",
    "    x = x[:n_batches*batch_size]\n",
    "    for ii in range(0, len(x), batch_size):\n",
    "        yield x[ii:ii+batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 7574])\n",
      "torch.Size([100, 7574])\n",
      "torch.Size([100, 7574])\n",
      "torch.Size([100, 7574])\n",
      "torch.Size([100, 7574])\n",
      "torch.Size([100, 7574])\n"
     ]
    }
   ],
   "source": [
    "for ii, x in enumerate(get_batches(train, 100), 1):\n",
    "    print(x.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(vol_sz, 512),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(128, 32),   # compress to 3 features which can be visualized in plt\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(32, 128),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(512, vol_sz),\n",
    "            nn.Sigmoid(),       # compress to a range (0, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return encoded, decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AutoEncoder(\n",
      "  (encoder): Sequential(\n",
      "    (0): Linear(in_features=7574, out_features=512, bias=True)\n",
      "    (1): Tanh()\n",
      "    (2): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (3): Tanh()\n",
      "    (4): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (5): Tanh()\n",
      "    (6): Linear(in_features=128, out_features=32, bias=True)\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): Linear(in_features=32, out_features=128, bias=True)\n",
      "    (1): Tanh()\n",
      "    (2): Linear(in_features=128, out_features=256, bias=True)\n",
      "    (3): Tanh()\n",
      "    (4): Linear(in_features=256, out_features=512, bias=True)\n",
      "    (5): Tanh()\n",
      "    (6): Linear(in_features=512, out_features=7574, bias=True)\n",
      "    (7): Sigmoid()\n",
      "  )\n",
      ")\n",
      "Epoch:  0 | train loss: 0.2768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Programs\\Anaconda\\envs\\py3\\lib\\site-packages\\ipykernel_launcher.py:20: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0 | train loss: 0.2671\n",
      "Epoch:  0 | train loss: 0.2658\n",
      "Epoch:  0 | train loss: 0.2585\n",
      "Epoch:  0 | train loss: 0.2588\n",
      "Epoch:  0 | train loss: 0.2575\n",
      "Epoch:  0 | train loss: 0.2308\n",
      "Epoch:  0 | train loss: 0.2045\n",
      "Epoch:  0 | train loss: 0.1822\n",
      "Epoch:  0 | train loss: 0.1550\n",
      "Epoch:  1 | train loss: 0.1287\n",
      "Epoch:  1 | train loss: 0.0933\n",
      "Epoch:  1 | train loss: 0.0706\n",
      "Epoch:  1 | train loss: 0.0535\n",
      "Epoch:  1 | train loss: 0.0515\n",
      "Epoch:  1 | train loss: 0.0576\n",
      "Epoch:  1 | train loss: 0.0378\n",
      "Epoch:  1 | train loss: 0.0216\n",
      "Epoch:  1 | train loss: 0.0259\n",
      "Epoch:  1 | train loss: 0.0298\n",
      "Epoch:  2 | train loss: 0.0392\n",
      "Epoch:  2 | train loss: 0.0310\n",
      "Epoch:  2 | train loss: 0.0350\n",
      "Epoch:  2 | train loss: 0.0315\n",
      "Epoch:  2 | train loss: 0.0408\n",
      "Epoch:  2 | train loss: 0.0526\n",
      "Epoch:  2 | train loss: 0.0344\n",
      "Epoch:  2 | train loss: 0.0196\n",
      "Epoch:  2 | train loss: 0.0248\n",
      "Epoch:  2 | train loss: 0.0292\n"
     ]
    }
   ],
   "source": [
    "autoencoder = AutoEncoder()\n",
    "print(autoencoder)\n",
    "\n",
    "optimizer = torch.optim.Adam(autoencoder.parameters(), lr=LR)\n",
    "loss_func = nn.MSELoss()\n",
    "\n",
    "for epoch in range(EPOCH):\n",
    "    for step, x in enumerate(get_batches(train, BATCH_SIZE), 1):\n",
    "        b_x = Variable(x.view(-1, vol_sz)).float()   # batch x, shape (batch, 28*28)\n",
    "        b_y = Variable(x.view(-1, vol_sz)).float()   # batch y, shape (batch, 28*28)\n",
    "\n",
    "        encoded, decoded = autoencoder(b_x)\n",
    "        \n",
    "        loss = loss_func(decoded, b_y)      # mean square error\n",
    "        optimizer.zero_grad()               # clear gradients for this training step\n",
    "        loss.backward()                     # backpropagation, compute gradients\n",
    "        optimizer.step()                    # apply gradients\n",
    "        \n",
    "        #if step % 5 == 0 and epoch in [0, 5, EPOCH-1]:\n",
    "        print('Epoch: ', epoch, '| train loss: %.4f' % loss.data[0])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AutoEncoder(\n",
       "  (encoder): Sequential(\n",
       "    (0): Linear(in_features=7574, out_features=512, bias=True)\n",
       "    (1): Tanh()\n",
       "    (2): Linear(in_features=512, out_features=256, bias=True)\n",
       "    (3): Tanh()\n",
       "    (4): Linear(in_features=256, out_features=128, bias=True)\n",
       "    (5): Tanh()\n",
       "    (6): Linear(in_features=128, out_features=32, bias=True)\n",
       "  )\n",
       "  (decoder): Sequential(\n",
       "    (0): Linear(in_features=32, out_features=128, bias=True)\n",
       "    (1): Tanh()\n",
       "    (2): Linear(in_features=128, out_features=256, bias=True)\n",
       "    (3): Tanh()\n",
       "    (4): Linear(in_features=256, out_features=512, bias=True)\n",
       "    (5): Tanh()\n",
       "    (6): Linear(in_features=512, out_features=7574, bias=True)\n",
       "    (7): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencoder.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## evalation\n",
    "# ques = answers[13]\n",
    "# qes = cv.transform([ques.content])[0].toarray()[0]\n",
    "# qesT = torch.from_numpy(qes).float()\n",
    "\n",
    "# fea = autoencoder.encoder(qesT)\n",
    "# print(fea)\n",
    "\n",
    "# back = autoencoder.decoder(fea)\n",
    "# print(back)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTrainFeatures(ae, X):\n",
    "    res = []\n",
    "    for i in range(X.shape[0]):\n",
    "        ans = torch.from_numpy(X[i]).float()\n",
    "        fea = autoencoder.encoder(ans)\n",
    "        res.append(fea)\n",
    "    return res\n",
    "\n",
    "hiddenTrain = getTrainFeatures(autoencoder, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the best answer\n",
    "def getIndexOrderList(ques, ansLst):\n",
    "    simiList = []\n",
    "    for ans in ansLst:\n",
    "        simiList.append(cosine_similarity(ques, ans))\n",
    "    res = list(range(len(simiList)))\n",
    "    return sorted(res, key = lambda i : simiList[i], reverse= True)\n",
    "\n",
    "# print(getIndexOrderList(fea, hiddenTrain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evalQuestion(index, train):\n",
    "    ques = questions[index]\n",
    "    tar = answer_mapping[ques.peer_idx]\n",
    "    \n",
    "    qes = cv.transform([ques.content])[0].toarray()[0]\n",
    "    qesT = torch.from_numpy(qes).float()\n",
    "    fea = autoencoder.encoder(qesT)\n",
    "    \n",
    "    l = getIndexOrderList(fea, train)\n",
    "    # find the anser\n",
    "    for i in range(len(l)):\n",
    "        if tar == l[i]:\n",
    "            return i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It seems to me that the addition of electrons and protons as you move across a period would cause an atom to become larger However I'm told it gets smaller Why is this\n",
      "As you move from left to right across a period the number of protons in the nucleus increases The electrons are thus attracted to the nucleus more strongly and the atomic radius is smaller (this attraction is much stronger than the relatively weak repulsion between electrons) As you move down a column there are more protons but there are also more complete energy levels below the valence electrons These lower energy levels shield the valence electrons from the attractive effects of the atom's nucleus so the atomic radius gets larger\n",
      "\n",
      "139\n",
      "My understanding is that $\\mathrm{NaCl}$ is an ionic compound in which $\\mathrm{Cl}$ becomes (effectively) $\\mathrm{Cl^-}$ and $\\mathrm{Na}$ becomes $\\mathrm{Na^+}$ So I understand why I would get a sea of particles that would stick together But why does the above mean that it will have a face centered cubic structure with the ions held in place so rigidly\n",
      "Crystals have inspired a great many chemists because they are fascinating for a good reason Not only are they aesthetically pleasing but they serve as an excellent subject to tour a variety of theoretical subjects important for understanding high-level chemistry Crystalline materials are made up of periodic structures We're only going to primarily focus on binary compounds where there is not a high degree of covalency There are several ways to think about this problem but let's start with the melting of a crystal We say that at some definite temperature a highly ordered crystal will melt into a liquid Those of us familiar with the language of equilibrium thermodynamics might recognize that the change in free energy for this phase change can be written at constant temperature as $$ G_{liquid} - G_{crystal} = H_{liquid} - H_{crystal} - T ( S_{liquid} - S_{crystal} ) $$ $$ \\Delta G = \\Delta H - T \\Delta S $$ If we suppose that this process is spontaneous then we would say that the change in Gibbs' free energy is negative i e $\\Delta G &lt; 0$ This is true if and only if $$\\Delta H &lt; T \\Delta S$$ Traditionally we interpret this as saying that there is a thermally-driven increase in entropy when we melt a highly ordered crystal into a liquid which more than offsets the energy cost associated with the enthalpies of the interactions holding that crystal together A chemist tends to learn early on that the reverse is not necessarily true at some definite temperature a perfect crystal rarely forms from the liquid This inability to just heat up any substance and always produce a perfect crystal by cooling illustrates how crystal formation is a case of kinetic- rather than thermodynamic- control So the process by which you form your crystal could possibly result in a different crystal structure Sometimes crystal structures change just by altering the temperature of the chamber you're measuring the crystal structure in Now neither of these cases apply to sodium chloride to the best of my knowledge The formation of an ionic crystal such as sodium chloride is a delicate balance between electrostatic attraction and Pauli repulsion Electrostatic attraction says that between two different charges $q_+$ and $q_-$ there is a Coulomb force given by $$F= \\frac{k q_+ q_-}{r^2}$$ where $r$ is the distance between the two charges If one plays with the numbers then it's easy to see that at short distances the force is strongest but there is a limit to how close they may come together Eventually a repulsive force due to a quantum mechanical principle called the Pauli Exclusion Principle overpowers the attraction An equilibrium results in which the atoms sit a certain distance from one another so that if you will humor me the forces between them balance out This is why we traditionally represent crystal packing using marbles with a unique radii The radii of the hard marble represents where the Pauli repulsion overpowers the attraction You might say Sure we have these kinetic electrostatic and quantum mechanical factors to consider but how do these help with the final crystal structure Hold your horses we're getting there A famous mathematician and scientist thought about the most efficient ways to pack spheres of the same size together By most efficient I mean this in terms of what FedEx considers efficient fitting things together into the smallest possible volume This is also what electrostatics want Kepler sug\n",
      "\n",
      "534\n",
      "A reaction proceeds towards the direction of lesser Gibbs Free energy (at constant T and P) So we could say that Gibbs free energy at equilibrium is minimum On the other hand we have $$\\Delta G=\\Delta G^o + RT\\ln Q$$ where $Q$ is the reaction quotient At equilibrium $Q=K_{eq}$ and we already know that $\\Delta G^o=-RT\\ln K_{eq}$ Substituting we get $\\Delta G=0$ at equilibrium But we know that $G$ minimized itself--thus there was a change in $G$ and $\\Delta G &lt; 0$ What am I missing here\n",
      "I think your question really arises from some confusion about what $\\Delta G$ represents In general $\\Delta X$ for a thermodynamic quantity $X$ is the change of $X$ along some process You could make it clear by actually writing $\\Delta G(\\text{A}\\rightarrow\\text{B})$ where A and B are before and after states (We'll note that in the general case $\\Delta X$ depends on the path take from A to B making this notation improper If $X$ is a function of state though you're good to go ) However in the equation you quote $$\\Delta G = \\Delta G^0 + RT \\ln Q$$ the $\\Delta G$ is a free energy of reaction and should thus be denoted $\\Delta_r G$ with the correct equation being $$\\Delta_r G = \\Delta_r G^0 + RT \\ln Q$$ The free energy of reaction is defined as $\\Delta_r G = G_{\\text{products}} - G_{\\text{reactants}}$ Thus this $\\Delta_r G$ is not the variation of $G$ over the entire reaction which would be the $\\Delta G$ of the system between the start of the reaction and the equilibrium PS I think this link is the online resource I found with the clearer use and explanation of notations Notations are important in thermodynamics\n",
      "\n",
      "327\n",
      "When N N-dimethylaniline is reacted with $\\ce{H_2SO_4}$ and $\\ce{HNO_3}$ it gives mainly the meta product even though $\\ce{NMe_2}$ is an ortho / para directing group Why is this\n",
      "In the presence of these strong acids the NMe 2 group is partially protonated and the protonated form is meta-directing Under the conditions I know for that experiment you get a mixture of para- and meta-product but no ortho-product due to steric hindrance\n",
      "\n",
      "24\n",
      "Recrystallization is a nice way of purifying a product but choosing a suitable solvent if you can't rely on the literature seems like a lot of trial-and-error Are there any general rules on which kind of solvents could be used for recrystallization Which criteria should one use when trying to recrystallize a compound for which no literature on useable conditions exists\n",
      "Generally speaking the best solvent will be dependent on the impurity that you are trying to remove The solvent must dissolve both the desired compound and the impurity at a high temperature but only the desired compound at lower temperatures The solubility product of the impurity as well as the common ion effect should both be taken into consideration\n",
      "\n",
      "23\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(evalQuestion(i, hiddenTrain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
