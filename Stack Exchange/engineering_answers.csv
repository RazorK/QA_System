Id,oblast,Body
2,inzinjerstvo,"I believe it's termed injecting not pumping but yes it's certainly possible Coal fired electric power plants grind the coal to a very fine dust and then blow the dust into the central burner This particular article is a summary of some DOE research for a Blast Furnace Granulated Coal Injection System Demonstration Project And this article discusses some of the hazards from a National Fire Protection Association perspective Coal dust having the particular dangerous property of being explosive and all that As far as specifics regarding danger I suspect it will vary depending upon the material(s) involved in the powder"
8,inzinjerstvo,"Unlike within the United States and the PE license it doesn't appear that it's possible to receive an EUR ING license by direct comity EU Engineering licensing is handled by FEANI and on their EUR ING page they state Application is open only to individuals if they are members of an engineering association represented in FEANI through a National Member But it may be possible to join one of the German FEANI members and see if they can support a reciprocity or comity process 1 1 There are other member organization in other countries but your question specifically mentioned Germany Digging a bit deeper it does appear that there may be limited cases where an EUR ING will be awarded based upon sufficient experience as a professional engineer Take a look through the FEANI EUR ING guide in particular Section 5 which details the minimum requirements Most of the cases cover qualifications based upon an education received within an FEANI member country Section 5 4b deals with special circumstances and provides the potential pathway that you could receive an EUR ING license Nevertheless it is possible to consider such alternative routes Very strict procedures however have then to be followed (see 7 1) and the applicant must have at least 15 years of Professional Engineering Experience recognized by FEANI So it does appear to be possible but again it's not as simple as the comity process within the United States"
19,inzinjerstvo,"The combustion chamber and nozzle of the Space Shuttle main engine were cooled with liquid hydrogen Liquid hydrogen was also the fuel It was used as a coolant before it was burned ( regenerative cooling ) Another example is the air breathing rocket engine SABRE Here too liquid hydrogen is used as fuel It's also used for liquefying the atmospheric oxygen In the industry one advantage that liquid helium and nitrogen have over hydrogen is that helium and nitrogen are inert and non-combustible Probably less corrosive too p s The question has a superconduction tag So I don't know if this is the sort of application of liquid hydrogen cooling that the O P is looking for"
24,inzinjerstvo,"Most stainless steels contain nickel around 10% It is combined normally with chrome and molybdenum The main problem that a natatorium (i e swimming pool) is a highly corrosive environment and most stainless steel alloy is a little bit slowly corroding here Even aluminium gets a greyish-white surface layer In the case of constructive materials the situation is a little bit better First because they are mostly in the wall and the concrete around them partially protects them In case of freely available surfaces plastic or painting is yet used"
30,inzinjerstvo,"The work history and temperature history of any metal can make a big difference in the final shape after machining If the material was cold worked (e g rolled) there may be significant residual stresses within the material When you start selectively removing material those stresses may cause the part to warp into a new shape Cast metals and ones that have been given surface treatments can have the same problem You might be able to address the problem by roughing the part to near-final shape and then running a finishing pass to remove that last little bit of material Otherwise take a look at the specifications of the material You may need to heat treat the part to relieve those stresses This is a problem that can exist with just about any metal not just Al"
36,inzinjerstvo,"The main problem which I can see in your idea that your system will have a cumulative error Only calculating this won't be enough you will have to find alternate solutions too In similar (but maybe bigger) scenarios for exampe for drones there is a similar problem The solution is using the wheel rotation counters to get a fast real-time but buggy input data (which contains the cumulative error as well) In case of flying drones this data is coming from giros and accelerometers your task is much simpler compared to their But you should get an alternate information source too (In case of the drones it is normally the GPS) This can be GPS or some other thing - there is a wide spectrum of possibilities Visual image processing Precalibrated ultrasound markers Marker paintings on the floor If I suspect correctly the size of your experiment maybe the last would be most promising to you"
39,inzinjerstvo,"Paradoxically maybe the home-sized magnetic levitation could be a narrow area where superconducting solutions could be cheap I explain why Although room-temperature superconductivity is a dream room superconductivity is not -) There are soon superconducting materials which can be cooled by liquid air and they aren't even costly Liquid air is also relatively cheap Maybe a liquid air processing machine were costly but you don't need to buy that Only to buy the superconducting alloy and the liquid air (nitrogen) For the first I tried ebay But it depends what you want to reach I once played chess with figures levitated by Meissner-effect The result could be"
40,inzinjerstvo,"Double nutting can be an easy option for this I was about to describe it myself but these folks do a great job This image from the article models the forces that do the job Take note of the contact surfaces between the treading of the bolt in the top vs bottom nut"
41,inzinjerstvo,"Very probably no Galvanic corrosion required the existence of a reactive and conductive medium which you don't have If there is some (for example at least moist air or similar) best you can do to paint both of the metals if you can Generally if none of the materials would corrode in an environment galvanic corrosion is a non-issue (in that environment)"
43,inzinjerstvo,"You could try Wikipedia but they note that the article may not meet there quality standards so I'll stay away from there as a source The World Nuclear Association seems more credible anyway Their page discusses the process Quoting the section entitled Reprocessing today – PUREX (emphasis mine) All commercial reprocessing plants use the well-proven hydrometallurgical PUREX (plutonium uranium extraction) process This involves dissolving the fuel elements in concentrated nitric acid C hemical separation of uranium and plutonium is then undertaken by solvent extraction steps (neptunium – which may be used for producing Pu-238 for thermo-electric generators for spacecraft – can also be recovered if required) The Pu and U can be returned to the input side of the fuel cycle – the uranium to the conversion plant prior to re-enrichment and the plutonium straight to MOX fuel fabrication Block of text $\to$ coherent summary Chemical solvents are used Further below the WNA notes The used fuel is chopped up and dissolved in hot concentrated nitric acid The first stage separates the uranium and plutonium in the aqueous nitric acid stream from the fission products and minor actinides by a countercurrent solvent extraction process using tributyl phosphate dissolved in kerosene or dodecane In a pulsed column uranium and plutonium enter the organic phase while the fission products and other elements remain in the aqueous raffinate In a second pulsed column uranium is separated from plutonium by reduction with excess U4+ added to the aqueous stream Plutonium is then transferred to the aqueous phase while the mixture of U4+ and U6+ remains in the organic phase It is then stripped from the organic solvent with dilute nitric acid The plutonium nitrate is concentrated by evaporation then subject to an oxalate precipitation process followed by calcination to produce PuO2 in powder form The uranium nitrate is concentrated by evaporation and calcined to produce UO3 in powder form It is then converted to UO2 product by reduction in hydrogen Compounds of importance tributyl phosphate kerosene and dodecane and aqueous affinate are used as solvents after the nitric acid Then there is a reaction with $U^{4+}$ and uranium and plutonium are separated That's the chemical part I haven't been able to find much information on the specific mechanical methods (i e the pure engineering you were looking for) In fact the original patent for PUREX does not cover the mechanical processes leading me to believe that the exact setup can vary This pdf is somewhat helpful It says that the input materials are first chopped up Perforated baskets are then used to separate this machinery from the rest of the solution; a wheel dissolver may be used I do have some information albeit for a non-PUREX method For aqueous reprocessing (go to page 12) mixer settlers (mixing chambers with settling chambers) columns and centrifuges are the main pieces of equipment"
53,inzinjerstvo,"Based on it being an EN steel grade The first number is 100x the carbon content percentage (so 0 11%) the letters are added elements (sulphur and maganese) and the last number is the sulfur content (0 30%) You can see the full details here The full format seems to be [X][% carbon][added elements][% of added elements hyphenated] Note that the X is only present for stainless steels Here is a good example Note also that this scheme is somewhat ambiguous The percentages are only an approximation and the example you gave is interesting because it lists Sulfur before Maganese despite the naming convention stating that they should be listed in order of content That's alright for getting quick basic info about the steel but for anything else you may want to use the EN number 1 0715 rather than the name Wikipedia has details on the format Given this classification you can find out much more about the steel's properties and see what general category it fits into The site I linked first says this EN 10277-3 2008 Bright steel products Technical delivery conditions Free-cutting steels EN 10087 1999 Free cutting steels Technical delivery conditions for semi-finished products hot rolled bars and rods"
58,inzinjerstvo,"I don't think they existed and it has its reason First a solar panel can be characterized mainly by its efficiency spectrum on which wavelength which ratio of light energy can it convert to electric power This needs to have its maximum around the visible light becausd the Sun gives most of its energy in this wavelength interval This is because our eyes can see best in this spectrum We simply evolved to the sunlight And this is because home light is also in this wavelength this is what we human like at most There were no need for different solar panels But the power of the sunlight is around some hundred $\frac{W}{m^2}$ although it varies very heavily The light power of a house bulb is around sometimes 10W - not for a $m^2$ but for a whole room Maybe we see in a well lighted room just so good as in sunlight but it is only because our eye is very adaptive The actual light power density is tenth or even hundredth smaller as in sunlight And the efficiency of most solar panels is around 10-20% There are experimental very costly version reaching 40% A solar panel in a room couldn't produce valuable energy at most some watts - on the cost of the price of a solar panel on the roof And the cost is their main problem even with their many many times bigger solar power"
64,inzinjerstvo,"Customer specified acceptable brightness is going to be a subjective affair unless you have something to calibrate their responses against Loosely you'll need to measure the available light within a particular area and then correlate that measurement with customer surveys With a large enough sample of responses you should be able to determine what constitutes an acceptable level of lighting for your customers I would expect that customer responses will fall along a curve of some sort and you'll eventually be able to determine what the standard deviations are for acceptable One factor to be concerned about is that differing rooms will have differing levels of reflection So even though two rooms may be of similar size and furnishings the more darkly painted room will reflect less light and will be viewed as more dim on a subjective basis To workaround that you might create a dedicated test room and have potential customers come through and provide their feedback While this is a more expensive approach it's a very good way to isolate out variables that you can't otherwise control against To get you started with measuring available light you'll likely need a light meter of some sort This and this are just two examples of what I'm sure you'll be able to readily find with some searching Once you have a lux measurement for the area you can convert that to lumens using a standard calculator Searching will turn up a number of sites such as this one I didn't put the equations here as there are a number of steps involved including determining the area of the room"
69,inzinjerstvo,"Short answer make it thicker Long answer The moment of inertia affects the beam's ability to resist flexing Use one of the many free online moment of inertia calculators (like this one ) to see how increasing the height of the beam will have an exponential effect on increasing the stiffness of the beam And this site helps provide a pictorial view of the load(s) upon a beam depending upon differing configurations such as where the supports are and where the load is applied It also provides a calculator to determine the forces involved Wikipedia has a decent article for area moments of inertia In your particular case you're asking about a filled rectangular area and I x = bh 3 /12 The height has an exponential factor of 3 whereas increasing the base does not have an exponential factor So for the same amount of material increasing the height stiffens the beam better To be clear you can make the beam sag less by increasing the width of the plate It's just more effective to make the plate thicker Current moment I x = 30 * 2 3 / 12 = 20 mm 4 Increase width by 1mm I x = 31 * 2 3 / 12 = 20 6 mm 4 Increase height by 1mm I x = 30 * 3 3 / 12 = 67 5 mm 4 And if for some reason you can't easily increase the thickness of the plate you can consider a different beam structure Currently your beam is a simple rectangle You can easily use a T-beam or an I-beam in order to stiffen the plate instead Again while I've provided some suggested links to online calculators feel free to search for and use others that you may prefer"
70,inzinjerstvo,"It sounds like what you're looking for is a bandpass filter It will filter out any signal outside of a given frequency range If the noise isn't too strong the main signal should come through fairly clearly The actual design of the bandpass filter is another matter depending on what frequency range you need and how prevalent the noise is It may not spit out a perfect sinusoid but it might get you close enough that you could use your least-squares method on the bandpass output to generate the sinusoid"
75,inzinjerstvo,"Most bridges (and overpasses) are built to cross over something With a few notable exceptions most of these somethings are relatively long perpendicular to the desired crossing direction and fairly narrow parallel to it Therefore a simple two directional bridge best meets the needs of the engineering problem Engineers always try to solve a problem in the simplest possible way to prevent introducing more problems than they have solved If a simple two-way bridge solves the problem then there is no reason to complicate the solution Along those lines vehicular traffic flow on a three-way bridge can be fairly complicated and may not easily facilitate high traffic volume I believe that is why you will find more pedestrian bridges created in this style As for Michigan I don't know It's entirely possible that a structural engineer or engineering company in the area had a fondness for that type of structure and bid on those projects with three-way designs"
76,inzinjerstvo,"I think you are talking about roundabouts not traffic circles It is baffling to those of us in the UK that Americans think roundabouts are a new idea In the UK we have so many variants from mini-roundabouts all the way up to full motorway junctions (a giant roundabout above or below the motorway) So do roundabouts take up more space Not necessarily this is a mini roundabout It's nothing more than a slightly domed area of paint on the road no lights are necessary you can actually drive straight over the top of it rather than around it its main purpose is simply to dictate who has right of way so that everyone knows who should yield and who should go In a busy town or city environment roundabouts do not work well because excessive traffic from one direction with right of way can completely stop all other traffic causing congestion in other directions Some roundabouts have lights or peak-time signals to prevent this One great thing is that they're easy to modify (adding lights making it mini (drive-overable) adding another entry-point etc Everywhere other than in busy grid-based towns/cities they are ideal So from a highway engineering perspective The main pros are Cheap to build Agile (Flexible / extensible) Scalable to suit any junction size Mutable (add peak signals bypass lanes extra incoming roads) Modular (google image search for double roundabout magic roundabout ) Easy and safe for drivers to use (rules don't change in any configuration) Aids navigation of complex junctions from simple road-signs (just count the exits) Cons are Annoys drivers on country roads when you'd like to just bypass Not suitable for busy city grids"
83,inzinjerstvo,"You can use a sort of double diameter washer I've made that name up but I am pretty sure such a device exists That's a cross section of one of your linkages The white things are the aluminum pieces the grey one is the screw the yellow is the nut while the red is the double diameter washer If you can found some washer that is slightly longer than your aluminum bars thikness you are good to go Of course if it's too long the linkage may become lousy but with a good cutter you can trim them down or you can also add some regular washers between the red one and the nut"
90,inzinjerstvo,"First of all the main documents you need to know DIRECTIVE 2004/108/EC relating to electromagnetic compatibility http //eur-lex europa eu/LexUriServ/LexUriServ do uri=OJ L 2004 390 0024 0037 en PDF DIRECTIVE 2006/95/EC on low voltage devices http //eur-lex europa eu/LexUriServ/LexUriServ do uri=OJ L 2006 374 0010 0019 en PDF (and the version entering into force in April 2016 http //eur-lex europa eu/legal-content/EN/TXT/PDF/ uri=CELEX 32014L0035&amp;from=EN ) Those documents form the basis of your work Also they define if your device is subject to the directive or not Depending on the case you could also be subject to the R&amp;TTE Directive and RoHs Directive) The Annex I of both documents informs us on the essentials requirements to meet Mainly it says that your product must be safe for humans pets and property and that it should not be impacted by the electromagnetic emissions of other nearby devices nor it should impact them with its own emissions What's really interesting lies in the Annex II of the first document and in Annex IV of the second document These parts defines the conformity assessment procedure you'll have to follow There are 8 procedures (called modules) of conformity assessment internal production control (module A); CE type-examination (module B); conformity to type (module C); production quality assurance (module D); product quality assurance (module E); product verification (module F); unit verification (module G); full quality assurance (module H) In our case the one to follow is called Internal Production Control Basically what you have to do is write the technical documentation This document should be made of those parts a general description of the electrical equipment conceptual design and manufacturing drawings and schemes of components sub-assemblies circuits etc descriptions and explanations necessary for the understanding of said drawings and schemes and the operation of the electrical equipment a list of the standards applied in full or in part and descriptions of the solutions adopted to satisfy the safety aspects of this Directive where standards have not been applied results of design calculations made examinations carried out etc test reports We learn what we have to do in order to respect the directives The main constraint here will be showing that you have indeed accessed the harmonised standards This can cost quite a hefty sum of money However Afnor (the French certification organism) provides subscription to their database which can be cost effective compared to buying individual standards) Your local certification organism should have a similar offer The harmonised standards list to be followed is available here for the EMC directive http //ec europa eu/enterprise/policies/european-standards/harmonised-standards/electromagnetic-compatibility/index_en htm for the LVD directive http //ec europa eu/enterprise/policies/europ"
93,inzinjerstvo,"In case Eurocodes do not provide enough information some sources exist In the case of elastic critical moment for lateral-torsional buckling an NCCI (Non-contradictory complementary information) document exists The document code is SN 003 and one version (maybe not the latest) can be accessed here Hopefully this will cover your current needs In addition the French technical centre for steel construction has developped a couple pieces of software that can run the calculation for you LTbeam for beams under bending sollicitations LTbeamN for beams under bending and compression sollicitations"
96,inzinjerstvo,"I will leave the main part of the previous answer below as it contains more information about the differences between several radio protocols As for the differences between Zigbee 3 0 and Z-Wave let's do a side by side comparison Frequency Zigbee uses the 2 4GHz band while Z-wave uses the 868MHz band in Europe and the 900MHz ISM band in the US (see the frequency coverage here ) OSI Layers Zigbee is based on the IEEE 802 15 4 spec for the PHY/MAC Layers where Z-Wave is based on ITU-T G 9959 rPHY/MAC The Z-Wave protocol stack was developped by Sigma Designs (and the standard is maintained by the Z-Wave Alliance) whereas Zigbee is developped by the Zigbee Alliance Both networks have mesh capabilities Ecosystem Z-Wave claims to have more than 1200 products on the market Zigbee claims a similar figure Documentation Zigbee 3 0 spec is (for now) only available to Alliance members (the other Zigbee standards can be downloaded after entering some information (which can be bogus if you somehow want to stay anonymous)) Z-Wave requires you to buy the dev kit or join the Alliance to access the spec Certification Both protocols will require a certification to be fully certified and to be able to use the protocol logo More generally here is what you need to take into account when choosing a protocol Frequency The protocols you named all uses the 2 4GHz band but a lot of other protocols uses the 868MHz the 915MHz the 433MHz band The idea is the lower the frequency the lower the energy use and greater the range At least in theory One also have to consider the physical size of the antenna (the lower the frequency the bigger the antenna) OSI Layers used Some of the protocols out there only implements one layer of the OSI model and you will need to have another layer implemented to be able to communicate Ease of use of protocol some protocols are very easy to use (for example EnOcean) some are more complicated (Zigbee BLE) They differ in their capabilities and the freedom you have to send the information Bitrate frame size useful payload How much information you can send in a given timeframe Depending on your use you may need to send a lot of data where different protocols may or may not behave well with this Collision and packet collision avoidance how does the protocol manages the band use and prevent packet collision (two objects sending at the same time) Ecosystem Are there a lot of other devices out there using this protocol What do you want to connect your object to (smartphone directly Internet directly Something else) Do you need a local gateway to access internet Documentation availability Is all the information necessary freely available Is it locked (does it require membership to an association to be read) Is it behind a paywall Last but not least Cost of use To be implemented and sold in your products some protocols will require you to join the Alliance so you can certify your product compliance with the protocol This often cost a lot of money (depends on the protocol) I thinks this pretty much sums it all and should give good pointers to the right direction to look for more information"
99,inzinjerstvo,"In general you want to stay below the recrystallization temperature Steel is composed of grains and different types of steel have different grain sizes The size of these grains affects the steels behavior once it gets past the yield point At the recrystallization temperature new grains will nucleate and grow which undoes any sort of hardening that the steel may have previously gone through However this temperature will vary depending on the alloying elements in the steel so if you don't know the grade it'll be hard to know the recrystallization temperature Unless you're going to be working it around 900°F (500°C) I don't think that should be an issue This chart shows the temperatures at which different heat treatments are done Source"
104,inzinjerstvo,"This is something that I've looked at with tunnels rather than pipes and arguably with smaller deflections Hopefully it'll be some help however If you can satisfy yourself that the rate of curvature is relatively small- then you can approximate the stress generated by the lowering using simple beam theory $$\sigma=\frac{Ey}{R}$$ Where E is the Youngs Modulus y is the distance from the neutral axis you are measure the stress at (i e the radius of the pipe itself) R is the radius of curvature Mechanically you'll need to check that the stresses in the pipe along the parabaola that defines your curve never exceed the limiting stress of the pipe material and (probably more critcally) the capacity of the connections This is a bit of a simplification and depending on the type of pipe the joints themselves will have some play"
105,inzinjerstvo,"Two points make a line so a long tube with a photodiode on the bottom should be able to find the sun (assuming no clouds ) In regards to Solar Photometry I think Forrest Mims is making photometers from the wavelength response of different LEDS using LEDs as detectors"
111,inzinjerstvo,"I know your question is about springs but I don't think that springs are the answer to your desired application As you mentioned the inward force from air pressure is very large so not only do you need a strong spring but you also need a material for the paneling which is light and can take those kind of forces Additionally you would need to evacuate the air that seeps in through the material at a molecular level There are good reasons lighter-than-air aircraft use a low-density gas inside the balloon There is no requirement for exceptionally strong and light materials There is no (or very low) pressure differential between the inside and outside and therefore no tendency for the gas to escape or the atmosphere to seep in You may be interested to read the Wikipedia article on vacuum airships in particular the section on material constraints This shows the proof that even using a strong shape (hollow sphere) made of diamond the pressures are too great for the structure to withstand once you thin it down to be light enough to become buoyant Given that in these ideal conditions it is not possible it seems unlikely that a spring in a balloon would be possible However the Wikipedia article does leave open the possibility that if the wall of the sphere was not solid for example if it was made of a honeycomb structure then it may still be possible I have no idea if anyone has done the math on this though"
114,inzinjerstvo,"There are a number of standards to address hydrogen embrittlement in the manufacture and coating of fasteners Here in the US ASTM F1940 and ASTM F519 would apply Hydrogen embrittlement is also a concern for structural welds but should be addressed by your welding code The problem doesn't seem to be in the spec as often as in the quality control at the point of manufacture In situations where this is a big risk requiring third party inspections and testing are likely the most effective step the engineer can take"
115,inzinjerstvo,"So there's an incorrect assumption underlying your question In an ideal world the lifting capacity required of the jacks would be the self-weight of the bridge divided by the number of jacks (+ allowances for wind/snow etc ) And the assumption there is that the lifting capacity is equivalent only to the weight of the bridge The problem is that if anything goes wrong you're likely to see a catastrophic failure of some sort which could lead to irreparable damage Real world lifts don't operate in that ideal manner and instead rely upon a safety factor in order to make sure that the lifted weight is well within the limitations of the equipment And in some cases the safe working limit (SWL) may be derated further from the working lift limit (WLL) if there are any extenuating circumstances such as worn equipment or hazardous weather So the ideal lift capacity is one that is significantly larger than load to be lifted The actual lift capacity used is tempered by the fact that you generally pay for that lift capacity whether you need it or not According to the Wikipedia article on safety factors a factor of 2 is common with building materials and 3 is common for automobiles You need to weigh the risk to human health or safety within the lift you're considering and use an appropriate safety factor A conservative approach would be to use a higher safety factor of 3 so you need at least 3 times the bridge weight for the lifting capacity Assuming you're staying within the SWL and WLL of the lift equipment you're using that still doesn't necessarily account for the binding forces caused by corrosion between the bridge and the bearings supporting it Static friction can also come into play if the bridge itself has to be slid out of the supporting structure Unfortunately it's hard to determine what that binding force is going to add up to without a lot more detail At a minimum you would need to know the materials involved and the cross sectional area of contact between them You would also want to approximate how long they've been exposed to the elements and what sort of conditions the elements have brought - such as salt water exposure vs mountain air This is where I'm going to wave my hands in an airy fashion and not attempt to swag the binding force created by corrosion Static frictional force can be guessed at a little more easily than the corrosion binding $\mu_{static}$ ranges from 0 6 to 0 8 for various materials in contact with steel And while the general formula for calculating the Force from friction is $F_{static} = \mu_{static} * F_{normal}$ that equation also assumes horizontal movement As you're likely lifting vertically not sliding horizontally the static frictional forces will be less since the equivalent $F_{normal}$ for vertical motion will be less Based upon that I'd use a conservative guess of $\frac 23$ or $\frac 34$ of the weight of the bridge to estimate the static friction forces involved in the lift Experience and the particulars of the lift will guide you in adjusting that guess up or down So depending upon your safety factor it could very well be that the SWL of the equipment will provide sufficient lift to overcome any static friction or binding caused by corrosion Or it could be that you need to increase the lift capacity requirements to overcome that effect And it's worth pointing out that equipment can exceed those limits so a lower safety factor may be good enough to get past the effects of static fr"
116,inzinjerstvo,"Considering that reverse osmosis is not the only way to desalinate water I think that yes there is a lot of development potential in desalination but that potential might not lie in improvements to existing techniques To justify this conclusion and illustrate some areas where there could be a lot of development potential I present to you my idea for a combined wave wind and solar desalination and power plant I haven't done any maths on this to calculate the area of land needed or costs or output so it might not be viable as-is But I think the concepts described below (and remember this is just one idea) demonstrate that there is development potential in the following areas Using renewable energy sources on-site to power the plant Using direct-drive energy instead of electrically transmitted energy Directing and amplifying natural processes of desalination Combined wave wind and solar desalination and power plant Inputs No external energy input Cleverly harnessed wave wind and solar Outputs Energy (electricity) Fresh water Cool air Location This plant requires a hot location with large area of cheap land by the ocean and a relatively consistent wind Stage 1 - Wave Pump A wave-powered pump raises sea water into a large lake on land Here is an example of a direct wave-powered pump other types of wave power harnessing typically convert mechanical motion into electricity However that motion can be easily used to directly drive a pump Stage 2 - Evaporation Lake The evaporation lake is a large shallow area covered in a greenhouse-like way to aid evaporation The sea-water flows away from the ocean along channels in the lake-bed then back again towards the ocean in the next adjacent channel where it drains back into the sea This prevents the build-up of deposits as the returning sea-water will take them with it and return to the sea more concentrated The roof may contain Fresnel lenses or other solar concentrators to help evaporation A wind-catching tower blows air across the lake to lower air pressure and aid in evaporation This tower could be like those used in Masdar City or a standard wind turbine tower with either electrical or direct transmission to a series of fans The result is a continuous airflow across the lake which carries the water vapor to the far side where it is channeled up a wide column into the next stage Stage 3 - Condensing Tower The water vapor is channeled up a large column to a condensing chamber high on the tower Here a series of fins are cooled by a heat-pump driven directly by a wind-turbine on top of the tower the water condenses on the fins and drains into a fresh-water tank near the top of the tower Stage 4 - Power Generation The water from the condensing tower is lowered to a height suitable for a standard water-tower through one or more water turbines to generate power Stage 5 - Filtering and treatment The salty sea-air will also condense on the fins and there may be small airborne particles and particles from wear on stages within this process that get into the water so it will probably need further filtering and treatment to make it drinkable Some of the power from the water turbine may be used for this There you have it you have clean water above ground level so pressure is already availabl"
118,inzinjerstvo,"I'm not a pump expert but I would have thought that an Archimedes screw pump would be ideal in such a situation"
123,inzinjerstvo,"I've found a fairly comprehensive source There are five main types Mechanical pumps A piston and a rotary valve are used the push the concrete through Hydraulic pumps Hydraulics are used to pump the concrete Schwing pumps Two cylinders are used One to receive concrete from the input hopper and one to release it into the output pipe The two cylinders are connected by a flat gate valve Apparently Fredrich Schwing started a company to manufacture concrete pumps Unfortunately their website does not have much information Thomsen pumps Similarly to the Schwing pump two cylinders are used with internal pistons The difference is that a flapper valve (a type of check valve ) is used Squeeze-crete pumps A rotating cylinder is used to squeeze the concrete through"
124,inzinjerstvo,"I think I know what you mean The terms selling and salesmanship are somewhat confusing in this context Really you just need to be able to explain the situation and the reasons behind it in a context that the other side can understand This differs when talking to management or engineers I would call this more justification than selling 1 Justifying to management If you are talking to management to get a green-light for an idea you need to not only explain why it is technically viable but also what the business benefits are of this idea above other ideas For example it costs less to initially implement; it is easier to maintain (costs less); it is harder to maintain (creates lucrative service contracts) What is 'good' really depends on the business strategy of the company as highlighted by the last two examples which are opposite from a technical perspective but both might be considered beneficial under different business strategies If you don't like or disagree with the business strategy of your company leave and start your own business and treat your customers better Or at least you can begin to understand why your ideas are not adopted despite being technically fantastic 2 Justifying to engineers It sounds like you've already got the go ahead from management and now you're struggling to convince other engineers why this way is actually better Well the reason you're struggling is because those other engineers don't understand the business strategy and how your idea fits in with it So now you find yourself in the odd position of having to explain the business strategy to engineers You don't have to justify the business strategy itself only why your technical solution fits in better with the business strategy dictated to you from management You need to do this in terms that non-business oriented engineers will understand which can be tricky because quite often the best technical solution is at odds with the business strategy Example I'll use a real-world example that I had to deal with recently to explain This is from a software company but applicable to any engineering situation The company wanted to have two separate work streams (two teams) doing essentially the same thing creating two different products (that do the same thing) for two markets Utter madness technically especially when both products could share so many components and in fact could even be the same product with some minor configuration options to address the differences in these two markets Engineers were struggling to agree with this approach until I explained to them that 80% of the revenue was coming from one market and the competition there was also much more fierce so the business strategy was to focus one team solely on this market without any consideration for supporting the other market so that they could move quickly and stay ahead of the competition The secondary market was still worth exploiting and growing so the second team would focus on that This is the strategy given to us this is the problem we need to solve not that other problem of making the single-product Conclusion I don't think you need to learn sales you just need to be able to identify when other engineers are trying to solve a different problem than you (normally caused by ridiculous non-technical things like customers profit economics and all that malarkey) Then you need to be able to explain what the problem actually is and why so that they jump on board with your (already accepted by management) idea"
131,inzinjerstvo,"This is quite a vague question so only a very broad and vague answer can be given Here are some ways to achieve this Invest in marketing to increase sales or hold more stock so that you can buy parts in larger quantities and get better pricing Negotiate with suppliers and make a commitment to buy a large quantity over time in order to get better pricing without a large initial investment Look for areas of your design where some parts are potentially unnecessary For example non load-bearing parts may be clipped on or glued in place rather than using bolts Fewer or smaller parts may be used in some areas etc Split your product into a core product with optional add-ons to reduce the base-price for entry-level customers Look for alternate materials and alternate parts that achieve the same function without compromising the product design goals"
134,inzinjerstvo,"You are correct that the cutting speed of the material is what determines the rpm for your drill-bit This actually makes the calculation very simple $$ \text{Spindle speed (RPM)} = \frac{\text{Cutting speed}}{\text{Circumference}} = \frac{\text{Cutting speed}}{? \cdot \text{Diameter}} $$ The thing you need to be careful of is the units of cutting speed and diameter For example Metric If your cutting speed is in $m/min$ and your diameter is in $mm$ then you need to multiply your cutting speed by 1000 so that it is in $mm/min$ Imperial If your cutting speed is in $ft/min$ and your diameter is in $inches$ then you need to multiply your cutting speed by 12 so that it is in $inches/min$ For more information see spindle speed calculations on Wikipedia"
139,inzinjerstvo,"The primary difference between Rankine and Coulomb earth pressure theories is that Coulomb's considers a frictional retaining wall In other words the interface between the soil and the retaining wall is not assumed frictionless (as it is in Rankine theory) That being said it is typically considered that Rankine underpredicts the true orientation of the failure surface whereas Coulomb overpredicts the orientation In that sense you could use both methods and use the two solutions to bound what will likely occur Terzaghi (and Peck)'s method is largely empirical It simply uses the soil's classification and the backfill slope then they simply tabulated coefficients of lateral earth pressure That being said they aren't bad it's just that any empirical solutions like this tend to be relatively site specific so the solution needs to be taken with a grain of salt"
142,inzinjerstvo,"A yahkchal is an example of a type of passively cooled building in Iran They utilise a combination of passive evaporative cooling and thick thermally insulating walls in order to keep the interior temperatures low enough First wind is directed into underground aquifers known as qanat They are then cooled due to the low humidity desert air causing water to evaporate The cooled air then flows through the interior of the yakhchal cooling the interior The thick insulating walls (filled with earth and various insulating materials such as straw and feathers) help to insulate the cool interior from the hot exterior therefore maintaining a low temperature inside the yakhchal"
145,inzinjerstvo,"You need to ensure that even in the worst case scenario you still meet your measurement spec of 10 +- 1mm If your tolerance is 0 2mm of your measurement then a measurement of 11mm while may look like it meets spec it doesn't because it could be 11 1mm So the worst case that still meets your spec is a measurement of 10 9mm because then with a max tolerance of 0 2mm you still meet 11mm With a 0 2mm tolerance your 10 +-1 spec becomes 10 +- 0 9 How should a measurement of 9 9mm be treated So revised spec is between 9 1mm and 10 9mm so 9 9mm is within spec"
149,inzinjerstvo,"I would think the friction regimes will be very different especially for something like a gum barrel where the projectile is contained At high speeds plastic deformation of the surface will be much more significant as well as possible chemical reactions due to the heat/energy Therefore I wouldn't necessarily expect the coeffcients to be similar You could possibly could set up a test rig using a high speed touching a flat plate to test the friction at high speed outside a barrel However I haven't thought about the details and this would probably be difficult\expensive\dangerous to set up A quick google gives me this paper which supports my theory that friction would not behave consistently"
159,inzinjerstvo,"Keep in mind that P-N junctions are created in the first place by diffusing impurities (dopants) into the silicon at elevated temperatures (1000 - 1500 K) The efficiency of the junction is related to how sharply the dopant concentration changes at the junction (its gradient) At lower temperatures such as those to which a solar array is exposed (say 270 - 330 K) diffusion is much much slower but it does not completely stop Given enough time the dopant gradients become less steep reducing the effectiveness of the P-N junction Over the extreme long term (millennia) semiconductor devices will stop working as their junctions disappear altogether"
161,inzinjerstvo,"It's common to start with a shorter stiffer tool such as a center drill or a spotting drill In addition using the shortest drill bit that drills the hole you need will increase stiffness Because of the flutes in a drill the stiffness goes down geometrically as the length increases The other variable you have control over is how you are holding the tool A collet will keep the tool in better alignment than a chuck If you're doing this often it's worth making sure you have a good grasp of feeds and speeds and that you're using the best type of drill for the job Both of those could have some affect on hole perpendicularity"
164,inzinjerstvo,"As it happens I just recently went through that calculation myself for a different site Given the following facts from a quick web search it isn't difficult to work out the numbers The maximum efficiency of a (large) windmill is about 40% The density of air is 1 225 kg/m 3 You need about 50 mW (10 mA at 5V) to light up an LED First we'll need about 50 mW / 0 40 = 125 mW of air power flowing through the windmill to create the electricity we need (ignoring other factors such as the actual efficiency of a small windmill and the efficiency of the generator) The power of the air flowing through the windmill is 0 5mv 2 where m is the mass rate of the air flowing through the disk defined by the diameter of the blades For example suppose we have a disk of 0 03m 2 (about 20cm in diameter) The mass rate of the air is the area of the disk multiplied by the air velocity multiplied by the density of the air $$\text{Mass rate} = 0 03 \text {m}^2 \cdot v \cdot 1 225 \text{ kg/m}^3 = v \cdot 0 03675 \text{ kg/m}$$ The power of that air is therefore $$P = 0 5 \cdot \text{Mass rate} \cdot v^2$$ Substituting and solving for $v$ $$v = \sqrt[3]{\frac{0 125 \text{ W}}{0 5 \cdot 0 03675 \text{ kg/m}}} = 1 9 \text{ m/s}$$ or about 7 km/h Taking into account the efficiencies we ignored earlier plus the losses in a gear train that might be needed to get the generator RPMs up to a usable level I would probably shoot for about 4&times; the area or about 2&times; the diameter (40-50 cm) in order to get reasonable results"
167,inzinjerstvo,"The transistor The transistor was the revolutionary replacement of the vacuum tube which had been at the heart of computers for the first half of the 20th century Vacuum tubes themselves had only two main problems They were power-hungry and they were relatively big Relative to their replacements that is They also had a tendency to burn out or leak during operation which could prove disastrous In 1947 John Bardeen and Walter Brattain along with William Shockley their boss at Bell Labs successfully amplified an electric current using germanium This point-contact transistor as it was called was soon used to speed up computing and to make computers smaller and more efficient A good example of the transistor and the vacuum tube is the construction of the Manchester Computers developed at the University of Manchester The first the Small-Scale Experimental Machine (SSEM) (developed in 1947) was a state-of-the-art testbed for new innovations in computing such as the Williams tube But it still used vacuum tubes It had 550 valves and took in 3500 watts of power The SSEM's successor the Manchester Mark 1 (developed in 1949) was much more powerful It used 4050 valves and consumed 25000 watts of power Yet the aptly-named Transistor Computer built in 1955 used only 200 transistors and 1 300 diodes and only used 150 watts It wasn't the first computer to exclusively use vacuum tubes but it was a huge step forward It's tough to say exactly why the transistor was created when it was (I'm answering the last part of your question now) but it could be argued that the computational advances of World War II (such as the Harvard Mark I ) ensured that many new advances would be made in computing; the transistor happened to be one of them The integrated circuit The integrated circuit developed about a decade after the transistor also had profound effects on computing It was developed by Jack Kilby in 1958 - though many others were involved along the way and there are disputes as to who should get the credit for inventing it first - at Texas Instruments He used semiconductors to create an entire computer chip - the integrated circuit An integrated circuit can contain incredible amounts of transistors and it is this complexity and compactness that make it so useful Manufacturing was also much easier and quicker Integrated circuits started a second computing revolution which laid the groundwork for cheaper computers that could be available to the masses Edit Now that the question is focused on the transition between vacuum tubes and transistors I'd like to add something about semiconductors because they play a key role in transistors Semiconductors allow for good conduction of electricity but one of their really useful properties is that their conduction can be modified in a process called <a href= https //en wi"
173,inzinjerstvo,"The device for taking horizontal and vertical angles that you mention is called a theodolite Theodolites only started being phased out as the main surveying tool in the 1980s when total stations where introduced Below is a Soviet theodolite from 1958 (ex Wikipedia) Theodolites were analogue devices and the angles measured had to be written in a notebook Total stations were electronic devices essentially electronic theodolites with electronic distance measuring devices based on infrared signals These devices could be connected to a portable electronic memory unit with a keypad to store the measurements The surveyor still had to manually enter a point identifier for each reading but didn't have to enter the measured angles When starting a survey a reference marker from the nation system of surveying markers closest to the surveying region was chosen as this had a known/established northing easting and elevation A picture of US survey marker follows (from Wikipedia) The theodolite would be set up and the first reading would be to the known marker peg to establish the baseline for the survey For very accurate surveys a surveying target on tripod was placed over the survey marker; either a plate with a cross on it or a short pointed rod with the point upwards A similar target would then be placed on a temporary marker and the horizontal angle between the two targets measured The vertical angle from theodolite's horizontal plane (in the eye piece) to the first target would be measured as would the vertical angle to the second target Each theodolite has specifically marker dot on it at eye piece (telescope) height This is the reference marker for the theodolite from which lateral distances are measured A measuring tape was place against the dot on the theodolite and the other end of the tape was place at the centre of each target cross or the tips of each pointed target rod to measure the slope distances The measuring tape had to have a certain tension applied and the readings would be recorded Later in the office the measured slope distances would be corrected for tape sag Additionally the heights of the theodolite and the two targets above the ground would be measured with a tape measure Having done all of that another temporary marker would be established the theodolite moved between the last two pegs and the process repeated For each set up the heights of the theodolite and targets was needed as were the slope distances vertical angles and horizontal angle Using trigonometry on all this data one could determine the co-ordinates and elevation of each peg Another method used of measuring was called stadia This used a theodolite but instead of a cross targets or pointed rod targets being used to sight to at each of the survey pegs surveying rods were used See the picture below from http //www tigersupplies com The surveying rod would be placed on each peg and three height reading were taken from the surveying rod the top cross hair the central (main) cross hairs and the bottom cross See the picture below The reading from the central cross hairs gives the height for the elevation The difference between the upper and lower cross hair readings multiplied by an optical constant for the optics of the theodolite gave the distance between the surveying rod and the theodolite Except for some Japanese the"
174,inzinjerstvo,"Generically speaking the first step would be to determine if your failures have any factors in common You could also examine your successes and see if they have anything in common These commonalities could be things such as the employee working on the product location where the tests were performed specific piece of equipment on which the tests were performed time of day etc Once you have identified some commonalities you can attempt to determine if any of them are the proximate cause of the failures and work backwards until you can determine the ultimate cause Of course the details of how to do this depend on the details of your situation"
177,inzinjerstvo,"Theoretically pontoon bridges with rope anchors keeping them to the bottom would work against wind and flow overcoming the problem jhabbot mentioned in his answer (same as train length limit - stretching force) In practice these come with more problems of their own They drift on water surface and as result rise and fall with water waves The larger the body of water they span the higher the waves; at certain point in stormy weather the bridge would just launch the vehicles into the air The anchoring isn't exactly simple if it's to withstand such forces You could just as well go with pillars these don't add much to complexity They are only a short way above the surface Waves could roll over them washing vehicles off Also they stay level to the local surface - a wave wouldn't need to roll over it - it could just flip a vehicle sideways by twisting the bridge Since segments need to be mobile relative to each over their joints will be uneven forcing a severe speed limit Unless they float freely they'd be very limited with water level If you anchor them firmly water rising (even due to a storm) may submerge them And yet again as the segments need to be at least partially mobile a longitudal force stretching one side of the bridge may lead to the other side to stack segments against each other We have construction technologies that are extremely durable against longitudal forces (stretching compressing) - reinforced concrete steel ropes etc But add lateral forces and the design becomes much harder to keep strong; buckling twisting and loss of stability become very severe With keeping the bridge suspended you keep lateral forces limited to wind If the bridge is partially submerged this goes out of the window The primary upside of pontoon bridges is the simplicity - they can be deployed in matter of hours and as such they play huge role in the military But since they are vulnerable against weather and due to the slew of problems they create - especially with increasing span = size of the body of water = mechanical influence of water conditions they make very poor permanent bridges and so firmly supported bridges are simply superior also where the ratio of vehicle traffic demand versus length of the span of water to cross is too low the right solution is a ferry Ferries can take many vehicles on board and cross the distance (and depths ) not viable for any bridge and of course their cost is a tiny fraction of cost of the bridge covering that distance Note pontoon bridges are okay as temporary solution (say in place of a bridge washed off by flood or for duration of construction of permanent bridge or in locations where building a permanent bridge would be overly expensive or difficult) but they are always considered a poor man's substitute - and while they are okay for crossing a moderately sized river the engineering problems scale up to insurmountable levels as the distance covered rises - they are really unsuitable for very long spans of water"
181,inzinjerstvo,"This is typically done with a PID (Proportional Integral Derivative) control algorithm There are heaps of literature about designing and optimizing PID controllers so there's not much sense going into a more specific detail here Typically you use the PID controller to regulate speed Assuming your stopping point is also critical there will be some trade-off between accuracy of position and accuracy of deceleration at the end point With relatively low inertia systems and reasonable deceleration values this is not usually a very big issue Wikipedia has a very in depth article on the design of PID controllers at https //en wikipedia org/wiki/PID_controller There are a number of sample and open source implementations available as well depending on what platform you are working on If you have already done this basic research and have a more specific question please clarify"
184,inzinjerstvo,"The ESA has a page on compressor blades They give a good dimensioned diagram of an approximate shape; here are some basic dimensions Length 300 mm Width 30 mm Height 70 mm Thickness 5 mm I can't find any complete open source designs (i e high-quality technical engineering drawings) but this is a good approximation"
189,inzinjerstvo,"Steel is defined as an alloy of iron and carbon; there is no such thing as a non-ferrous steel If you alloy some other metal with carbon it becomes something other than steel Looking for a steel without iron in it would be like looking for brass or bronze without copper You can alloy things other than copper with zinc tin or aluminum but those would not be kinds of brass or bronze As far as other alloys that contain carbon this Wikipedia article has a good list of various kinds of alloys (as you can see there are a lot of them) and searching through it you'll see that there aren't a lot of other things that are alloyed with carbon besides iron As for why this is I don't have a good answer"
192,inzinjerstvo,"The RM-1 Russian submarine reactor had a core of less than one cubic metre It had about 100kg fuel load which was 90% enriched (i e 90kg) Uranium 235 This was liquid-metal cooled [specifically a eutectic lead-bismuth alloy (44 5 wt% lead 55 5 wt% bismuth) - source as below p40] so didn't need a moderator Submarine 901 had in its right-board reactor just 30 6 kg of Uranium 235; this was at 20% enrichment so a total fuel load of 153 kg These were controllable chain-reaction based reactors Source NKS-138 Russian Nuclear Power Plants for Marine Applications Ole Reistad Norwegian Radiation Protection Authority Norway Povl L Ølgaard Risø National Laboratory Denmark Published by Nordic Nuclear Safety Research April 2006 ISBN 87-7893-200-9 http //www nks org/scripts/getdocument php file=111010111120029"
194,inzinjerstvo,"Your question sort of has two parts How to supply heat and how to keep it in Large open rooms with a high ceilings are most efficiently warmed with radiant ceiling heat Warm air rises which renders forced-air systems inefficient because the pumped heat ends up at the ceiling and the coldest part of the room is near the floor where you actually want the heat Radiant floor systems are limited to about 87F because they are in contact with occupants and so their peak output may not be enough to keep the space comfortable They also lose more heat to convection than radiant ceilings (See this ref ) As for keeping heat in besides solid insulation/barriers air doors (a k a air curtains) are the standard solution in high traffic passages"
201,inzinjerstvo,"My first thought was to use a small amount of chromium and then another metal (gold) This is how its done for electronics structures on glas The chromium does react with the silicium-oxid and hence has strong bonding I've done this only with layers of very small thickness (sub µm) and only via a PVD -technique But as you can read on this wikipedia page it can be done in this way for glas-metal seals For example with chromium and stainless steel As such I think it is rather cost effective at least if you compare it with pure indium and gallium If you application has temperature gradients be aware of the maybe very different temperature coefficients"
205,inzinjerstvo,"For tapping threads there are a few steps between free hand tapping and fully automatic machines Which one is right for you will depend on the material you're tapping the size of the threads you're tapping and how large your production run is The first step would be to buy a guided tap wrench which is a regular hand tap wrench with a bushing you can put on the top and chuck into a drill press (or mill) or lathe This helps keep your tap square to the workpiece and also lets you apply a little pressure with the quill If you want to get into power tapping (And need more precision than a cordless drill with the clutch turned down ) there are reversing tapping heads you can attach to a drill which reverses the tap out when threading is complete These are available for drill presses or hand-held drills They run around \$500-$1 000 here's another style of tap chuck called a tension compression chuck which allows a little vertical give but I believe that is only of much use if your spindle is computer controlled If you want a more robust solution and have some money there are tapping presses that start at around $3 000 It's not quite 6 figures but they aren't cheap Depending on what you are making self threading screws may also be a valid option There are a wide variety of types that work in different situations As for external threads after die cutting the only cheaper method I'm aware of is roll threading This requires a dedicated machine that is relatively expensive unless you're working on very small threads"
206,inzinjerstvo,"Here are some other criteria Cycle time Piston speed Piston friction Device lifespan (most likely a function of the number of cycles completed) Number of suction/discharge ports Type of valves used Hope that helps"
208,inzinjerstvo,"I have written mostly about CFD in this answer however same points should also work for FEA or other simulation techniques CFD is mostly used for design optimization and parametric study of the design Following are a few examples showing how engineers use simulations Selection of a design Read A conceptual study of airfoil performance enhancement using CFD This thesis shows use of CFD for selecting the best design out of a number of candidate designs Engineers often go for simulations to select 'the one' out of many Shape optimization using CFD This paper gives an example of wing shape optimization using CFD And this amazing YouTube video is an excellent example of the way an engineer would use a CFD software ( OpenFOAM ) and genetic algorithm CFD makes it possible to arrive at a better design without actually building a number of prototypes and testing (which is an expensive and long process) Actually design optimization is the most common way the CFD is used According to this survey mechanical design engineers make the use of CFD the most (note I do not know the authenticity of the report) Using simulations where experiments are difficult to carry out / might cost a lot of resources (or life) Applications where experiments are not possible to carry out such as the heat transfer in hypersonic re-entry vehicles ( examples here ) or blood flow in human body can be simulated with a computer and final design can be tested Another example; CFD is used for placement of probes on a wind tunnel model CFD gives for example the position of the stagnation point on a surface of the model and there we can have the pressure probe placed and then test the model in actual wind tunnel This presentation explains how CFD and wind tunnel are complimentary to each other Also CFD is used to predict the results where experimental results are not available (one can not have probes everywhere on the model) Design and optimization of the experimentation facility itself Simulations are commonly used for design of the facility itself For example this report describes how CFD is used for design of the wind tunnel To develop a theoretical model This is often seen in cosmology Scientists carry out simulations based on a model and validate with the experimental data This iterative process results in better understanding of the physics and working of the universe NASA astrophysics group have done some simulation of Supermassive Black Holes this video talks more about it In movies art and animations This question and following answers on Scicomp SE show how much a role CFD has to play in movies and animations (disclaimer I have asked the question) Some other applic"
220,inzinjerstvo,"It isn't completely infeasible Just to get a ROM (rough order of magnitude) let's assume that a typical household that isn't using electricity for heating uses about 1 kW on average and that you'd like to be able to store a half day's energy or 12 kWh which is roughly 45 MJ Commerical air compressors can easily achieve 15 Bar or so (over 200 PSI) The energy in a tank of compressed air is equal to the pressure times the volume so to store 45 MJ at 15 Bar would require $$\frac{45 \text{MJ}}{15 \text{Bar}} \cdot \frac{1 \text{Bar}}{10^5 \text{Pascals}} = 30 \text{m}^3$$ or about 8000 gallons Can you get or build a tank that size that can hold the pressure The real key is what kind of thermodynamic efficiency you can achieve while making the conversion from electricity to pressure and back again When you compress the air it will get hot and some of that heat will be lost to the environment However you can get some of that heat back if you run the expanding gas through a suitable heat exchanger (and get some free air conditioning in the process)"
221,inzinjerstvo,"According to the ASME Process Piping Code (B31 3) $$p = \frac{2 * t * S * E}{D - 2 * t * Y}$$ where $p$ = internal pressure $t$ = wall thickness $S$ = material's tensile strength $D$ = outer diameter $Y$ = wall thickness coefficient (B31 3-1999 Table 304 1 1) $E$ = material and pipe construction quality factor (B31 3-1999 Table A-1A) Note that this equation does indeed have a safety factor included"
222,inzinjerstvo,"One of the problems that plagued older rechargeable batteries (e g Nickel Cadmium ($\text{NiCad}$) and Nickel Metal Hydride ($\text{NiMH}$)) was the memory effect The memory effect occurs when a rechargeable battery is not fully discharged It then forgets that it has a greater capacity than it thinks it has and so in the future it discharges less A good example is a water bottle Initially water bottles have a certain capacity for water Let's say that I drink most of the water in a water bottle during one usage If the memory effect affected water bottles I would not be able to drink any water in the future occupying the space that had held the water that had not been drunken the last time That extra space would be forever lost Over time this can wear down a rechargeable battery Fortunately this generally only affects $\text{NiCad}$ and $\text{NiMH}$ rechargeable batteries I haven't been able to find much about effects that influence only lithium ion batteries but there are a lot of across-the-board factors Here's a short list Chemicals breaking down Passivation (which affects lithium ion batteries) which is when a layer of unwanted chemicals form on the battery cell This discusses a related phenomenon on page 4258 Unfortunately on recharge the lithium has a strong tendency to form mossy deposits and dendrites in the usual liquid organic solvents (cf Figure 15B) This limits the cycle life to 100-150 cycles (considerably lower that the 300 cycles required for a commercial cell) as well as increasing the risk of a safety incident Mechanical stresses and leaking Batteries can be damaged in a variety of ways causing internal components to break and causing chemicals to leak out This can be very dangerous to humans There are other long-term factors that increase battery aging The page I linked for the above list seems to be fond of the Arrhenius equation $$k=Ae^{-E_a/RT}$$ which shows that the rate of chemical reations changes as temperature changes High temperatures mean faster reactions but also possibly a shorter life; this can affect non-rechargeable batteries significantly Finally there's the phenomenon of self-discharge which is when unwanted reactions in the battery eat away so to speak at the battery's capacity The process can differ based on the type of battery Battery University has a page on it which you may have already seen It reiterates that temperature can speed up this process Scarily enough lithium ion batteries may discharge as much as 5% within the first 24 hours slowing down to 1-2% per month after that"
225,inzinjerstvo,"This actually isn't as much of an engineering question as it is a physiology question There are actually a number of widely used estimates to predict your one-rep maximum aka 1RM if you know how many repetitions you can do at a lower weight See here for more info All of the methods are based on empirical studies and are basically look-up tables of coefficients Brzycki Epley Lander Lombardi Mayhew et al Connor et al Wathen National Strength And Conditioning Association (NSCA) Coefficients"
228,inzinjerstvo,"A bimetallic strip is used Two different metals like steel and copper expand at different rate and a strip of the two bound together as result bends with temperature changes A contact placed at the end of the strip will close when the temperature is right - and the temperature can be tuned by turning a screw that pushes the strip closer or farther from the contact In simpler systems the natural inertia of the system is used to create the hysteresis - as the refrigerator compressor works it takes time for the temperature drop to reach the bimetallic strip and switch it off so the temperature is brought below the cut-off point by some hard to control factor In more complex systems a two-level switch (either through two strips or just mechanism that shorts two switches at two different temperature levels) in connection with an analog RS switch (usually based on a relay) creates a better-controllable hysteresis As temperature is too high both switches are engaged and so the cooling compressor starts Temperature drops At cut-in temperature the On switch disengages but power to the relay is still supplied through the Off switch At cut-out temperature the Off switch disengages and the cooling process stops With temperature rising the Off switch engages but since the relay is open it doesn't supply power to the coil Further rise causes the On switch to power up the coil and the relay switches both the compressor and the Off switch circuit on With temperature drop the on switch will disconnect but the coil powered through the relay will remember the state until Off opens (and I'm sorry but I don't know about calibrating thermostats with jars of water My vote would be against as they are rarely submersible )"
233,inzinjerstvo,"I know would describe those kind of programs as 3D parametric feature-based modeling (just to clarify and distinguish between these and dumb geometry solid modeling or 2D drafting) There are a number of free tools for solid modeling including some parametric feature-based ones FreeCAD is a strong contender really looking to provide functionality to commercial software http //www freecadweb org/wiki/index php title=Feature_list SOLVESPACE might be an option as well http //solvespace com/index pl Other solid modeling (non-parametric) Blender OpenSCAD Sketchup(not open but has free option)"
235,inzinjerstvo,"Generally speaking meeting the EMC/EMI requirements is the EE job The biggest part of radiated EM waves usually comes from a poor PCB and of course there is little the ME can do about that Providing more space if possible can relax some constraints for the EE that will have more space to properly route his/hers tracks In your case it appears to me that no EM problem should be present the fastest signal lying around would pass through the USB connection but you say it's used only for power Your device is also battery power and that gives a huge help since there's no risk to inject some unwanted frequencies in the mains through the power supply If there's some high speed clock inside and I guess there is proper routing should be enough A great help would come from a metallic enclosure as you probably know a grounded metallic enclouser greatly helps to keep unwanted EM from escaping your device Bonus points it also keeps unwanted EM from messing with your circuit that might be a bigger issue To help with the routing relaxing the position of the usb connector and/or the lcd can help but really in this case it seems quite trivial to me to make a circuit that has no problems Without additional informations on the specific device in question I'd say just relax make a prototype and test it you're probably getting away with it without any problem"
239,inzinjerstvo,"It was stated previously that a perfect sphere cannot exist in terms of engineering or manufacturing but ignoring trivialities let's answer the question A Prince Rupert's drop is such that molten glass is viscous enough to droop off your rod and into a bucket of water causing the glass to cool rapidly enough to create high amounts of internal tension which causes the famed effect of making an unbreakable teardrop Even if you were to spin the rod quickly so as to not have a long tail some thin dragging would still exist and make a tail It may be small but it would still be there If you were interested in making it more spherical you might think to shave off the tail end but as you know a single nick or disturbance to the tail end results in a solid glass explosion Let's say you spun the rod in a way (in a magical world) so that there was no tail Then you wouldn't have a Prince Rupert's drop The answer to your question is no it is not possible to make a spherical Prince Rupert's drop because either the glass would explode or you simple don't have the drop you were looking for"
242,inzinjerstvo,"There are cases where DC power generation is superior to AC If it's going to be transmitted by DC and you can generate at the transmission voltage or you can step up the voltage to transmission levels without too much loss DC is superior If your load is DC and is close to your generator (in space and in voltage) DC is superior Both of these assume you've got suitable DC breakers available electrical systems are usual designed to be fault tolerant to the degree that no single fault can be dangerous to humans This typically means ensuring the system fails to safe and sometimes that requires breaking the current With the development of DC breakers even for HVDC applications this has improved considerably previously it was one area where AC had the advantage in an AC circuit current hits zero twice every cycle making safe circuit-breaking much easier And sometimes you just don't have a choice and the technology (e g photovoltaics) will only generate DC and if you want AC you're going to have to convert it"
247,inzinjerstvo,"I don't know about any ready-made multi-position bi-stable solenoids but let me help with What are the alternatives Depending on what force you need to exert and distance to cover a monostable solenoid driven by PWM might suffice but that seriously limits the range (as the magnet power rises with square of distance after certain threshold the shaft will drop into the solenoid) Alternatively you can stack two 3-position ones; needle of one moving the frame of another For more than two you'll be better off using some servo/stepper actuators instead And last but not least get a 2-position bi-stable solenoid and some position sensors - either a linear optical encoder or just some open-frame optocouplers along the way at the stop points Power the coils with PWM adapting to current position of the shaft modifying the pulse width ratio so that given optocoupler gives out half the normal level (covered halfway) stopping the shaft wherever you want"
248,inzinjerstvo,"The barometric height formula is defined as $$ p(z) = p(0)\exp(-Mgz/(RT))$$ For a centrifuge the upper expression is transformed to $$ p(r) = p(0)\exp(M\omega^2r^2/(2RT))$$ As you see the $g$ is transformed to the radial acceleration $a = \omega^2 r$ The factor $2$ stems from the integration which you have to do during the derivation of the barometric height formula (see barometric height formula derivation) And of course the minus sign disappeared because the force points outward Both formulas hold also for a mixture of gases I cite Kemp R Scott Gas Centrifuge Theory and Development A Review of US Programs Science and Global Security 17 (2009) 1-19 When the rotor contains a mixture of gases the distribution holds independently for each species So as a first approximation it is as I suggested in the comment section The cited source gives the following equations for a two-isotope gas $$ p_A(r) = p_A(0)\exp(M_A\omega^2r^2/(2RT))$$ $$ p_B(r) = p_B(0)\exp(M_B\omega^2r^2/(2RT))$$ A separation factor can be calculated $$ \alpha_0 = \frac{p_A(0)}{p_B(0)}/\frac{p_A(r)}{p_B(r)} =\exp((M_B- M_A)\omega^2r^2/(2RT))$$ Keep in mind that all formulas here don't include convection of any kind In earth's atmosphere we have too much convection and temperature differences for those formulas to work Furthermore in centrifuges to improve their performance there is a current-flow introduced (see the cited source )"
252,inzinjerstvo,"This will at least depend on the Rate of Cooling Magnetic field strength Exact composition The magnetic field will alter the microstructure as you can read in for example Yudong Zhang Nathalie Gey Changshu He Xiang Zhao Liang Zuo Claude Esling High temperature tempering behaviors in a structural steel under high magnetic field Acta Materialia Volume 52 Issue 12 12 July 2004 Pages 3467-3474 ISSN 1359-6454 http //dx doi org/10 1016/j actamat 2004 03 044 G M Ludtka R A Jaramillo R A Kisner D M Nicholson J B Wilgen G Mackiewicz-Ludtka P N Kalu In situ evidence of enhanced transformation kinetics in a medium carbon steel due to a high magnetic field Scripta Materialia Volume 51 Issue 2 July 2004 Pages 171-174 ISSN 1359-6462 http //dx doi org/10 1016/j scriptamat 2004 03 029 ( http //www sciencedirect com/science/article/pii/S1359646204001770 ) For me it is behind a paywall But as you can read in the abstract 30 Tesla will result in for example more ferrite The other paper reveals that for a hyper eutectoid steel you will have particle like cementite I am not aware of any models with which you can make predictions about tensile strength and so on But for the question Can we change steel properties by application of magnetic field while quenching it is a clear yes A more complete lookup in the literature would be the next step for models and experiments for a more specific case"
276,inzinjerstvo,"Water meets the low compressibility requirement but there are many other considerations in the design of a hydraulic system Boiling point/vapor pressure If the system warms up during operation the fluid may boil which results in high compressibility and thus decreased effectiveness of the hydraulic system Hydraulic fluid has a higher boiling point than water to help combat this Related to this is the concept of vapor pressure Hydraulic systems often involve small orifices which can cause cavitation (localized boiling) This cavitation has the same effects as boiling and can cause pitting damage to the components near the cavitated region Hydraulic fluid has a lower vapor pressure which helps here Freezing point It would not be a good thing if your car's brake lines froze every time it got cold outside Most hydraulic fluids have much lower freezing points to prevent this from happening under normal circumstances Oxidation/corrosion Water being an electrolyte will cause rust inside the lines as soon as air inevitably leaks into the system or the system isn't bled properly Water will also exacerbate galvanic corrosion when dissimilar metals are used in the system Lubrication Hydraulic components use seals and often involve sliding interfaces (cylinders and spools for examples) Using an oil as the fluid means the working fluid can also function as a lubricant Organic growth If perfectly distilled water and a closed system could be guaranteed this would be a non-issue But in practice this is never the case Oil-based hydraulic fluids are much less conducive to organic growth than water Water is used in some systems where other considerations trump these (for example some food-grade applications) but for a wide variety of applications oil-based hydraulic fluids are the better choice because of the design considerations above"
278,inzinjerstvo,"Controlling the pressure with variously sized holes will be a fairly difficult task It may be simpler to run hose with no perforations along the length of the planter and then place the hoses with perforations as branches off of your main line That still won't lead to a perfect distribution but would help Another common approach to this is to use so called 'drip emitters' which are very cheap regulated orifices They are designed to put out a certain volume per time relatively independent of the pressure I don't know if they can be buried"
282,inzinjerstvo,"Short Answer YES you can Long answer A) Limits of continuum mechanics The continuum model of fluid dynamics is valid only till the fluid behaves as a continuous medium This is characterized by the Knudsen number The Knudsen number is given by $Kn = \frac{\lambda}{l_s}$ where $\lambda$ is the mean free path and $l_s$ is the characteristic dimension of the channel (diameter in the case of the circular pipe) Non equilibrium effects start to happen if $Kn &gt; 10^{-3}$ Modified slip boundary conditions can be used for $10^{-3} &lt; Kn &lt; 10^{-1}$ and condinuum model completely breaks if $Kn &gt; 1$ ( Fun fact because the distance between two vehicles on a crowded road is much smaller than straight portion of the road itself (length scale in $1d$ flow) we can model the traffic flow with a PDE However it will not work if there is only one car on a long stretch of road) Coming back to water as the water molecules are not freely moving and are loosely bound we consider the lattice spacing $\delta$ for computing $Kn$ For water $\delta$ is about $3 nm$ So continuum theory will hold good for a tube of diameter $300 nm$ or larger $^*$ Now this is a good news $^*$ Reference Liquid flows in microchannels B) Applicability of Hagen Poiseuille equation Since your tube is in sub-millimeters range it is much larger than the minimum diameter required (sub-micrometer)for the continuity equation However depending on the shape of cross section of the tube the results will differ ( Link to ref ) Liquid flows are much simpler to analyse since they are characterized by much smaller Reynold's number and velocities Also density essentially remains constant So there should not be a problem in considering the theory to be valid Now since the Hagen Poiseuille flow is derived from the Navier Stokes equations it follows the assumption of continuity If your flow is through a porous medium you might have to consider effects like electrokinetic effect There might be other complications in straightforward application of H-P equations to microfluidic flows but I am unable to comment since do not know much in this field C) Some examples In a report on microfluidics networking Biral has used the continuum theory for modeling and simulation (in OpenFOAM) of the microfluidic flows Fillips discusses more about the Knudsen number in his paper- Limits of continuum aerodynamics This report clearly mentions that HP equation is applicable even to microfluidic flows This document on PDMS Viscometer gives derivation of HP equation for microfluidic flows Finally here is a You"
286,inzinjerstvo,"Seriously D There are going to be two parts to the solution you are looking for A) Till the steak is in the cannon B) The steak leaves the cannon is into the air and cooking starts A) Internal ballistics You are dealing with compressible flow NEVER use simple Bernaulli's equation beyond Mach no 0 3 Make sure you are using correction terms till Mach number 0 7 and beyond that use equations of gas dynamics (refer Modern Compressible Flow by John Anderson) That said your case is same as an air rifle case Instead of pellet you are shooting steaks So if you know the muzzle velocity you can design your cannon as shown in this paper Now your question is how does one get $P_0$ mentioned in this paper right For that you will have to do reverse calculations B) Steak leaves the cannon Assuming that you want your steak medium (as rare is not recommended apparently ) figure out the internal and surface temperatures for cooking Also time required for cooking At these temperature your steak most probably will be flying at supersonic speeds Then there will be a bow shock in front of the steak You can safely approximate it as a normal shock and use normal shock relations to calculate total temperature ratio across the shock Now $T_{01}$ becomes the atmospheric temperature and $T_{02}$ becomes the surface temperature on the steak (using total pressure ratio and gas dynamics relations) This will give you required shock strength and hence the flying Mach number Assuming STP conditions at sea level find acoustic velocity and hence the steak velocity Now this is average steak velocity But there is going to be wave and pressure drag on the steak all the time Use this Stanford supersonic wing drag calculator to calculate this drag In this take aspect ratio (AR) = 1 $C_L = 0$ put length of steak and its thickness / length as t/c So compute the muzzle velocity by using newton's second and then first law Now substitute this muzzle velocity in point A discussed above That will give you your chamber pressure Also I found one report in which internal ballistics of spring loaded gun is considered There is a matlab code as well You can take author's permission to use it Another issue is as you are going to use pre compressed pneumatic cylinder the temperature is going to drop considerably when expansion happens So flames is not a problem however during compressing of the gas in that cylinder things are going to heat up so using helium is smart move Another way you can do this exercise is to write a small code in your favorite language and carry out those iterations you mentioned However don't use Bernaulli's equation All the best for your paper Speculation If your"
288,inzinjerstvo,"Your intuition is correct those are high However you would need to be moving very slowly for wave-making resistance to be negligible And since it is typically higher than skin-friction I don't think that you can realistically expect to have a significant skin friction and a negligible wave-making resistance Perhaps a better simplified approach would be to ignore the skin friction and focus only on the wave-making resistance"
291,inzinjerstvo,"Distributed DC power is actually used in some new construction It's driven less by the efficiency of the transformer than by other logistics Here in California at least we have a law (known as Title 24) which requires some fairly sophisticated controls of lighting as well as fairly low power consumption per square foot The control requirements include compensating for daylight by automatically dimming lights occupancy sensing and brownout usage reductions This means that the controllers are fairly sophisticated and expensive and that LED light fixtures are sometimes preferred in new construction As a result some systems are being sold with the controllers that output dimmed DC which directly powers the LED fixtures DC distribution reduces the number of wires that have to go to each fixture (DC+ and DC- instead of AC hot neutral and a separate control line) and saves some money on electronics As far as I know though there is still at least one controller per room I'm not aware of any systems that distribute DC all throughout a building I imagine this is because as lengths and currents increase the advantages of high voltage in reducing wire size become more significant"
292,inzinjerstvo,"I can't give a definite answer since I hardly plan either type So please read this answer as an incomplete list of questions you could ask the sales engineers of your pumps Both can be operated hydraulically or pneumatically (for example if you don't want to bring a motor into an ex-zone or there would be cooling problems) In the case of piston pumps you'd need extra air/oil pistons for that The diaphragm can be operated directly by the fluid so the pump can be simpler So for a sump application you could have a diaphragm pump in your sump powered by a compressor nearby I've seen small and huge piston pumps but I've never seen huge diaphragm pumps (huge= flowrate >10m³/h) I assume some economy of scale here that makes large piston pumps cheaper than comparable diaphragm pumps (But I could be totally wrong) Diaphragm pumps have no seals that touch the medium This is an advantage since you probably need to service them less This also means that they are better for aggressive media or applications where cleanliness is important With any pump you need to know if there are any solids or abrasive stuff in your medium As for your question what specifications should be stated ask your vendor(s) What do you need to know to size and configure a pump"
294,inzinjerstvo,"From my brief involvement in shocks I think the most likely solution would be to image the exhaust probably optically but maybe using interferometry or something depending on what the exhaust is The most obvious indication that you have supersonic flow is if you can see a shock diamond I think you could probably also work it out from the length of the exhaust but I cannot remember how Alternatively you could also look at the thrust generated You should be able to calculate the expected thrust This is what they do when testing rockets/jet engines as they don't actually care if the flow is supersonic just that it generates enough power The simple way for pipes is to just measure the exit flow Its a pipe so flow should be constant However in practice I suspect long pipes also have regular inspection hatches/areas where they measure the flow somehow to check for leaks/faults"
301,inzinjerstvo,"You need to control how fast you apply the power Some sort of damper mechanism that would allow power to be gradually applied and not cause it to spin out You could try and wrap the 'power string' over different sized pulleys so that you changed the gear ratio If you used a cone it would be similar to a CVT where starting out you would have a low gear ratio to get moving then as the string unwound down the cone it would propel the car faster rather than just spinning out Edit You can either guess at the gear ratio or use some basic statics to figure out coefficient of friction and how to avoid slip while applying torque"
304,inzinjerstvo,"While some of these answers are close they are (at the time this answer is written) all incorrect to some degree Pressure and stress are very closely related -- in fact one could argue that pressure is in a sense a subset of stress To be specific the pressure in a material is the isotropic part of the total stress in a material Pressure is a scalar quantity -- the same in every direction while stress is a tensor quantity that captures all deforming forces Pressure and stress are related as follows if the components of the stress tensor are given by $\sigma_{ij}$ then the pressure is (using Einstein notation) $$p = -\frac{1}{3}\sigma_{ii}$$ That is to say the pressure is the opposite of the average of the diagonal elements of the stress tensor When speaking more specifically in terms of a boundary condition or an applied load for a structural analysis problem it refers specifically to an applied normal stress over a given area"
311,inzinjerstvo,"A pneumatic tire provides the mechanical decoupling of the tiniest variations in a road surface (the highest freqeuncies) involving a small unsprung mass (the rubber of the tread) and a spring (the air pressure) working against the sprung mass of the wheel and the axle The vehicle's suspension (springs shock absorbers etc ) working between the axle and the frame of the vehicle decouple the larger slower (lower frequency) variations in the road surface In this case the entire tire + wheel + axle assembly is considered the unsprung mass While there are other tire designs that have similar qualities they are generally heavier and much more complex to manufacture At least for now the pneumatic tire remains the most cost-effective way to get the desired overall performance (Not to mention the fact that the entire vehicle service infrastructure is currently set up to deal with pneumatic tires )"
314,inzinjerstvo,"If you have access to a lathe you should be able to quickly machine a flanged reducer out of Nylon 6 Not sure what temperature you're operating at but make it a bit thick and it will easily be able to handle 3 bar"
315,inzinjerstvo,"The blades are indeed made of or tipped with a hard steel/carbide but they also don't contact each other The shredding action is created by the shear forces of the two corotating drums of teeth and this non-contact between the hard points high shear strength and gearing advantages allow it to power through whatever you throw at it https //www youtube com/watch v=aVkTj9VrH4o"
317,inzinjerstvo,"Yes Michigan left This page (pdf here ) is very informative though you really have to dig to get what you want In a sub-section of 10 2 2 Median U-Turn Crossover I found this (A Michigan left is referred to as a median U-turn crossover ) A study on a Michigan corridor used simulation to compare median U-turn crossovers with two-way left-turn lanes (TWLTL) The study showed that during peak hours the corridor with median U-turn crossovers had a lower travel time by 17 percent and a 25 percent higher average speed than the same corridor with a TWLTL However vehicles made more stops on the arterial with median U-turn crossovers In nonpeak hours the median U-turn crossovers had the same efficiency as the TWLTL even though a higher delay for left-turning vehicles had been expected due to the higher travel distance a vehicle must cover to turn left using a median crossover So that's a yes for stopping congestion during peak hours More information on that specific simulation can be found under found under footnote 149 which isn't too easy to find Other simulations reportedly found similar results Simulation studies using a range of intersection configurations (number of through lanes on the major and minor street) and volumes from intersections in Virginia and North Carolina suggest a reduction in overall travel time for all movements through the intersection when compared to a conventional intersection -21 to -2 percent during off-peak conditions and -21 to +6 percent during peak conditions The studies also show a general increase in the overall percent of stops when compared to a conventional intersection -20 to +76 percent during off-peak conditions and -2 to +30 percent during peak conditions The rest of 10 2 2 has some more safety information The collision rate is lowered slightly There are less conflict points (i e locations where collisions are likely to happen) Jersey jughandle The Jersey jughandle (referred to as simply a Jughandle ) does reduce conflict points though not as much as a Michigan left It too appears to increase efficiency Simulation studies using a range of intersection configurations (number of through lanes on the major and minor street) and volumes from intersections in Virginia and North Carolina suggest a reduction in overall travel time through the intersection when compared to a conventional intersection -6 to +51 percent during off-peak conditions and +4 to +45 percent during peak conditions The studies also show a large increase in the overall percent of stops when compared to a conventional intersection +15 to +193 percent during off-peak conditions and +19 to +108 percent during peak conditions Is there a clear winner Both clearly reduce travel time and congestion so the answer to your question is a definite yes The Michigan left has many less conflict points (16) than the Jersey Jughandle (26) which I consider quite the advantage (the standard four-way intersection has 32) It also has a lesser increase in stops I'd give the edge here to the Michigan left though both are probably improvements over your standard four-way intersection"
319,inzinjerstvo,"In dry/arid areas a wind catcher tower in conjunction with a qanat is a great way to keep buildings cool The underground water stays cool and cools the air passing over it that is drawn in through the wind catcher However in a humid climate you would want to isolate the incoming warm/moist air from the cool underground water You could pass pipes through a large underground water tank and draw the air through those this may also condense some of the moisture out of the air and you'd need drainage outlets from the air-pipe that come outside of the water tank Obviously you'd use a much better heat exchange method than I drew in this rather crude picture but hopefully it explains the basic idea"
320,inzinjerstvo,"This gives you the starting point for a return on investment calculation It tells you the value in fixing each issue in terms of how many defects would be prevented The idea is that the curve shows the cumulative percentage of issues vs the total issues I've drawn a red cross-hair at where the curve crosses the 56 8 mark - 56 8 is 80% of the 71 issues This means if you fix the Important issues on the left of that point you will have solved 80% of the problems so they are more valuable The engineer should then estimate the effort/cost for each possible course of action for those important issues on the left Estimates just need to be relative to each other not in any absolute terms You can then divide the value by the cost to get the relative return on investment For example if I estimated to fix the radiator core issue is about 90 units of difficulty/cost to fix the fans is 20 and to fix the thermostat is 15 my ROI calculations would be as follows $ \text{Fix Radiator Core ROI} = \frac{31}{90} = 0 34444 $ $ \text{Fix Faulty Fans ROI} = \frac{20}{20} = 1 $ $ \text{Fix Faulty Thermostat ROI} = \frac{8}{15} = 0 53333 $ This gives a good idea of where to start in order to maximise the value added as quickly as possible (you are maximising the area under the curve if you plotted total value added against time as you work on fixing these issues) As you can see it's not as simple as starting with the first one because it will fix the most issues - this may take a long time due to the effort and in the meantime you could have fixed the simpler fans and thermostat issues The ROI gives you the order to work on these issues fixing the fans gives you the best return then the thermostat then the radiator core You've then fixed 80% of the problems and you can do the same again to work on the other 20% Of course there may be other factors that add to the 'value' component - such as negative press around one issue Likewise the effort estimate may have different weightings for time vs financial costs opportunity costs of using your resources on this as opposed to something else etc All of these factors should be considered in the 'value' and 'cost' components of this ROI calculation It depends how detailed you want to get Note that you might think you could just work out the ROI for all issues including the less important ones but in practice doing the estimates themselves may be quite a lot of work especially if there are lots of issues (this example only has 6) So the Pareto analysis allows you to very quickly work out the most important issues so that you can estimate those first"
321,inzinjerstvo,"I'm going to have to make this an answer If someone feels it's inappropriate I can delete it what follows is the start of my comment Grin I bought my house with an outdoor wood stove we have a love/ hate relationship I love It burns all my waste paper and meat products (Including the treats our cats bring us and the scraps the dogs drag home during deer season ) It heats my garage and preheats hot water (It could heat the house but more wood ) Gathering and processing of wood yes it's work but it's also an excuse to be in my woods which I enjoy I sit on my butt most of the day so wood = exercise That the ashes are all outside I hate When something breaks down it's always on the coldest day of the year (of course that'a not true but plumbing and outside windy below freezing is not fun ) It belches smoke because of poor burning If I leave my house during the winter (say Xmas) then I either have to drain it or have someone come over and feed it Of course some of that is open to engineering I'm leary of overly complicated and I burn a lot of wood on cold winter nights Here's a pic It's cold here tonight (7 F) and it's burning nicely"
330,inzinjerstvo,"The PWM solution is better under any aspect plus the chip you linked has a quite high full scale resistance $5\text{k}\Omega$ that will lead to use less steps than available PWM is better because its efficiency is near unity and because it does not require any external IC if the micro can handle the led current which is usually true When you use high power leds their drivers usually have a PWM input anyway and a resistor would be impractical anyway because of the amount of power it should dissipate Digital potentiometers are used to attenuate an analog signal such as a line level audio signal you're trading some power dissipation for linearity Such a signal is essentially a voltage signal little to no current should flow so power dissipation in the resistors is not really a concern When you want to power an LED instead you actually want power to flow into it that's why you say power an LED and a resistor dissipates power that's inevitable I'd just add a series fixed resistor to protect the led or to use the whole PWM scale if that's a concern A standard red led wants some 2V @5-10mA to work properly a 3V3 output is enough to burn it adding a series small resistor in the $200\Omega$ range allows you to set the pwm up to 100% and protects the led in case of an hw/sw fault"
333,inzinjerstvo,"This may not fully answer your question but hopefully it will be a good start I thought a distributed mass model would be a good approach for this so I did some searching and found this paper Real-Time Deformable Soft-Body Simulation using Distributed Mass-Spring Approximations (PDF) I also found this which goes beyond what you need including variable cross-sections and sheer stresses Bars under Torsional Loading A Generalized Beam Theory Approach I think this second one is what you need I included the first one because I can actually understand it whereas the second one is way beyond me If you can simplify out the bits you don't need by substituting in suitable constants it might be what you're looking for"
334,inzinjerstvo,"It's important to remember that these pipelines don't exist as a single isolated line and will have a number of branches that tie into the main pipeline and split off for gas to be sold to different locations This touches on a very broad subject of flow assurance and pipeline network modeling The pipeline will also be broken into smaller sections by compressor stations because as you mentioned friction losses will require the gas to be re-compressed as it travels along the pipeline To answer your question the gas flow rate will be regularly measured using a variety of flow meters such as an orifice meter ultrasonic meter Coriolis meter turbine meter etc This will occur at any point were gas flows into the main pipeline or is branched off the be sold It also may occur at regular intervals (i e the compressor stations) along the pipeline If you want to determine the mach number you could simply calculate it from your measured velocity I am not aware of any target mach number or velocity in pipelines it will certainly be subsonic the rest will depend on the specifics of the section of pipe For example the elevation profile the presence of liquids corrosion and the required pressure at the destination all will have an effect on the velocity you can transport the gas"
335,inzinjerstvo,"This angle is determined by the lift characteristics of the rotor and the rotational inertia of the whole helicopter You are exactly correct that the angle changes the phase of the feedback For the purpose of discussion let's assume the helicopter has pitched forwards slightly and needs to be corrected backwards also let's assume a single rotor that spins anti-clockwise when viewed from above If we adjust the rotor when it is exactly at the front to give some more lift the time it takes for the additional lift to overcome the rotational inertia of the whole helicopter would mean that the net effect of the lift was applied at some point to the front-left of the helicopter instead of exactly at the front The next correction would then be slightly further around and the next further round again This would cause the helicopter to oscillate in the same manner that a coin does when you knock it over on the table So you pretty much answered it yourself the angle controls the phase offset between the detected error and where the lift begins to be applied Remembering that the net effect of the lift will be as if applied between where the rotor is adjusted and where the rotor is reset (after some time the helicopter has moved to correct the error) Changing the offset angle of the flybar will change the angle at which the correcting force is applied relative to the error The effect of this will be an oscillation in one direction or the other Up to 90 degrees from where it should be the flybar will give negative feedback out of phase so should (at least mathematically) remain in a stable oscillation Beyond 90 degrees from where it should be the flybar will start to give positive feedback instead of negative feedback again out of phase until 180 degrees causing the thing to exponentially spin out of control"
340,inzinjerstvo,"I suspect you won't find a definitive answer on the fate of those two particular 747's for the following reasons The Stratolaunch project is likely still actively using the parts from those planes so there's no need to dispose of them yet 747 parts are becoming quite commonplace and their value in the parts market continues to decline So that's a bit of a killjoy answer for which I apologize Aeronautics is a wonderful field for generating compelling romantic ideals that appeal to our dreams The thought of a plane (or two) being used for historic progress and then just being parted out is frankly somewhat depressing But when we look at the project management behind an engineering project like the Stratolaunch the numbers become pretty telling This Bloomberg article details how the 747-400 is increasingly being sold off by various airlines Now ten-year-old passenger 747-400s are worth a record-low $36 million about 10 percent less than similarly aged planes last year according to London-based aviation consultancy Ascend as carriers seek more fuel-efficient models There’s even little interest in converting the passenger jets into ?air freighters because of a slump in air cargo demand Some 48 of the humpbacked passenger 747-400s worldwide have also been placed in storage according to Ascend The onetime “Queen of the Skies” has been shunned in favor of Boeing’s smaller 777 widebody (which has two fewer engines sucking fuel) or Airbus’s mammoth A380 double-decker “There’s not a lot of demand for the 747 ” says Paul Sheridan Ascend’s head of consultancy Asia “They’re mostly being broken up for parts ” And this Aviation Week article highlights how 747-400s are increasingly being sold off for parts as it's more cost-effective to sell them than to continue operating them with airfield space often at a premium airliners get withdrawn re-cycled and chopped up in a much smaller timeframe The airline can recoup more money selling the aircraft than operating it for several more years before it might otherwise have retired Likewise this NPR article piles on the bad news for the 747-400 Boeing has scaled back production to only one and a half new 747s per month Aviation consultant Ernest Arvai expects the company to keep the line running just long enough to replace Air Force One To draw that together We have a number of elements pointing us towards the fact that 747s and their associated parts are depreciating assets that are possibly depreciating faster than traditional valuations would have accounted for Depreciating assets receive proportionately less attention as their value continues to decline Stratolaunch is well funded and is focused on pushing the edges of space exploration technology While they may be concerned about how to wind down operations once they're done I tend to doubt it Nor are they likely worried about maximizing resale value of the 747-400 parts due to their current funding levels I suspect they are more focused on how to solve the problems they set out to tackle than they are on afterwards So we have parts whose value is approaching that of salvage only and a company focused on broader objects without mu"
341,inzinjerstvo,"I think the best (and simplest) way to describe something like this is Bernoulli's equation $$P+\rho gh+\frac12 \rho v^2=constant$$ To use this we're looking only at instantaneous velocity because as the air leaks the pressure will go down We also should assume that the valve is really more of a small hole than anything that fluctuates too much with pressure variations because that complicates it a bit more The constant in the Bernoulli equation gets applied to any point in a continuous stream So what we want to do is pick two points one on either side of the hole and relate those two points using the Bernoulli equation What we'll get will look something like this $$P_{tire}+\rho gh_0+\frac12 \rho v_0^2 =P_{atm} + \rho gh_1 +\frac12 \rho v_1^2$$ In this situation we'll say that any vertical movement of the air is small enough to neglect Also the velocity of the air inside the tire is negligible as well if not in practice than for the case of determining the relationship between pressure and velocity Lastly there's an important distinction between absolute pressure (which is in the equations above) and gauge pressure (which would be what we measure with a tire pressure gauge [go figure]) Gauge pressure is defined as $P_{gage}=P_{abs}-P_{atm}$ Bringing that all together we get the following $$P_{tire gauge}=\frac12 \rho v_1^2$$ The other couple of important points to make with this is that it's valid for inviscid flow (no friction) with constant velocity The first assumption is pretty valid if the pressure differentials are significant the second may not be especially since that starts to drive up the fluid velocity and constant density goes out the window when we get to compressible flows ($Ma&gt;0 3$) Again though for the simple case of examining the nature of the relationship this evaluation should be fine"
344,inzinjerstvo,"The turbulence model can make a big difference in your simulation There are many turbulence models around It becomes a tough job to select one out of them There is no perfect turbulence model It all depends on several parameters like Reynold's number whether the flow is separated pressure gradients boundary layer thikness and so on In this answer brief information about a few popular models is given along with pros and cons and potential applications However interested users can see this excellent NASA website and references therein to know more about turbulence modeling A) ONE EQUATION MODEL 1 Spalart-Allmaras This model solves for one additional variable for Spalart-Allmaras viscosity According to a NASA document there are many modifications in this model targeted for specific purposes Pros Less memory intensive Very robust fast convergence Cons Not suitable for separated flow free shear layers decaying turbulence complex internal flows Uses Computations in boundary layers entire flowfield if mild or no separation aerospace and automobile applications for initial computations before going to higher model compressible flow computations Applicability to your case a good candidate for reducing simulation time You can predict the drag fairly well with this model However if you are interested in knowing the flow separation region this model will not give highly accurate results ________________________________________________________________________________ B) TWO-EQUATIONS MODELS $k$-$\epsilon$ turbulence model A general purpose model This model solves for kinetic energy ($k$) and turbulent dissipation ($\epsilon$) The equations for this models can be found at this cfd-online page This model requires wall functions to be computed for the implementation Suitable only for fully turbulent flows Pros simple to implement fast convergence predicts the flows in many practical cases good for external aerodynamics Cons Not suitable for axi-symmetric jets vortex flows and strong separation Very low sensitivity for the adverse pressure gradients difficult to start (need initialization with Spalart-Allmaras) not suitable for near wall applications Uses Suitable for initial iterations good for external flows around complex geometries good for shear layers and free non wall bounded flows Applicability in your case Although this model is good for external bluff body computation it is suitable only for turbulent flows Since the velocities are low flow is going to experience transition from laminar to turbulent (max $Re = 1 98*10^6$ using this calculator ) You might benefit better with a variant like realizable $k$-$\epsilon$ model 2 $k$-$\omega$ turbulence model Solves for $k$ and turbulence frequency $\omega$ Gives better results for near wall flows Predicts transition (although early sometimes) Quite sensitive to the initial gue"
347,inzinjerstvo,"First here's a great answer to a similar question on the physics SE I will attempt to summarize it a bit and tune it to your question but I think that all of your answers are already in that thread if you don't wish to read it from me I don't think that the mesh is tuned to particular frequencies for EM waves It's just cheaper than building walls out of thick sheet metal The thickness of the resulting mesh must be thick enough to attenuate the frequency (see Skin Effect) and the holes in the material must be smaller than the wavelength of signal Overlapping layers of mesh provides the right thickness and the randomness of one layer of mesh on top of another should eliminate the holes for most applicable wavelengths making it a high-pass filter"
349,inzinjerstvo,"Here's a really (and I mean really ) quick and dirty set of calculations that might give you an idea of the magnitudes of settlement you could be dealing with The settlement potential of the tank location can be determined a number of ways but probably the best thing to do would be a plate load bearing test The test can be run to simulate the range (though not the duration) of loads you are expecting A test like this will give you a spring constant $k$ that represents the modulus of subgrade reaction of the bearing soil (for the tested loading range) However it's a short term test that doesn't take into account creep so the long-term $k$ value will be lower In general a short-term $k$ will run from something like 80pci for a very soft clay to something like 250pci for a very dense sand (caveat this is just from the top of my head without looking anything up) So let's use the worst case scenario here and to take into account creep let's do what geotech engineers do best and slap a 2 5 safety factor on it So we have about a 30pci modulus of subgrade reaction Let's also assume that most of the differential settlement will occur as a result of the uneven loading of the empty tank and that the emptying/filling of the tank is going to have a negligible contribution to the differential settlement This isn't too terrible of an assumption since the difference in applied surface pressure (which determines differential settlement) is much greater in the empty state and it's also conservative because it will only be empty 10% of the time anyway So here we go (I'm American so we're doing everything other than what you gave for dimensions in imperial-scum units first and then converting - sorry ) $k = 30 \frac{lbf}{in^3}$ $\gamma_{concrete}=150\frac{lbf}{ft^3}$ $H_{concrete}=2m$ Applied pressure under half the tank $q_c=H_c\times\gamma_c=0 98ksf=5 3 \frac{tonf}{m^2}$ Settlement under loaded half of tank $S=\frac{q_c}{k} = 0 23in = 5 8mm$ If we assume the other side of the tank does not settle at all our differential settlement comes out to about 6mm Now this number assumes the loaded side of the tank is free to settle while the unloaded side remains static This is not the case Assuming the tank is nice and stiff some of the applied pressure on the loaded side will be transferred to the unloaded side (which will reduce the settlement of the loaded side) I don't know what the application is for this tank but the above is probably a pretty conservative analysis of the situation you described I would be surprised if differential settlement potential turns out to be a problem for you EDIT One thing to note is that the tank will wiggle when it is being filled/drained What I mean is the entire thing will settle more when it is filled but it will settle more in the unloaded side (thereby undoing some of the differential settlement in the empty condition) Then when drained the soil will rebound and the tank will return to the more-tilted empty condition when the unloaded side rebounds more than the loaded side (though it is likely neither side would rebound fully) Assuming the 6mm of settlement from above the deflection angle for the 24m diameter tank comes out to be $\arctan\frac{6mm}{24m}=0 014^{\circ}$ Pretty tiny"
355,inzinjerstvo,"This is related to the amount of current (Amps) the electrical parts of the fixture can take without overheating and causing damage $$\text{Watts} = \text{Amps} * \text{Volts}$$ So at 120v a 50W bulb will draw $\frac{50}{120}$ (0 416) Amps and a 75W bulb will draw $\frac{75}{120}$ (0 625) Amps All electrical conductors have some resistance this makes them warm up when you pass a current through them The higher the current (more Amps) the faster it will get hot and the hotter it will get There are many factors that determine how hot something can get before it will fail for example how fast it can dissipate heat what temperature the insulation can withstand The rating is there so that you don't put too high a current through the electrical components of the fixture which would cause it to get too hot and melt catch fire etc How hot something will get can be predicted mathematically but ultimately the rating value is determined by testing the fixture in a lab with light bulbs of different powers and measuring how hot it gets The rating is set such that it will always remain at a safe temperature within a safety factor"
358,inzinjerstvo,"Disclaimer - I'm not an expert in batteries but I do some related work with those particular types Lithium-ion batteries are a little different from regular lead acid batteries in that the Li-ion batteries can provide more power for short periods of time than what their nameplate rating would otherwise indicate The trade measurement term for this effect is called C-rate The downside of exceeding a 1C (normal nameplate discharge) rate is that it creates more heat within the battery wears out the battery faster and can only safely be done for short periods of time So the ability to exceed the regular 1C rate and the fact that batteries tend to be exposed to a lot of environmental wear from being banged about leads to problems within the internal connections of the battery This blogpost from The Economist does a good job explaining the thermal cascade that causes the thermal breakdown When the battery is charged lithium ions are driven from the electrolyte into a carbon anode When the battery is discharged they flow back creating a balancing flow of electrons in a circuit that powers the device The trouble comes about if there is a small fault or damage is caused to the extremely thin separators that keep the elements of the battery apart This can lead to an internal short-circuit and a subsequent build-up of heat This can trigger what is known as a “thermal runaway” in which the battery overheats and can burst into flame"
368,inzinjerstvo,"The experiment can be done during daylight but you need a very tall building overlooking the ocean &amp; having an uninterrupted view of the horizon This video show a group of US students doing the experiment You Tube - Measuring the Radius of the Earth To get an accurate measurement you will need to be able to view the horizon unimpeded have reasonably accurate way to measure vertical angle a theodolite would be ideal but not practical with younger kids know the height of the viewing platform The main reason why an unimpeded view of the horizon is required is for accuracy the longer the target distance the more of the Earth's curvature is being considered Also the higher the building the further the viewable horizon The other thing is to decide which point on the horizon to measure to consistently One of the problems will be weather and air clarity particularly heat shimmer haze absence of fog etc The guys in the video had an error of 78% with their measurement due to air clarity using a thick point marker and not having very accurate angle measuring equipment The third option you list could be done during a school camp by the sea but I don't know that is practical or even possible in your situation"
374,inzinjerstvo,"The first thing to remember is that the naming of eras such as the Stone Age or the Bronze Age is never done by those living during the period It was always done by others much later To a certain degree the reason why bronze was the first important alloy was luck For whatever reason design or mistake someone at some stage during antiquity mixed copper and tin in a furnace and bronze was produced Prior to the use of bronze copper was used Copper being a soft metal became blunt very quick when used in tools and needed to be sharpened at frequent intervals Additionally copper corrodes easy compared to bronze When bronze proved superior to copper copper was abandoned as the metal of choice for tools During the time of the Pharaohs in Egypt copper was for tool making The Bronze Age was from about 3300 to about 800 BCE The first bronze made was arsenic bronze When tin was discovered it replaced arsenic as the alloying metal Tin bronze was better than arsenic bronze because the alloying process was more easily controlled and the resulting alloy was stronger and easier to cast Bronze became important because It was a strong metal It was easy to cast It was easy to sharpen It maintained its sharp edge for a long time Weapons that maintained their sharp edges were very useful in battles Likewise for non military uses such as knives and chisels It is resistant to salt water corrosion making it useful for fittings in boats and ships It has a high resistant to corrosion and has fatigue endurance It does not oxidise beyond the surface It was useful as armour in ancient times It was fashioned into tiles for building construction It has a relatively low casting temperature When struck against a hard surface it does not generate sparks It's heavy usage in ornamental work cannot be ignored Statues and ornaments were important in ancient times and bronze was easy to produce and cast In short there was a need for metals and bronze was available Iron only started being used when the tin trade was disrupted Steel wasn't invented until much later and until it was iron was very soft it corroded easily and it was not as useful as bronze"
379,inzinjerstvo,"There is heat that can be recaptured but you won't get much of it away As one of the commenters has mentioned your absolute maximum is the Carnot efficiency $$\eta_{Carnot}=1-\frac{T_c}{T_h}$$ This is an idealized condition you'll never reach this efficiency But to find our limit let's figure it out anyway $T_c$ will just be room temperature it might be slightly warmer inside the tower but we'll give ourselves the benefit of the doubt and pick a nice round number at 20C (293K) $T_h$ will vary as the GPU works harder (this is one of the issues with this design in general; the power you get from the cooling system won't be consistent because GPU temperature varies depending on how much you're stressing the chip ) We don't want to run it too hot and damage the card that defeats the purpose of a cooling system After some quick searching (Google GPU operating temperatures you'll see a bunch of forum posts that give a lot of different numbers none of which I think are strong enough to cite but I'm pooling their data to make my own assumption) it looks like most cards have a strong upper limit of ~100C before you start to do serious damage However running that hot will still reduce the life of your card and judging by the picture in the question this is a nice card for which we've paid a pretty penny and we want to keep it around as long as we can 70C is a good place to shoot for but 80C (353K) is still probably pretty safe and we want our best possible case With those numbers we get $$\eta_{Carnot}=1-\frac{293K}{353K}=0 17$$ This means that at the very maximum the best we can do is to get 17% of the heat we're generating on the card back as electricity to power something in the tower We can vary the card temperature and as it goes between 60C and 100C efficiency goes between 12% and 21% Regardless we're not getting a lot back That's the max efficiency though This site which sells thermoelectric generators says that the top of the line TEGs will run at 8% efficiency While this is better than the nothing that we'd have been getting before the real issue here is cost and implementation TEGs are not cheap and cooling fans are A basic cooling system is also much easier to install Even if we can hook up a TEG to cool the card we have to find something we can do with that electricity and we don't want the variable power to be used for an critical components Tower lights and extra fans are probably the extent of our usage So to answer your actual question in there I'm sure we can find all kinds of creative ways to get that heat converted into electrical or mechanical work Making it useful is an entirely different story"
381,inzinjerstvo,"I'm going to answer my own question because just before posting it I found an answer The Otto Lilienthal Museum has a comprehensive list of Lilienthal's designs One is listed as the small wing-flapping machine It didn't use a propeller (or jet engine of course ) but instead used a small engine weighing about 22 pounds when fully fueled Its wingspan was 22 feet which was quite impressive Wikipedia lists it as Small Ornithopter Another page calls it the kleiner Schlagflügelapparat which Google Translate translates to small flapping wing apparatus Here's a picture of Lilienthal with it pre-flight"
385,inzinjerstvo,"Some reasons why noise reduction in vehicle cabins is a standard feature yet As @Trevor Archibald states safety is a very good reason There is still a need to hear some noises from outside the vehicle such as the sirens of emergency vehicles police ambulance fire fighters truck Hearing car horns from other drivers is still needed The sound of the engine lets people know if the engine is performing as it should It's an added cost some people may not want to pay People haven't asked for it Most people don't object to hearing some noise as long as it's not intrusive Insulating vehicle cabins against noise by using sound proofing materials has suited most peoples needs until now It has been introduced in a small number of cars Auto Makers Shush Cars but these are a bit more expensive than the average car See also Cars Go Quiet Bose Noise Cancelling in Cars However introducing electronic noise reduction technology in cars could reduce the weight of cars by reducing the amount of sound proofing materials used Harman Quiets the Car On a different angle in the 1980s electronic noise cancelling had been used to cancel the engine noise made by heavy vehicles used in inner urban development site to reduce the amount of noise heard by nearby residents"
387,inzinjerstvo,"To begin with I am assuming each of your horizontal surfaces desktop and the three selves are each made the same material To use a ridiculous and exaggerated case the left half if the desktop is not heavy marble and the right side is not light weight balsa wood The desktop is composed of one uniform material and each self is made of its own uniform material wood glass metal particle board and laminex plywood whatever As shown each of the shelves and the desktop are independently attached to the vertical supports that acts as the legs Hence the weight of each horizontal surface is directly transferred to the vertical supports All horizontal surfaces are of uniform materials that have a uniform weight distribution Consequently each leg is carrying half of the combined weight of all the horizontal surfaces The section of each leg that experiences the full weight of what is above it is the short section between the two triangular braces the one for the desktop and the other for the stand/foot of the desk The stress in each of these short sections of leg will be the weight carried by each leg divided the cross sectional area of the leg in the z-plane (breadth by width of the leg) The sloping part of the desktop brace will carry some of the weight of the desktop Whereas the short vertical part of the desktop brace carries all the weight of the vertical support above the desktop the three shelves and some of the weight of the desktop What the proportion of the desktop weight carried will be will depend on the perpendicular (normal) distance from the vertical support Likewise at the base of the leg the triangular brace will redistribute the load in the foot according to your triangular configuration This is just a general overview of how to think about things relating to your design As @Rick Teachey states you really need to do a course in statics get numbers for weights and cross-sectional dimensions of supports and plug it all into some formulae"
388,inzinjerstvo,"This answer owes a lot to @HDE 226868 who put me on the right track If we simplify the whole bridge into 2D thin beam with a constant section size no internal damping and subject only to small vertical deflections then the natural frequency is determined by simple harmonic motion $$ n_0 = \frac{1}{2 \pi} \sqrt{ \frac{ k } { m } } $$ Where $ n_0 $ is the natural frequency $ k $ is the ratio between restorative force and deflection (the equivalent 'spring stiffness') and $m$ is the mass per unit length of the beam In a beam the restorative force is the internal shear caused by the deflected shape As the force exhibited by a beam is proportional to the rate of change of shear which is related to the stiffness ($EI$) and the rate of change of moment it can be shown (note the deflection is proportional to the length of the beam) that $$ k = \alpha \frac{ EI } { L^4 } $$ Where $E$ is the Young's Modulus of the beam material $I$ is the Second Moment of Inertia of the beam section $L$ is the length of the beam and $\alpha$ is a constant determined by the support conditions and mode number of the response All of the literature I have seen expresses this in a way that more convenient for the frequency equation $$ k = \left( \frac{ K }{L^2} \right)^2 (EI) $$ Substituting back in $$ n_0 = \frac{ K }{ 2 \pi L^2 } \sqrt{ \frac{ EI } {m} } $$ Calculating the value of $K$ is quite involved and there is an exact approach for simple solutions and approximate methods including the free energy method and Raleigh Ritz A few deviations for a simply supported beam can be found here It should be noted that this equation would have been enough but as it requires a table for $K$ and the calculation of a value of $EI$ that represents the bridge as a homogenous beam the authors of the Eurocode seem to have decided it would be better re-integrate the assumption that $k$ is constant along the beam To do this they have used the following relationship $$ \delta_0 = C \frac { w L^4 } { EI } $$ Where $\delta_0$ is the maximum deflection $C$ is a constant dictated by the support conditions $w$ is a constant uniformly distributed load across the length of the beam Under self-weight $w = gm$ where $g$ is acceleration due to gravity (9810 mm/s 2 ; as deflection in this equation is given in mm ) Therefore (re-arranged ) $$ \sqrt { \frac { EI } { m } } = L^2 \sqrt { 9810 } \frac { \sqrt { C } } { \sqrt { \delta_0 } } $$ And so $$ n_0 = \frac { 15 764 K \sqrt { C } } { \sqrt { \delta_0 } } $$ General values for $K$ and $C$ can be found in structural tables- for example here and here respectively For a simply supported beam $$ K = \pi ^ 2 \text{ and } C = \frac { 5 } { 384 } $$ $$ 15 764 K \sqrt { C } = 17 75 $$ $$ n_0 = \frac{ 17 75 } { \sqrt { \delta } } $$"
391,inzinjerstvo,"TL;DR It depends but probably not You might argue that all 15 samples are within spec so the supplier should be approved Not so fast depending on the parameters of your full production run the 15 samples may or may not be statistically significant There are numerous calculators online to do these calculations; I used this one A good resource for the actual formulas is online at NIST's Engineering Statistics Handbook or in any undergraduate Statistics textbook From the spec we know that our confidence interval is 10% (two-sided) This means that any parts within 10% of nominal are acceptable Next we need to determine the confidence level This is usually 95% but sometimes 99% and represents how sure we can be of the result The final piece of information we can give if it's known is the population size In this scenario this is the total number of parts to be ordered from the vendor over the lifetime of the product/process Most calculators allow this to be left blank and if missing assume a large value because the effect of the population size decreases as it grows in relation to the sample size Assuming a 95% confidence level and our 10% confidence interval with population left blank we need a sample size of 96 parts in order to have a statistically significant result Increasing to a confidence level of 99% requires a sample size of 166 parts So for industry-standard confidence levels we cannot conclude that the vendor be approved based on the initial sample of 15 parts alone Wait a minute—the entire FAIR run was within spec Well yes but what's to say the next 15 won't be out of spec We don't know—that's why we have statistics -) Well under what circumstances is our sample significant Just for illustration I entered population values until I found a statistically-significant result at $n=15$ for 95% confidence level $n=15$ samples would be significant only if the population (total production run) is 18 parts For a confidence level of 99% the situation is even worse significance only if the production run is 16 parts Other Notes The calculations above assume that the process follows a normal distribution and that the sample is representative of the population In practice both of these assumptions may be inaccurate"
396,inzinjerstvo,"As others stated before induction loops are the primary - most reliable method the coils (usually just several loops of wire) embedded in the road; fed given frequency from a generator in presence of metal the frequency of the LC circuit changes and the sensor circuitry detects the change of frequency producing a presence signal In some cases these may fail to detect bicycles but they are by far most common as they aren't affected by weather (or more precisely the detection circuit tunes in to slow changes of frequency caused by weather) and are immune to accidental false positives Note the loops can be localized (~2m size) or cover a lengthy part of a lane Detection is performed by cards like these and by induction loops made with wire laid in grooves like these or placed in pipes under the road surface at construction time (in the photo is a loop for tram detection but pre-built loops are similar) Videodetection - cameras connecting to a specialized card with detection zones defined through specialized software detect the vehicles They are vulnerable to bad weather and tend to produce false positives from glare of car headlights shadows of vehicles on neighbor lane and such but in certain cases - primarily where road surface makes installing detection loops impossible (gravel or bad road surface) they are preferred Additionally the video detection cards are significantly more expensive than cards for detection loops There are a few lesser used techniques like geomagnetic (detecting changes in magnetic field; These largely depend on size of the vehicle so a large truck can trigger a sensor in neighbor lane - but they are more durable) radar (detect only moving vehicles * - but are frequently used to detect pedestrians as they rarely stay immobile) laser (measuring distance to road surface; vehicle in the way changes the distance measured Quite reliable but only point-detection no area detection) Pictured below is a geomagnetic sensor and a radar sensors (short range for pedestrians and bicycles and long range for cars) I heard of pneumatic and piezzoelectric but I've never seen these in use for traffic control - probably problems of wear and durability; I know these are used for automated barriers for parking lots but they obviously support an order of magnitude lower traffic For city transport traffic the vehicles are equipped with an on-board computer with a short-range radio (up to 500m) and GPS and they broadcast messages about entering pre-defined checkpoints to the traffic system alongside with data about intended turn direction delay against schedule and some others allowing the controller to prioritize An alternative is a system that feeds vehicle position to a central unit which then contacts controllers with messages about prioritizing these vehicles Last but not least cameras/sensors detecting strobe lights of specific frequency give immediate priority to oncoming emergency vehicles (and take a photo of the vehicle in question to prevent abuse ) Controllers can communicate with each other and share t"
398,inzinjerstvo,"Chronic animal experiment This is not the only stage of evaluation of the material but it's a mandatory stage Chronic means that the animal is survived after implantation for a period of time and observed Afterwards the animal is sacrificed and autopsied Humans have a strong immune system compared to other animals So you would want to use an animal that also has a strong immune system Pigs become likely candidates 1 Dogs too Indeed since the immune response and repair functions in the body are so complicated it is not adequate to describe the biocompatibility of a single material in relation to a single cell type or tissue Sometimes one hears of biocompatibility testing that is a large battery of in vitro test that is used in accordance with ISO 10993 (or other similar standards) to determine if a certain material (or rather biomedical product) is biocompatible These tests do not determine the biocompatibility of a material but they constitute an important step towards the animal testing and finally clinical trials that will determine the biocompatibility of the material in a given application and thus medical devices such as implants or drug delivery devices [ exceprt from wiki ] 1 Pig's also weigh roughly the same as humans 2 Nick worked on several projects developing electronics for surgical devices As a part of my duties I assisted with a handful of animal experiments However those devices weren't of an implanted sort Biocompatibility was not one of my direct responsibilities; the mechanical design team was responsible for it An actual biocompatibility expert can provide this question deeper justice I'm writing this answer only as a fallback"
399,inzinjerstvo,"This is mostly an issue of safety trade-offs and not necessarily technical shortcomings While a WIG vehicle often referred to as a Ground Effect Vehicle (GEV) has improved efficiency it is also forced to fly very low Above around 50 feet (wingspan-dependent) you will not see much in the way of ground effect benefits The problem is that even over water there are numerous obstacles that could be in a WIG vehicle's flight path Commercial aircraft normally fly above 18 000 feet (5 500 meters) so that they stay in Class A airspace This allows them to remain far above obstacles and it also allows air traffic control to deconflict air traffic and prevent dangerous proximity There is no practical way to deconflict like this near the surface of the water where ubiquitous craft such as sailboats would pose deadly peril to a WIG vehicle Furthermore it is very difficult to maneuver at such a low altitude Aircraft can only safely pull upwards from a WIG vehicle flight altitude and that is the least-advantageous direction from an aircraft power perspective Also due to the low altitude an obstacle may not be visible until very shortly before a possible impact; even on a clear day with good visibility As for why hovercraft are more common they are able to (and typically do) move at relatively slow speeds and even stop on the water They can slow down stop and make sharp turns more easily and more safely than a WIG vehicle Due to their relatively slow speed they also typically have more time to react to an emergency situation Finally hovercraft technology is much less complex and much less expensive Reference Aeronautical engineering experience and pilot experience"
407,inzinjerstvo,"One is training to be an engineer and one is training to be a technician The best way to see the differences are to look at the course curriculum for each Purdue BS EE Plan of Study Purdue BS EET Plan of Study Purdue BS ME Plan of Study Purdue BS MET Plan of Study both are able to become PEs This depends on your local rules for becoming a PE Also note that being a PE isn't required for a lot of ME &amp; EE engineering jobs in the USA because they fall under industrial exemption Canada and other countries where Engineer is a protected title have different rules I was hoping to get answers from the point of view from an engineer who has hired people with these backgrounds I haven't directly hired both but I've interviewed both and personally the question comes down to what is needed To me it's like asking if you should go to a Dentist (DDS) or a Medical Doctor (MD) they both share the word doctor but they are different jobs with different training If I need a technician I wouldn't hire an engineer If I needed an engineer I wouldn't hire a technician Like all jobs and majors the longer you are out the less it matters as knowledge learned on the job overshadows schooling So 5-10 years down the road both could have the potential to end up in the same spot Anecdotally engineers make more than technicians but it still comes down to company and location Edit I want to point out that I'm using technician/technologist interchangeably here I've heard it both ways but I know some areas may be separate jobs The wiki on technologist is pretty good And don't pick your answer based on which one makes more It's irrelevant since I know a lot of people that have tried to force their way through one way or the other and ended up hating their education/jobs because of it I would explore both and see which one you like better"
415,inzinjerstvo,"Buy a depth finder As @SF pointed out you can use an ultrasonic device to measure the depth of the water These devices work by sending out an acoustic impulse and recording the return signals The acoustic impulse reflects off of any density boundaries in the medium By measuring the time for the signal to come back the distance from the transducer can be inferred by knowing the acoustic speed in the medium If the sediment is very dense then you will only get a signal from the sediment boundary but if the sediment is not so dense (like muck on the bottom of a lake) then you will get a signal from both the sediment boundary and the bottom of the tank You might think that this sounds like an expensive device to design and build but they are mass produced for boaters Almost every lake-going boat I've ever been on has one They range in price from \$50 to \$350 on Amazon depending on features and accuracy"
418,inzinjerstvo,"Standard threads are classified for accuracy by a tolerance class (You can see a bit about the metric thread fit classes at http //www amesweb info/Screws/IsoMetricScrewThread aspx ) The screws you find at your local hardware store will probably be a relatively rough tolerance class meaning that the threads are designed to have some gap between them and also that there will be a large variation in the dimensions of each individual bolt or nut Unfortunately for your question the requirements of the tolerance classes are about the diameter of the crests and roots (and therefore truncation) of each thread but not about the accuracy of the pitch (how many threads are in any given unit length ) So there isn't a great theoretical way to predict the backlash for a thread that wasn't really designed for use as a leadscrew I think your estimate of 1?m is pretty optimistic though For point of reference a well maintained manual milling machine might have as little as 001 (~25?m) and that's using a specifically designed leadscrew with a low-backlash design A less pristine machine could easily have as much as 010 (~ 25 mm) of backlash If your design is using a smaller diameter leadscrew there will be some improvement but the tolerances designed in standard threadforms will make it worse So your backlash would probably be on the order of magnitude of 1mm - 1mm for common leadscrew sizes (with 10mm being at the lower end of that range ) There are strategies for dealing with backlash including a 'split nut' where two separate nuts are coupled together and with either spring tension or fine adjustment forced into contact with opposite edges of the bolt thread The other strategy is to always approach the target from the same side If this is an option for you it is the simplest way to solve the problem of backlash without any materials cost"
419,inzinjerstvo,"It is called a steam wand gasket Technically it's a ball-and-socket joint made out of two concave PTFE (Teflon) gaskets and an EDPM gasket for the seal There may be other designs too; I learned this by watching Saeco StarBucks Barista - How to Replace Steam Wand Seals and Gasket (YouTube video)"
422,inzinjerstvo,"The testing of ventilation systems and of building air-tightness is done using smoke sticks or similar - non-toxic smoke generators that produce no ash residue One of these may be suitable for your needs Smoke generators typically use mixes of two or more of alcohol glycol glycerol water I suppose strictly speaking they're fog generators rather than smoke generators There are a wide variety of parameters to chose from There are fast-dispersing mixes and lingering ones There are generators that run off batteries and some that run off mains power Some generate high flows some low flows Some start generating within seconds of being switched on others take a few minutes before they start producing the fog (smoke) (Drager Flow Check - source as above)"
424,inzinjerstvo,"This is a constant acceleration problem on an inclined plane which ignores friction The force acting on the truck is the force of gravity The acceleration the truck will experience will be solely due to gravity $9 8\text{ m/s}^2$ Using the 38 degrees for the slope you have to find the component of gravity/acceleration going down the slope Given the truck will travel 100 m down the slope and the acceleration in the plane of the slope that you calculated you have to rearrange the following equation to get the time required to travel the distance $$s=ut+0 5at^2$$ The initial velocity $u$ will be zero if the truck starts rolling from a standing start To find the final velocity of the truck you use the following equation $$v^2=u^2+2as$$"
426,inzinjerstvo,"And so as promised I'll do it First assumption Let's work with 1m long strips of gutter It'll be easier to calculate everything starting from here Let's say the gutter is already full of ice (We'll work on the ice filling problem later on) The standard gutter size (according to this site ) is 5-inch K-Style or 6-inch half round If we use the half round version we can learn it holds around 9L of ice The latent heat of fusion of 9L of ice is 3000kJ or 833Wh This means that if you wanted to melt this ice in an hour you'll need 833 Watts of power For each meter of gutter But what's the available solar energy Second assumption Let's say the sun shines all day with the perfect orientation regarding the gutter and the gutter absorb all the energy it receives from the sun Let's assume it's the shortest day of the year at the US mean latitude (around 38°N) According to the third chart the length of day on the shortest day of the year is around 9h30m Let's round this up to 10h (I like round and easy numbers for my ballpark calculations) The cross section of our gutter is around 0 15m² The sun irradiance reaching the ground stands at around 1000W/m² This means around 150W reaches our gutter On a 10h day course that would be 1500Wh Hey that would be enough Well yes if you take your gutter and put in the perfect orientation regarding the sun Which it's not true In this case the energy received will be lower Moreover one also has to take into account the efficiency of the energy conversion High quality solar thermal collector (which collect solar radiation and convert it to heat) typically have efficiency at around 60% This means in our case (where the efficiency will be lower we'll receive at best 900Wh If we take into account the fact that our gutter has a fixed orientation the energy received will be even lower than that Thus we won't have enough energy to melt the ice Given this data I'd say it's not possible As for the ice filling the gutter The problem is still the same Making sure that the water flowing from the roof stays hot enough will still means providing enough energy to keep it above freezing Also usually water melts on the roof but doesn't flow alone i e it brings down some snow and ice in the gutter which you'll have to melt of you want to prevent ice buildup in the gutter"
432,inzinjerstvo,"You build the delay into your system [This first requires your system to be repeatable If your delay varies then this method won't work ] You would collect lab data and map all of your actuators and their respective delays with the hydraulic line length etc Say you have 4 hydraulic actuators and the results show the following delays Act1 30 ms Act2 50 ms Act3 200 ms Act4 1 ms If you wanted Act3 and Act4 to run at the 'same time' you would artificially add a 199ms delay to Act4 The timing of the events would look like this &lt;0ms Determine what you want to do Move Act3 and Act4 by a certain amount 0 ms Send the Act3 signal 199 ms Send the Act4 signal 200 ms Act3 and Act4 move at the same time It is similar to how moder compression ignition fuel injectors are calibrated Each actuator has a slightly different delay based on multiple environmental factors By mapping out the different delays you can repeatably inject fuel when required"
434,inzinjerstvo,"You are looking to build a sewer robot so you maybe want to look at other sewer robots I have no diret experience with designing or handling sewer robots but I can talk about the ones I saw at trade fairs Some builds have sets of 3 or 4 wheels with a mechanism that pushes the weels against the pipe walls or - more typically - sets of 2 wheels In the first case the wheels are quite small In the latter case the wheels are sometimes shaped so that when looking at the robot from the front the cross section is more or less circular Find a handful of pictures of the seconfd type here The first type with an X shape you can see here I think it's far larger than 4 Not that this is not a classical sewer robot but for some pipe building application that I don't understand One small robot I have seen had a small pivotable arm at the front with a light source and a camera At a branching in the pipes the robot could push the arm into one of the branches When the robot moved forward it would be slide into this branch because of the arm I've never seen tracked robots that would fit into a 4 pipe - again don't take my word for it Note Most hits for the term sewer robot where on german sites Maybe the actual english term is different and only germans insist on calling them sewer robots"
436,inzinjerstvo,"There are a few main reasons why suspension bridges aren't used for railroads The main reason is that suspension bridges are typically used where very long spans are needed Trains are very heavy especially when compared to lanes of highway traffic This means that long spans require very strong support structures which in the case of suspension bridges are cables and towers The second reason goes along with the first; trains cause high dynamic loads as they move along the rail This can increase the vertical loads by 30% Third is that trains don't really have suspensions especially freight trains This means that any movement in the bridge itself has little opportunity to be dampened before it reaches the train Suspension bridges are relatively flexible by design which makes transferred motion even more of a problem You do not want the bridge to be rolling under a train None of these are impossible engineering issues to overcome But by the time that you have accommodated all of them you might as well find a different location or build a truss bridge instead"
439,inzinjerstvo,"It's important to remember that this budget is part of a proposal; it is inherently not set in stone so it does not have to be perfect You do not have to identify every expense that you will incur during the course of the project but you need to show that you have considered identified and can estimate the major costs of the project If you don't have one already I'd highly recommend creating a timeline (possibly in the form of a Gantt chart for which you should be able to find multiple Excel templates or other programs to create one ) It sounds like you have a decently clear goal in mind (make sure you know what you mean by improved though that's mostly unrelated to this question ) But you also need to have a good idea of how you'll get to that end result Some questions I would consider asking yourself What stages will this project require (Research design building testing etc ) How long will it take me to complete each stage OR How long can I allocate to each stage (The second may be more relevant if you have a strict deadline and certain things that you know will take X amount of time [shipping times special orders for unique parts etc ]) What could prevent me from staying on schedule and how likely are these events (Out of stock parts design challenges that may cause problems) What will I need for each stage and will this incur any cost Are any of these expenditures particularly risky and how does this affect the budget (Components that are liable to break during assembly or testing travel costs that may be volatile) You don't have to have the answers to all of these questions but you should consider them Some of them are inherently unanswerable; some you will only be able to answer after years of experience However as you create this general outline for the project you should start to see where the budget naturally comes into existence Obviously you'll need to plan for building prototypes But are these off the shelf parts or are they things that will need to be made new and if so how do you plan on making them Is the test you're going to run potentially destructive necessitating multiple prototypes These are fairly obvious questions but consider some slightly less obvious ones especially if you're focused solely on the new product What kinds of things is this going to grab and will you need samples for the testing process This is an existing tool that you're improving will acquiring a current iteration help you to understand it better and will the company provide that or do you have to buy it Are there similar products in existence from other manufacturers that could help you out Basically you have to look at the project from as many angles as possible And my first point is important enough to re-iterate This is a proposal you are absolutely allowed to change it halfway through the project assuming you have a compelling reason Also note that this is how I approach a budget and how I've done it on projects I've managed or participated in in the past But this may not be exactly what this company wants Don't be afraid to ask your supervisor or whoever you're presenting the budget to what they're looking for You're a student they know that they don't expect you to know how to do everything You're there to learn so take advantage of that"
445,inzinjerstvo,"Please note I'm not a building designer by trade but I have had to investigate related questions for other reasons I'll let you in on a dirty secret about sanitation lines within buildings - The biggest concern is not about how fast things are flying it's about maintaining air pressure and providing adequate ventilation This guide presents a bit of a historical background to tall building design with respect to sanitation system design This presentation provides some more recent perspective and goes into some of the research that is refining current building standards The traditional viewpoint is that waste forms an annular flow and has a terminal velocity in the range of 3 - 5 m / s Current research casts some doubt on those assumptions The contention is that drainage tends not to be steady state And with multiple streams contributing to the outflow the actual velocity may vary But as mentioned the real concern is making sure the air flow balances out and that water traps don't blow People aren't as concerned about what happens to the waste on its way out so long as they don't have to smell the process Typically for tall buildings a secondary vent stack is used to provide adequate ventilating air to the fixtures in use Current building codes in the US and UK specify sufficiently wide drainage pipe such that sufficient balancing air flow will always be available The second presentation I linked goes into detail regarding different drain pipe sizes and considerations for very tall buildings such as skyscrapers The secondary provides sufficient air ventilation so that water traps at the lower levels don't get blown out by the pressure waves generated from drainage coming from the upper floors Another approach that's used is to separate the different building levels into different drainage zones This allows for smaller diameter pipe to be used while minimizing the risk that lower level water traps will get blown out from upper level drainage And yet another approach that's used is the use of Positive Air Pressure Attenuators (PAPA) such as the following Note this happens to be just one vendors product portfolio Other vendors for these devices exist One challenge with PAPAs is that their status with current building codes in tall buildings is unclear I personally have installed PAPAs in my residence and can attest to their abilities However building codes are understandably slow to update and the long term reliability of PAPAs need to be demonstrated Again from my own experience using PAPAs requires modifying the system design from traditional approaches and requires additional considerations"
450,inzinjerstvo,"Commercial air traffic is in pretty constant contact with air traffic control and GPS is not the primary method of tracking aircraft position This article written in the wake of the MH370 disappearance gives a pretty good rundown of how planes are tracked currently The essence of it is that most air traffic control systems still rely on radar whether primary (bouncing radio signals off of objects) or secondary (using a transponder to get more accurate data and computing position/velocity using successive pings ) However most modern aircraft has GPS installed on it it's more for pilot reference than anything else At this point we have 3 systems to track position one of which is entirely separate from the plane so the entire tower would have to go down in order for it to fail Also considering the ubiquity of cell phones if a plane's entire navigation system falls apart simultaneously there will still be dozens or hundreds of GPS capable devices on board In this sort of emergency the GPS only needs to function well enough to get you below the cloud ceiling and into a position to fly VFR (using sight instead of instruments) and land safely at a close airfield And really what this means is that the question is based on a flawed premise If the GPS on a plane cuts out the pilot probably goes Damnit that sucks Gotta get that fixed later and continues about his day as if nothing happened because he's relying on the transponder for position tracking"
451,inzinjerstvo,"In Ye olden days DC generators were brushed commutated devices They had a one or more stator windings and an armature winding Field wound DC generators as well as motors were commonly connected in one of three methods Series Shunt and Compound Without getting into details each had its own set of strengths and weaknesses But you only have to remember these two things the voltage of a DC motor is dependent on its input shaft speed Current is a function of torque More voltage means more RPM's and more amps means more newton-meters (or foot-pounds) So with all that you need a constant speed source to get a constant voltage And you need to ensure you have enough torque to satisfy the current demand of your load otherwise voltage drops off Old automobiles had commutated generators They couldn't regulate the voltage so they used a range of around 10-14 volts and used a relay that simply closed when the engines speed was within the voltage range If the voltage went too low or too high the relay opened Primitive by today's standards The Alternator in today's automobile uses a voltage regulation circuit that varies the armature current which changes the field strength based on the stators output voltage Lower speed means more current to the armature and less current at higher speeds So how different were DC generators from motors Not very different at all If anything they mostly differed in mechanical design as they were to be coupled to a prime mover (steam ICE electric etc ) Though in much larger dynamos they had adjustable commutator brushes to compensate for the shift in the commutation plane as a result of heavy load charastics A hand wheel would turn a worm gear which would advance or retard the commutation plane to bring the generator back into its normal operating parameters You dont need to worry about this as I am sure you motor isn't megawatt capable I am guessing your motor is a permanent magnet type motor Its nameplate RPM is what you need to spin the motor at to get the nameplate voltage This means if you have a 12V motor that spins at 6000 RPM you need 6000 RPM to get 12V If you don't have a constant speed source you have no way to regulate the voltage You would need a buck-boost switching regulator to get a constant voltage from your motor If you are using this for a renewable energy project like wind or hydro a charge controller is usually designed for a wide input voltage swing via a buck/boost regulator Solar panels are a close analogy to a permanent magnet DC generator no internal voltage regulation and a varying amount if input energy Sun might be shining bright one minute and a minute late be blocked by a cloud So the charge controller does its best to make a useful steady-voltage from its varying input From there use storage batteries to capture that power for later use and to act as a buffer for low input events And just for reference an AC motor can also generate power if you spin it faster than its nameplate RPM usually at synchronous speed But again no voltage regulation and a constant speed is needed More trouble than its worth Also of note jet planes use a very elaborate mechanical speed regulator to produce constant shaft speeds which ensures a constant 60 or 400Hz AC frequency as the throttle is varied"
458,inzinjerstvo,"Production rates of lab-on-chip fluidic devices can exceed the production rate of silicon ICs easily Some types of lab-on-chip devices can be fabricated via injection molding Of course there are subsequent operations assembly QC But those can be automated I'm aware of a device that's already being produced at a rate of 5 million units a year These ones They are not small enough to fit the chip category the disk is about 100mm diameter Nevertheless it follows the philosophy of lab-on-chip At the same time In terms of complexity present cutting edge lab-on-chip devices are 8 to 10 orders of magnitude simpler than present cutting edge ICs Let me put it this way if today's lab-on-chip were silicon ICs they would be 741 OpAmps from 1968"
463,inzinjerstvo,"I'd recommend searching for shaft couplings These are a great option for connecting two rotating shafts in a non-permanent fashion A set screw (as mentioned) can work in low load cases but is often best with a flat spot cut into the shaft so that you've got more than just friction between the screw end and shaft to constrain things The setscrew may also scratch or dent the shaft Other designs of coupling will actually tighten and clamp to the shaft in question They'll have a much better surface to keep static friction and may actually have more clamping force Certainly better for a smooth shaft A further advantage of a coupling is that there are a number of more sophisticated designs that will transmit rotation even if the shafts are not well aligned Options exist to allow small amounts of bending or translation"
467,inzinjerstvo,"You're looking for a Rotary Viscous Damper like these - a device that provides viscous friction in rotary movement Viscous friction is proportional to speed of movement See video of such a damper activated by a spring and a practical use - slowly falling lid"
469,inzinjerstvo,"That model of encoder is not intended for mechanical measurements It's meant to be used as a fancy knob with high friction for turning by hand say to pick a position from menu on a display and trying to couple it to a mechanism for readout is a misguided attempt What you need is an encoder based on slotted optocouplers or a reflective optocouplers Note you can print the code wheel on transparency to get the desired number of pulses per turn Put the wheel on the axle attach the optocoupler to the main plate - or even extend the tracks a little and use them as code strip If you have trouble purchasing these you can often find them in computer mice and cheapest inkjet printers"
471,inzinjerstvo,"Yes Especially considering gold and platinum prices as of today Pt costs less than Au - but let's earn much more with more modern solution and simultaneously slowly murder the king in a very nefarious plot Gold is 40 $\$/g$ at 19 3 $g/cm^3$ Platinum is 39 $\$/g$ at 21 45 $g/cm^3$ Depleted Uranium is about 6 $\$/g$ at 19 1 $g/cm^3$ (1) [sorry for the source but at least it cites some references so I believe the prices aren't that much off ] $\rho_{Au} = 19 3 g/cm^3 \\ \rho_{Pt} = 21 45 g/cm^3 \\ \rho_{DU} = 19 1 g/cm^3 $ Let's assume we want to produce $1cm^3$ of the new alloy of same density as gold Our set of equations will be $ V_{Pt} + V_{DU} = 1 \\ \rho_{Pt} V_{Pt} + \rho_{DU} V_{DU} = 1 \rho_{Au} $ so changing sides and substituting $ V_{DU} = 1-V_{Pt} \\ \rho_{Pt} V_{Pt} + \rho_{DU} (1-V_{Pt}) = \rho_{Au} $ $\rho_{Pt} V_{Pt} - \rho_{DU} V_{Pt} = \rho_{Au} - \rho_{DU}$ $ (\rho_{Pt} - \rho_{DU} ) V_{Pt} = \rho_{Au} - \rho_{DU}$ $ V_{Pt} = { { \rho_{Au} - \rho_{DU} } \over { \rho_{Pt} - \rho_{DU} } } $ $ V_{Pt} = 0 2/2 35 = 0 085 cm^3 ; M_{Pt} = \rho_{Pt} V_{Pt} = 1 823 g$ $ V_{DU} = 1 - V_{Pt} = 0 915 cm^3 ; M_{DU} = \rho_{DU} V_{DU} = 17 476 g$ Dividing these by density of gold we'll obtain masses that comprise 1 gram of the fake gold $ m_{DU} = 0 905g \\ m_{Pt} = 0 095g $ Now considering the prices 0 095g Pt is 3 70 USD; 0 905g DU is 5 45 USD for a total of 9 13USD for gram of the alloy replacing 40USD for gram of gold That's less than one fourth the price Of course since the appearance of platinum-DU alloy won't fool anyone you'd need to make some kind of skeleton to be plated with gold on the outside to hide the forgery and somewhat extend the king's life before he dies of brain cancer Using the values of metals in 287 BCE –?212 BCE you won't find depleted uranium on sale But I remember a historian friend telling me the ancients didn't recognize platinum as a precious metal - miners would dig it up sometimes along with silver and naming it young silver believing it needs to rest in soil for a few generations more would bury it to mature So I believe with platinum being considered nearly worthless addition of some lighter metal like copper to balance out the density gain replacing majority of the crown with platinum would bring immense profits If only the crown maker knew of the density of the young silver"
473,inzinjerstvo,"Looks like a typical water based central heating system (apart from the scale) You need to balance the flow by restricting flow for the units that get too much This can be done by partially closing a valve or adding a restrictor for those units Better if there is some kind of feedback loop to auto restrict when too much water is going through"
476,inzinjerstvo,"For structural applications (in the US) the most common bolt for weathering steel is ASTM A 325 Type 3 Type 1 is a plain steel bolt that can be galvanized but in this situation the zinc in the galvanizing will quickly be used trying to protect the rest of the structure Update for British bolts Interestingly the only option for UK seems to be to get these same bolts in metric (M24) size References Supplier Weathering steel bridges (page 13 of pdf) Specific discussion about the A325 bolts meeting EN standards"
479,inzinjerstvo,"On a mass manufactured part one solution is to cast the piece and shape the die (with an inside fillet) to form an outside fillet Of course if you want your casting to have an inside fillet you'll need an outside fillet on your die Assuming you're not casting and working with a commonly machinable material external corners would typically be rounded over using a tool called a 'radius cutter' or more formally a 'corner rounding end mill ' A particular cutter only cuts one radius so if you have two fillet radii you'd need a tool change They are available in a range of sizes and made out of different materials with different coatings High speed steel is probably the most common material"
482,inzinjerstvo,"You are correct getting oil pressure will require an electric motor as your source of rotation potentially a gear box to slow that rotation down and the pump itself You'll also need a reservoir to store the non-pressurized oil and probably a filter to keep the oil clean (dirty oil will damage your components ) If you're new to the world of hydraulics and want to keep things a little simpler for now you might want to search for a 'Hydraulic Power Unit' which usually includes all of these components already connected together This way you just have a pressure line out and a return line ('tank line') and don't have to be too concerned with RPM torque or any of the rest If you want to assemble the whole thing yourself or no HPU's are available that work for your application you'll want to find a motor with the same output as your pump's input (In your case a tapered shaft ) In your specific example if you see their datasheet at http //docs-europe electrocomponents com/webdocs/12c4/0900766b812c44d2 pdf you can see that they offer these pumps in a range of different shaft types most of which conform to a standard interface (SAE J744 for example) You'll want to pick a motor with a compatible output shaft You'll need to make sure that the RPM and Torque of the motor work with your pump and provide enough flow an pressure respectively for your hydraulic application Please remember that hydraulics can be very dangerous There can be significant amounts or stored energy that you can't see Think of the whole system as a giant line capacitor"
485,inzinjerstvo,"Meet Vladimir Shukhov a Russian architect who first developed hyperboloid structures He was born in 1853 died in 1939 and created over 200 hyperboloid structures in the intervening years He was the reason hyperboloids gained the popularity that they did His first design the first hyperbolic structure ever was the Shukhov Tower in Polibino pictured here Another tower also bears the name of Shukhov and it achieved great fame too Shukhov also built the Adziogol Lighthouse In total Shukhov designed and built 200 hyperboloid structures He died in 1939 which could one reason for the decline in the popularity of hyperboloids It isn't common for the death of one architect to essentially stop an entire movement (although Gaudí also worked with hyperboloids) but that may have been the case here Shukhov was the reason hyperboloids were popular in the first place; without him their popularity died down Also part of the reason that Shukhov was interested in hyperboloid structures was that hyperbolic geometry was being developed during the time when he first entered architecture A Google n-gram for hyperboloid shows that the term's usage during the late 19th century was its peak As a final note many hyperboloid structures were built after Shukhov's death so their decline wasn't that steep"
488,inzinjerstvo,"Even the best metal tape measure is susceptible to significant thermal expansion over large distances Try a laser measurement device ('electronic tape measure') instead http //www engineersupply com/Laser-Measurers aspx The laser distance measure flat plates clamped to the object and some shims of known thickness should be all you need to precisely locate something over a large distance"
490,inzinjerstvo,"According to the following two sources diamond is a ceramic Ceramic Strength-Density Graphs Ceramics @ Virginia University The graph below from Ceramic Strength-Density Graphs shows diamond is the strongest ceramic whereas zirconia is one of the most dense and strongest ceramics The site matweb has a database of all sorts or materials By specifying ceramics with a compressive yields strength ranging between 100 MPa &amp; 16 500 MPa &amp; a density range between 2 5 &amp; 22 6 $\text{g/cm}^3$ corundum aluminium oxide &amp; alumina have the highest compressive yield strength &amp; density with a compressive yield strength of 3000 MPa &amp; a density of 3 96 $\text{g/cm}^3$"
495,inzinjerstvo,"The short answer it's not optimal but may work Hopefully if these welds are critical they're being performed to some code (In the US for most structural work they'd be AWS D1 1 and D1 2 respectively ) Aluminum is considered among the hardest metals to weld well so quality control is especially important there Overall the wire feeder itself doesn't have too much impact on weld quality - a bunch of other factors including the power source welder (person) joint preparation consumable choice and cleanliness of the welding environment will make a bigger difference Steel is an easier metal to feed through a welding lead because it has a higher column strength and is less likely to buckle It's guided by a narrow tube ('liner') that runs through the cable and is typically just pushed by the wire feeder (which may be built into the power supply or a standalone unit ) Because aluminum has a lower column strength it is more likely to buckle and thus not feed well When this happens the wire will stop feeding out of the gun and just fill up the liner which then gets jammed and has to be cleaned out This is a pain for the fabricator but shouldn't affect weld quality other than maybe causing some extra starts and stops For a short welding lead and using large diameter aluminum wire they may well be fine using the same wire feeder If not there are a number of other strategies For small amounts of work they may use a spool gun which is essentially a miniature wire feeder which the welder carries around A small spool of aluminum wire is actually inside the gun along with the drive rollers which only have to push the aluminum a few inches as a column before it gets to the weld Another common strategy is a push-pull gun where the wire feeder pushes the wire with to help it come off the spool but additional drive rollers in the gun pull it to keep some tension on the wire as it goes through the liner It is possible that they are using the same wire feeder but changing from a conventional MIG gun to a push-pull gun when they switch from steel to aluminum That would be a common practice At the end of the day I wouldn't be worried about their choice of wire feeder For the most part it will work or it won't I would be more concerned with their overall quality control system Are welders working to a written procedure Have welders been tested before they're set loose to weld How and how often are finished welds inspected If they are welding to a code these answers will be dictated by it If they are not welding to a code you can get a rough idea of what level of QC they employ"
500,inzinjerstvo,"Believe it or not we could have done this 50 years ago if government funding hadn't been pulled from a project at the last minute Frustratingly after years of work by scientists engineers and technicians the Boeing X-20 Dyna-Soar project was cancelled just after work had started on the actual spacecraft Here's an artist's impression of the X-20 The X-20 was the result of a military program that aimed to develop an orbital spaceplane to be used for bombing and reconnaissance It was designed to be launched into orbit and to stay there for a short while Despite its small size - only 35 feet long - it would reach orbital speeds after launch in theory It managed to get to Mach 18 during practice glide tests The X-20 was not designed to be air launched but to be launched on top of a Titan III missile However a similar design - a precursor to the X-20 if you will - called Bomi was designed to be launched like this Here $^1$ is a comparison of Bomi (on the left) the X-20 (the two on the far right) and Robo a related project There were two versions of Bomi a suborbital one with a maximum speed of Mach 4 and an orbital one with a maximum speed of - well orbital velocity The latter is probably the one you're interested in It would have been 23 feet long and would have had a payload of 34 000 kilograms - enough for two nuclear bombs Both versions would have been launched on some sort of launcher - the larger vehicle which Bomi is shown attached to This design might also be changed depending on whether the flight was to be orbital or sub-orbital Bomi was eventually cancelled as funding was pulled for Dyna-Soar (the X-20) which then suffered the same fate But Dyna-Soar got past the glide-test stage (being dropped from a B-52) and almost actually made it to space Had the resources been moved to Bomi it could have succeeded Could Bomi have escaped Earth orbit With a bit of work it could have Think of how various rocket families have evolved Different types can fulfill different missions The Saturn V was the end result of smaller suborbital and orbital rockets If Bomi had been developed to the extent of the Apollo program I think it's very likely that it could have made it out of Earth orbit $^1$ This image appears to be in the public domain as stated here"
509,inzinjerstvo,"It depends &mdash; can you run air through the plenum without simultaneously running the active heating/cooling system If not you'll never get the benefit of its thermal storage capacity For example when heating the plenum will always be hotter than the slab putting heat into it In order to extract heat from the slab you need to be able to put building air that's cooler than the slab directly into the plenum Yes there will be some transfer of heat directly from the slab to the rooms through the floor but that would only occur near the plenum The same thing applies with the heat flow reversed when cooling There are secondary issues too The transient response of the system (i e the ability to change the temperature of the building interior rapidly) will be slowed down by the thermal mass of the slab if the plenum is not insulated This could be a problem in some situations Also when cooling there could be condensation issues in the plenum With active cooling excess moisture in the air is removed at the expansion coils and the system is designed to handle it However when doing passive cooling from the slab the condensation will occur inside the plenum which could cause maintenance and reliability issues"
510,inzinjerstvo,"Per the Wikipedia article on the Coriolis effect Contrary to popular misconception water rotation in home bathrooms under normal circumstances is not related to the Coriolis effect or to the rotation of the earth and no consistent difference in rotation direction between toilet drainage in the Northern and Southern Hemispheres can be observed Only if the water is so still that the effective rotation rate of the earth is faster than that of the water relative to its container and if externally applied torques (such as might be caused by flow over an uneven bottom surface) are small enough the Coriolis effect may determine the direction of the vortex You can be forgiven for believing this to be true though Again from the Wikipedia article The idea that toilets and bathtubs drain differently in the Northern and Southern Hemispheres has been popularized by several television programs and films including Escape Plan Wedding Crashers The Simpsons episode Bart vs Australia and The X-Files episode Die Hand Die Verletzt [30] Several science broadcasts and publications including at least one college-level physics textbook have also stated this [31][32]"
512,inzinjerstvo,"The choice of time step sets the bandwidth of the control loop The highest unity gain frequency (UGF) you can hope to achieve in the closed loop is the Nyquist frequency $$ f_N=\frac12 f_s=\frac{1}{2\ \Delta t} $$ where $\Delta t$ is the sample time Practically the UGF will be somewhat lower than this This means that above this frequency your feedback will not be suppressing the disturbance fluctuations in your system The UGF also limits how much gain you can have at frequencies below but near the UGF For frequencies within an order of magnitude of the UGF $\text{UGF}/10$ you won't be able to have a gain much higher than $\sim10$ A gain of $10$ in the closed loop means that disturbance fluctuations at those frequencies are suppressed by a factor of 10 So the choice of operating frequency is a practical one Faster systems are more expensive; slower systems may not provide enough disturbance suppression"
515,inzinjerstvo,"Hand-held sensors measuring multiple properties may require more than one type of sensor element; it's important to know what exactly you need to measure There is no one property (or fixed set of properties) that defines water quality though some (including the two you mention pH and TDS) are more commonly used than others In the case of pH and TDS either can be determined by passing a current through the sample and measuring the voltage of the circuit Rather than using a specialized sensor the unit may simply use a battery with some fixed electrodes (so the distance traveled through the sample is known); your RPi/Arduino/whatever device can handle the voltage measurement on its own This water quality monitoring text gives the following expectation for commercial devices Pocket-sized battery-powered portable meters that give readings with an accuracy of ± 0 05 pH units are suitable for field use This should be achievable with a DIY device though calibration may be tricky The source above also points to an electrode-based method as being acceptable for dissolved oxygen (DO) measurements though you haven't mentioned that as one of your target measurements The electrodes required may work for pH as well or you may require separate circuits; this probe measures pH and electrical conductivity (EC) with what looks like a single electrode It's used in this device which also measures TDS and I'm guessing it does so by using the EC measurement Your best option may be to find a pet store that specializes in aquarium supplies; they sell many different affordable sensor devices for home monitoring You should be able to find manufacturer contact information if none of those devices can be modified cannibalized or reverse-engineered to meet your needs You should expect that TDS in the sample may affect pH readings and vice versa since you'll be measuring them in nearly the same way (conductivity and voltage) Related Reading I'm not sure if you've already come across these in your research since you didn't elaborate in the question but readers may get some useful information from the following sources USGS Water Quality Methods &amp; Techniques - Field Methods &amp; Equipment California Rapid Assessment Method for wetland monitoring US EPA Test Methods - These are sometimes very specific and you may find one that identifies particular sensor types See also Clean Water Act Analytical Methods Total Dissolved Solids Measurement"
516,inzinjerstvo,"The strength of a horizontal beam is defined based on a property called the section modulus Shapes that have more material distributed near the top and the bottom have a higher section modulus This is why I-shaped beams or tall rectangles are common choices Round shapes on the other hand have most of their material concentrated around the center of the shape and very little at the top and the bottom so they don't make very strong beams That means a round beam would need a lot more material to achieve the same strength as a taller shaped beam You could see this intuitively if you take a ruler or other piece of thin material you have handy and try to bend it You'll notice that in the flat direction it's very weak and easy to bend In the wide direction though it's quite strong Even though the piece of material is the same size having more of it at the top and bottom makes it stronger The reason for this isn't too hard to understand when a beam bends it can't stay perfectly straight In order to form a curve it has to change shape slightly The top of it gets compressed (pushed inwards) and the bottom part experiences a tensile stress (gets pulled apart ) As you move closer to the center there is less stress of each type In fact when you get to the middle the beam is neither getting pulled apart or pushed together it is called the 'neutral axis ' So the top and bottom have to be strong to resist the compression and tension but the center only has to be strong enough to keep everything attached If you did really want to use a cylindrical shape for a horizontal beam (maybe for visual reasons or wind resistance) a round tube would be more efficient than a solid cylinder because it has less material around the neutral axis It still wouldn't be as efficient as a rectangular tube or traditional structural shape though"
527,inzinjerstvo,"According to TWI-Global TIG welding electrodes usually contain small quantities of other metallic oxides which can offer the following benefits - facilitate arc starting increase arc stability improve current-carrying capacity of the rod reduce the risk of weld contamination increase electrode life Oxides used are primarily those of zirconium thorium lanthanum yttrium or cerium Additions are usually of order 1%-4% All these oxides greatly improve arc initiation especially when direct current (DC) welding is employed Thorium oxide (thoria) has been used for many years having been found effective in terms of long life and thermal efficiency Zirconium oxide (zirconia) has been commonly used for alternating current (AC) TIG welding normally for welding aluminium Because thorium is radioactive there are hazards associated with using it The main cause of concern with thorium are the alpha particles it emits Alpha particles cannot penetrate the skin but if they enter the lungs or digestive track they can be carcinogenic The issue with using thorium in electrodes is not so much about radiation exposure during welding but the potential to inhale or ingest thorium laced when the tips of electrodes are ground to have a conical point The conical point is required to maintain maximum arc stability during welding Due to electrode erosion during welding the tip needs to be frequently ground During the grinding process thorium laced dust is produced If not enough care is taken during the grinding of the electrodes (via dust masks &amp; ventilation) there is a risk to the person grinding the electrodes in breathing in the thorium and alpha particles in the dust This then increases the posibility of the person doing the grinding of developing cancer The risk of cancer is low but it appears to be enough of an issue for the Danish Welding Institute to recommend the phasing out of thoriated electrodes because non-radioactive alternatives are available The linked article goes into more detail According to Welding Tips &amp; Tricks pure tungsten electrodes are inferior to thoriated electrodes becaue they quiver and dance when the current is increased leading to difficulty in producing a good weld The site recommend lanthanated electrodes for those concerned about radiation exposure associated with thoriated electrodes Similar warning about the inhalation or ingestion of thorium laced dust can be found on these sites Diamond Ground Products with recommendations from the American Welding Society &amp; the Welding Institute Miller Welds gives a brief comparison between pure tungsten and thoriated electrodes Tungsten electrodes have the highest consumption rate of all electrodes They provide excellent arc stability for AC balanced wave welding and good stability for sine wave welding They are generally not used for DC welding because the arc are not as strong as those produced by tungsten alloy electrodes Thoriated electrodes are apparently the most commonly used electrodes They are preferred for longevity and ease of use Thorium increases the electron emission qualities of the electrode"
528,inzinjerstvo,"According to Maintenance Coatings of Weathering Steel published in 1995 by the US Department of Transportation From page 11 of the report Painting of new uncontaminated weathering steel is generally not considered a problem Test fence and laboratory data developed by the paint industry have indicated that conventional coating systems such as oil alkyds and epoxies will perform comparably on weathering steel and on carbon steel if the degree of surface preparation is equivalent The major problem faced by highway departments and other owners of weathering steel structures is protecting weathering steel that has corroded in the presence of chlorides and other contaminants Conventional cleaning techniques such as dry abrasive blasting do not remove the chlorides which apparently penetrate the bases of pits in the steel The performance of standard highway coatings such as oil alkyd epoxies and zinc-rich systems over chloride contaminated steel has not been satisfactory Additionally the Florida Dept of Transport is recommending using weathering steel for new steel bridges in suitable environments Use a single coat of inorganic zinc paint system for extremely aggressive environments Where higher aesthetics are required use a 3 coat inorganic zinc paint system with clear coat finish The use of the 3 coat system with clear coat finish should be an exception"
529,inzinjerstvo,"That is correct there are a number of unwanted or tramp metals (Cu Sn Sb As) that enter the recycling stream from for example car bodies that are ground into scrap without removing all the copper wiring or tin-coated steel cans Antimony and arsenic tend to creep in from low-quality and low-cost primary iron sources The answer to the question is no Recycled steel is mixed as evenly as possible from varied sources its composition is measured and then pure iron is added as needed to dilute the tramp metals to tolerable levels for resale or further processing such as meeting a specific steel grade for a specific product or application Stainless steels and other high-alloy grades which are known at recycling time are processed separately due to the value of Ni Cr etc It is currently uneconomical to reprocess iron to remove tramp elements and so it simply isn't done at all Two books mention the process as a regular and economical one ( Minerals Metals and Sustainability Meeting Future Material Needs p 284 starting at dilution ) and ( Steel Production Processes Products and Residuals starting on p 104 read until it isn't relevant anymore) The reason it is uneconomical is that the tramp elements react more weakly with oxygen than iron at constant temperature so to remove them by oxidation would require oxidizing all of the iron first The reason for this is thermodynamic and predicated on the fact that among competing reactions those with the largest decreases in free energy proceed virtually to completion prior to other reactions even starting especially with large differences in free energy among the competing reactions To determine which reactions have the largest decreases an Ellingham diagram may be used In the Ellingham diagram below the horizontal axis is temperature the vertical axis is change in Gibbs free energy The lines running across the diagram at various angles correspond to free energy change caused by element oxidation reactions with oxygen as a function of temperature In our case the diagram may be read by choosing a temperature of interest and reading up from the bottom to find the first element to react with oxygen For example if we have steel with Fe Mn Sn and Cu in it we can see that at 1000K then Mn Fe (to FeO) Sn and Cu are the order of largest to smallest drop in free energy Granted the temperature of interest is closer to 1900K (above the melting point of iron) but the general trends of each Gibbs free energy change function continue to the right on the diagram and iron remains below the tramp elements Cu Sn As and Sb at practical temperatures and likely to their respective boiling points As a result removing tramps from Fe would require oxidizing effectively all of the iron first And because Sn Sb As and Cu are slightly soluble in iron they require separation via chemical reaction One can see the solubility of tramps from their phase diagrams with iron of which I have posted Sb-Fe below The diagram has temperature against composition with"
534,inzinjerstvo,"A round-a-bout introduces a disruption in any traffic no matter which direction In your case the only two collision directions - two left turns - are the least used ones so traffic lights working in a preference program with induction loops on the two possible left turns plus uncontrolled right turn lanes would be optimal The crossing would stay in green light as long as there's no need for vehicles to take the turn Only detectors would activate the phases for left turns (and only one would block both the north and the south road) You can either limit the rate at which the side exits can be activated by detectors on the main direction (better more expensive) or by timing it so that there must pass at least X time between the turn activations If real estate is a premium you can create about all of the infrastructure for this crossing by reducing the main road to one lane in each direction dedicating the remaining lane to turns inclusion and the islets Here's an actual map from one of our controllers with a similar situation This one includes two zebra crossings as well It uses more detectors to determine optimal timing of green light and change signals even before given vehicle would have to stop (an interesting thing about this one it uses detection loops on the main road but videodetection on the side road - the road was still due to be finished when the traffic lights were installed and so we couldn't put induction loops in the gravel - currently induction loops would be better there (both more reliable and cheaper) except videodetection is already in place so replacing it would only mean more cost Also if your investor is rich and wants something prestigious and memorable (and truly optimal for the traffic) get a round-a-bout with a viaduct"
536,inzinjerstvo,"Actually handlebars and steering wheels are less similar than you might think When a two-wheeled vehicle is moving fast enough to balance the front wheel is never turned more than a few degrees The primary mechanism for steering is leaning the vehicle not turning the front wheel For example to turn right you actually tug briefly on the left side of the handlebar This causes the wheels to track to the left of the center of mass which in turn causes the bike to lean to the right This lean is what causes the direction to change while maintaining balance &mdash; the total force on the bike's center of mass still passes through the contact patches between the tires and the road During the turn the front wheel is essentially straight with respect to the bike's frame and it's the geometry of how the tires contact the road aided by slight tension on the right handlebar that keeps the bike turning To come out of the turn you tug slightly more on the right handlebar (not the left) which causes the tires to track to the right bringing them into a more vertical alignment with respect to the center of mass This causes the bike to stop leaning and stop turning This is so intuitive when using handlebars most people don't even think about the details of what's actually going on The only time you steer a bike by moving the handlebars by large amounts is when you are moving so slowly that you have one or both feet on the ground (holding the bike vertical) and are trying to maneuver in a tight space Additional points in response to comments An inexperienced bicyclist at low speeds will turn the handlebar wildly back an forth in an effort to maintain his balance but I would hardly call that steering The main reason a 2-wheeled vehicle stays upright is the gyroscopic effect of the rotating front wheel If the bike should start to lean the wheel will experience a force that steers it toward the direction of the lean which corrects the lean If the bike is moving slowly this effect is greatly reduced and the rider is required to use the handlebar to maintain balance Also on a bicycle the rider is typically 90% or so of the mass of the system (bike + rider) and an experienced rider can steer merely by shifting his weight without touching the handlebar at all On a motorcycle the rider might only be about 25% of the total weight or less which makes using the handlebar pretty much mandatory"
539,inzinjerstvo,"Any limit is going to be hard to quantify There are a lot of factors that have to be weighed when choosing the basic material type The short answer is that the limit has already been chosen for each building This was done during the design by the architects and engineers that worked on the building Some of these decisions might have depended on the technologies that were available at the time that the building was designed Some of the factors that would have been taken into account Cost of steel versus concrete - The relative price of the materials has changed throughout history Strength of concrete available - It used to be the case that concrete was limited to about 4 000 psi (27 6 MPa) compressive strength Modern high strength concrete can be higher than 10 000 psi (69 MPa) Strength of steel available - Steel strengths have increased from 36 ksi (248 MPa) to 50 ksi (345 Mpa) and even 100 ksi (689 MPa) Area of wall and column space required to support the upper floors - Buildings are heavy As the building gets taller there is more weight pressing down on the bottom floors This increased force requires more area of material At some point the usable space on the bottom floors is reduced more than is acceptable Per unit of area steel is stronger than concrete so it will take less area to support the same load Stiffness of the building - Very tall buildings sway as the wind blows on them How much they move is controlled by the weight and stiffness of the building Future creep (shortening) of the building - Both steel and concrete creep That is they compress over time if a constant force is applied The amount of creep is affected by age strength or the material and forces acting on the material In very tall buildings this shortening needs to be accounted for in the design A lighter building will need to accommodate less creep Seismic (earthquake) design - Steel is a ductile material Concrete is a brittle material In locations where high seismic forces are expected steel may be required It has the ability to undergo extreme deflections without complete failure Quality control - Concrete will be poured onsite and steel is typically fabricated offsite under controlled conditions The anticipated quality of the end product or amount of oversight required to ensure a quality product are both a cost consideration There are a lot of factors that go into the design of skyscrapers Each item above has a cost associated with it The end result is at least partially controlled by the estimated price Modern skyscraper designs sometimes include a concrete core that goes all or most of the way to the top This shows that there isn't much of a height limit to concrete construction as long as you are ok with a reduced usable volume"
543,inzinjerstvo,"To answer your question bluntly yes the cold can be a concern for standard structural bolts because the cold can be a concern for just about any metal or plastic I can give some insight into why the cold is a factor but I want to be clear that I can't make a recommendation on acceptable temperature ranges for the standard bolts so if you can't find some data to ensure the proper operation of them it may be best to use the low-temperature bolts to be safe In metals this phenomenon is known as the ductile-brittle transition which occurs at a specific temperature based on the material and the strain rate Note that this means it's dependent on how quickly you deform the material not how much force you apply There are two types of deformation elastic (where the material returns to its original shape) and plastic (where the material permanently deforms ) On a molecular level elastic deformation happens when the bonds between molecules in the material are stretched Because no permanent change in the structure occurs the material can retain its original shape At a certain point though the structure begins to change often in the form of dislocation movement Existing defects in the material begin to move and this movement cannot be spontaneously reversed causing the permanent change The ease and rate of dislocation movement is in part based on temperature Temperature is a measure of energy and if there's more energy in a material some of that energy goes towards dislocation movement This is important because dislocation motion can help prevent fracture Cracks exist in all parts they are impossible to avoid and parts will fracture at those cracks because stress is concentrated there and the material is naturally weaker At these cracks part of the energy goes into forming new surfaces (propagating the crack further into the part ) Most of the rest of the energy goes into moving the dislocations by plastically deforming the material If the material is too resistant to deformation (too brittle) no energy will be dissipated by deformation leaving more energy available for crack propagation This is what causes metals to be more likely to fracture in lower temperatures At a certain critical temperature the material becomes more brittle than ductile and its fracture toughness is significantly reduced Determining this temperature is not simple from a theoretical point and it's usually done experimentally I believe This paper is what I used to refresh my memory from my molecular material behavior class it's good some good info but I wouldn't worry about getting bogged down in the math because it's incredibly complex for non-simplified cases The explanations aren't as bad though As I said at the top the simple answer is yes it will affect the bolt strength and I think the standards are reluctant to give specific answers if the bolt isn't designed for a temperature range because bolt size defect size and type of loading can all affect this"
547,inzinjerstvo,"Thermal plumes have been studied extensively for fire safety applications Often you know the heat release rate $Q$ but little more A dimensionless group called $Q^*$ (pronounced Q star ) is used instead of more common parameters like the Reynolds number and Rayleigh number This parameter can be thought of as the strength of the heat source at a particular distance It correlates well for thermal plumes You can derive this group by non-dimensionalizing the Navier-Stokes equations and setting dimensionless groups equal to 1 to define the characteristic length and velocity For more information check out Gunnar Heskestad's paper on this dimensionless group In the fire modeling case generally people ignore Prandtl number similarity and some other things so they say the dimensionless temperature and velocity distributions are only functions of $Q^*$ The most relevant parameters are $$T^* \equiv \frac{T - T_\infty}{T_\infty}$$ $$Q^* \equiv \frac{Q}{\rho c_p T_\infty (g x)^{1/2} x^2}$$ To be more explicit if you know the temperature ($T$) as a function of height ($x$) above the hot object you can find $T^*$ as a function of $Q^*$ $Q^*$ is like a dimensionless spatial coordinate Strictly speaking your setup is not going to be exactly similar because your coil and a human are not geometrically similar (and the heat flux distribution on the coil is probably not similar either) In your photo I assume the human would be lying down if any reasonable geometric similarity is desired The far-field should be okay and I'll assume this is what interests you [2] It's also not exactly clear what quantity you are interested in I assumed you want to get the temperature distribution in the plume say at a height $x_1$ above in reality which would be $x_2$ in your model Correct me if this is wrong Also while I don't do experiments I imagined your heating coil has an output of $W$ not heat flux Let me know if I'm mistaken and I'll change my answer Ignoring the other parameters may or may not be valid in your case (it seems to be okay for fire safety [1]) so I'll do the analysis assuming it's not You can skip the remainder if you want to assume the two mentioned parameters are all you need You can get the number of required groups from the Buckingham $\pi$ theorem The relevant parameters I've identified are $T$ (temperature at height $x$) $x$ $Q$ $g$ $\alpha$ $\beta$ $\nu$ $T_\infty$ $\rho$ and $c_p$ The Buckingham $\pi$ theorem suggests there will be 6 dimensionless groups here (Assuming that I am not missing a parameter I also need to check that the dimensional matrix is not rank deficient For more details about dimensional analysis I recommend reading Dimensional Analysis and Theory of Models by Henry Langhaar ) So the first 5 dimensionless groups are $$T^* \equiv \frac{T - T_\infty}{T_\infty}$$ $$Q^* \equiv \frac{Q}{\rho c_p T_\infty (g x)^{1/2} x^2}$$ $$Pr \equiv \frac{\nu}{\alpha}$$ $$Gr_x \equiv \frac{g \beta (T - T_\infty) x^3}{\nu^2}$$ $$\rho^* \equiv \beta (T - T_\infty)$$ This fifth group is inspired by the Boussinesq approximation In that approximation the density difference is modeled as a temperature difference Similarity in this parameter ensures that your density field is similar For the remaining group I needed to get a little creative Similarity does not require this group takes any particular form but it's best to stick with parameters"
552,inzinjerstvo,"In my civil engineering degree we used ODEs for the relationship between force moment and deflection I don't remember using PDEs myself but my brother-in-law (doing civils at a different university) used them for hydraulics In real life (as a bridge designer) I can't remember actually using calculus University mainly concentrated on the theory and the mathematical models used whereas in actual engineering design we have computer software that does all the calculation for us I think there is a lot of benefit to a theoretical and mathematical background at university - as a professional engineer you need to have a basic understanding to know whether the software is giving you a sensible answer (As an aside as you mentioned Excel I've used that a hell of a lot in real design )"
553,inzinjerstvo,"The problem is that the term uncertainty is colloquially used to refer to lots of different things where it probably shouldn't be For a guide to definitions of various terms in metrology see the VIM The GUM provides a somewhat practical guide to actually computing the uncertainty In particular if you compare a single measurement to some nominal true value as in your first point This is not the uncertainty This would be the measure accuracy or error The uncertainty is a description of the spread of a set of measurements It also requires that a distribution and confidence level of the measurements is given Commonly a normal distribution at 95% (~2$\sigma$) is used The standard uncertainty also sometimes called the standard error is just the standard deivation of the set of measurements i e the uncertainty at one $\sigma$ I your situation you are dealing with a binary situation (Supergrain or not) which is not what I'm very familiar with In such a situation your distribution is bi-nominal although for larger sample sizes you can use the central limit theorem to approximate to a normal distribution In this case wikipedia says the standard uncertainty is given by $\sqrt{\frac{1}{n} p(1-p)}$ where n is the number of grains and p the probability (2%) Scale this up to get whatever your desired confidence level is For the third question I feel you are missing some information to work out how 100 $\mu$m balls effect the number of particles in your sample Do you have some information on the density"
555,inzinjerstvo,"Make sure that the 12V supply is capable of sourcing sufficient current This the source that powers the output A &amp; B The 5V input supply is most likely to power the logic circuitry On PCB board 12V supply power supply should be routed to the H-bridge which will provide the voltage to both Output A and Output B as mentioned above Also when you are measuring current at output A or B make sure their a load I suggest placing a 1K or 10K resistor It is best that you select an appropriate load resistor representing the actual load Product Description from Amazon Feature Chip L298N Logic voltage 5V Logic current 0mA-36mA Storage Temperature -20°c to +135°c Operating mode H-bridge driver (dual) Drive voltage 5V-35V Drive current 2A (MAX single bridge) Maximum power 25W Dimensions 43x43x27mm Package includes 1x L298N Module References Full-Bridge Motor Driver Dual - L298N"
560,inzinjerstvo,"Auto-ranging meters start at the highest range and work down until the reading becomes a certain percentage of full scale on that range Switching can be via eg MOSFETs used to either short resistors in dividers or to pick voltages from appropriate tapping points In cheap meters protection is limited Current much over 200 mA on low current ranges will usually blow the internal fuse Too high current on the high amps range (10A 20A will blow the shunt or a fuse if fitted High voltage on lower voltage AC or DC volts or on current ranges will sometimes destroy the meter (Ask me how I know -) ) All that said Over ranged inputs will not necessarily stress other than the input circuitry Higher than minimum necessary input resistor wattages can protect short term Zener diodes or other clamps can stop high voltage getting into the circuitry proper Very keen manufacturers can provide electronic switches These can be a simple as a high voltage MOSFET in series with the input) which can be turned off when needed This will add some error due to voltage drop but this can be controlled and designed for So eg an overvoltage condition is applied to a current input the input resistor starts to dissipate excessive power the inside end of the resistor is clamped by a zener or TVS (transient voltage suppressor) and a fast MOSFET switch is triggered to disconnect the overload The higher than necessary power dissipation rating of the input circuitry provided enough time for the protection to act Real world example This Intersil application note - AN046 Building a Battery Operated Auto Ranging DVM with the ICL7106 provides specific examples of the design of autoranging equipment and how the issues involved are addressed Here is how the front end looks conceptually and here is how it ends up in practice http //www intersil com/content/dam/Intersil/documents/an04/an046 pdf"
564,inzinjerstvo,"Does the DC load somehow feedback and lower the resistance of the primary coil so that more power can be drawn Yes It would be simpler to analyze an AC load though The diodes are not central to your question The impedance of RL is also transformed so if you have a 10 1 transformer and RL is 2 ? the AC source will see the transformer as a 200 ? resistor ($10^2?2$) As the current in a coil changes it creates a changing magnetic field In the case of a transformer with a load however the change in magnetic field creates a current in the secondary which immediately creates its own changing magnetic field in the opposite direction cancelling out the primary's field People tend to forget that an ideal transformer has no magnetic field while operating Any change in either coil's field is immediately cancelled by a change in the other The feedback is caused by the same effect The primary causes the secondary to change and the secondary causes the primary to change in return When there is no load on the DC side does power still flow through the AC primary coil and if so why doesn't it just melt With nothing connected to the secondary side the secondary coil is open circuited and does nothing It's just some metal that happens to be nearby The circuit is now just an AC source driving the primary coil which behaves as a lone inductor Ideal inductors do not consume any power; they just store energy temporarily in one half of the cycle and return it to the supply on the other half Real coils are not made of perfect conductors though and have some resistance so the power consumed by the primary coil will be determined by the resistance of the wire"
565,inzinjerstvo,"The given information describing a compacted soil sample is as follows initial moisture content $\omega_{init}$ specific gravity $G_s$ initial volume $V_{init}$ initial weight $W_{init}$ For completeness the following information has already been determined moist unit weight $\gamma_{wet}$ using the relationship $\gamma_{wet}=\frac{W_{init}}{V_{init}}$ dry unit weight $\gamma_{d-init}$ using the relationship $\gamma_{d-init}=\frac{\gamma_{wet}}{1+\omega_{init}}$ saturation $S$ using the relationship $S=\frac{V_{water}}{V_{voids}}=\frac{V_{water}}{V_{init}-V_{solids}}=\frac{\frac{W_{init}\omega_{init}}{\gamma_w}}{V_{init}-\frac{\gamma_{d}V_{init}}{G_s\gamma_w}}$ (where $\gamma_w$ is the unit weight of water) Problem The problem is to determine the unit weight and the moisture content after the soil sample has been submerged and allowed to swell 5% The key detail for this problem is This compacted soil sample was then submerged in water After two weeks A soil sample that has been submerged in water for two weeks can/should be assumed** to have become saturated ($S=100\%$); i e all of the air in the void spaces has escaped and the void space is now 100% filled with water The list of soil sample properties that can be assumed to remain constant after submersion is fairly short Specific gravity $G_s$ Weight of solids $W_s$ All of the other properties such as saturation unit weight dry unit weight moisture/water content void ratio etc are dependent on the volume of voids and the amount of water in the soil Both the amount of water (it was submerged) and the volume (it has swelled) have changed so ALL of these properties will also change Once all of this has been recognized the remaining portion of the problem is trivial New wet unit weight $\gamma_{new}=\gamma_{sat-new}=\frac{W_s+W_{w-new}}{V_{new}}=\frac{\gamma_{d-init}V_{init}+\gamma_w(V_{new}-V_{solids})}{V_{vew}}=\frac{\gamma_{d-init}V_{init}+\gamma_w(V_{new}-\frac{\gamma_{d}V_{init}}{G_s\gamma_w})}{V_{init}(1+5\%)}$ New moisture content $\omega_{new}=\frac{W_{w-new}}{W_{solids}}=\frac{\gamma_w(V_{new}-V_{solids})}{W_{solids}}=\frac{\gamma_w(V_{init}(1+5\%)-\frac{\gamma_{d}V_{init}}{G_s\gamma_w})}{\gamma_{d-init}V_{init}}$ Mechanism of Soil Swelling Behavior The simplified effective stress equation is as follows $\sigma^{\prime}=\sigma-u$ Where $\sigma^{\prime}$ is the effective stress $\sigma$ is the total stress and $u$ is the pore water pressure The above equation assumes a static condition However when the simplified effective stress equation is imbalanced a dynamic condition occurs and the soil must either consolidate (i e shrink ) or swell Swelling of soil occurs when the two sides of the simplified effective stress equation are not balanced and There is positive pore water pressure inside the void space of the soil and the effective stress inside of the soil matrix is greater than the externally applied total stress minus the pore water pressure Said another way when a soil is compacted some amount of total stress is applied Once equilibrium has been achieved this total stress is associated with some combination of effective stress and pore water pressure If the total stress changes the pr"
577,inzinjerstvo,"The loops are known as expansion loops They need to be placed in pipelines to enable the pipelines to contend with thermal expansion and contraction and other forces that can affect the pipeline They are typically placed in gas pipelines irrespective of when the gas is hot or cold - natural gas or steam The following quote is from Pipeline Design It's near the end of the page under Pipe Expansion and Supports Steel piping systems are subject to movement because of thermal expansion/contraction and mechanical forces Piping systems subjected to temperature changes greater than 50°F or temperature changes greater than 75°F where the distance between piping turns is greater than 12 times the pipe diameter may require expansion loops Such loops are not only used in gas pipelines and are used in pipelines that convey other fluids such as oil Pipes transporting liquid fluids can use other expansion control devices such as single slip expansion joints or bellow type expansion joints Accommodating Thermal Expansion in Pipes Pipeline Safety"
579,inzinjerstvo,"user466 it's true that some people aren't cut out for engineering coursework and exams This doesn't make them dumb You might be one of these you might not Regardless of which it is that isn't really your problem Your real problem is you have not learned how to deal with failure productively Success in your engineering courses - or in school in general - is a very poor predictor of one's future success as an engineer or in any other profession It is also a poor predictor of your employability earning power and happiness in your vocation However whether or not one has learned how to fail well is probably the single most important predictor of their future success as an engineer (as well as the other things) Go read this book dust yourself off and persevere EDIT You said this in the comments i just can't figure out why i have to go through this i could be getting a perfect GPA Your priorities are in dire need of adjusting I'm totally serious about the above advice You need to learn how to fail Getting a perfect GPA while certainly nice is not going to serve you in the real world nearly as much as having some early experiences at failing and learning how to do that the right way But if you continue to handle it the wrong way it will ruin you So go do it right"
587,inzinjerstvo,"Doing a brief review of the literature for spherical particles at high Reynolds numbers it appears that you can use standard drag curves to get the drag coefficient as long as you use the following Reynolds number $$\mathrm{Re}_\text{PL} \equiv \frac{\rho V^{2-n} d^n}{m}$$ This Reynolds number assumes the fluid follows a power-law $n$ is the flow behavior index which is less than 1 for a shear thinning fluid $m$ in the above equation is $K$ on Wikipedia So if you just want the terminal velocity you can plug this in the standard equations for terminal velocity as all they assume is that you know the drag coefficient at the terminal velocity state (You can derive them from a steady force balance or from the unsteady case I recommend trying both just to get a feel for these sorts of problems )"
592,inzinjerstvo,"The short answer is yes it is that simple Think about it this way The pins are connected to end plates of a given area If all we're worried about is the tensile strength we can place a one dimensional force on each end plate to put the system in pure tension Now split the system into a couple parts and do a force balance If we split it right down the middle so that we're seeing the actual force on the top plate and the internal force along each of the pins These two forces have to be equal otherwise the system would be in motion meaning the pins have yielded or fractured Now to calculate the stress in the pins we simply divide the force applied across all the pins by the total area Because in this case the pins are all of even cross sectional area the stress will be divided evenly between them The implication of this is that none of the pins should fail before the others and any difference in their tensile strength is due to manufacturing defects or variations so we can just multiple the tensile strength of a single pin by the number of pins to get the overall tensile strength There is an important assumption here that the pins are distributed evenly on the plates If the pins are lopsided they're going to end up under some amount of bending which complicates the stress calculations in them and adds to the stress that they see under the same applied load If we make the pins of variable cross-sectional area then the load begins to concentrate in the larger pins but it should still end up with the same stress in each pin because the load will distribute proportionally to the area which is how we calculate stress anyway Where this gets more complex is if the pins are made of different materials or treated differently to change the tensile strength This is essentially creating a composite material and there we have to take into account volume fractions to calculate new yield and tensile strength numbers but also consider which material fails first For further reading I'd suggest chapter 3 in this MIT book (that is a 128 page pdf beware if you're on mobile)"
593,inzinjerstvo,"After starting to redo the analysis I've come to realize this is a fairly complicated process and instead I'll outline what I would do to solve this if I had more time First some assumptions I'd make perfectly plastic behavior of the HDPE (i e the stress-strain curve is flat after yielding) thermal convection from the flat surface is irrelevant (though it is taken into account for the pin) the pin has an adiabatic tip the metal is at a constant temperature the pin pierces the surface when the energy absorbed by a cylindrical portion of the material directly beneath it is exceeds the toughness of the material multiplied by the volume of cylindrical portion of the material (I can explain this differently if desired ) the contact resistance decreases as the HDPE melts (not sure how) some material properties can come from this paper and others from this data sheet and this book (this data may or may not be valid for the HDPE you have) I'm not sure what value of the convective heat transfer coefficient for the pin is valid and would have to think more about this heat of fusion comes from this data sheet Given all of this information you can write 1D heat transfer solver for the Stefan problem to compute the temperature and thickness of the solid skin From there you can use the temperature of the skin to find the yield stress My simple model suggests that $F = \sigma_\text{y} A_\text{contact}$ Old answer The answer is likely zero force or nearly so because the melting temperature of HDPE is about 266 F ( it's reported a little higher here at 279 F ) so the HDPE probably has already melted at the specified temperature You'll meet the molten layer immediately upon touching the surface (at least if the temperature is uniform enough) If the temperature range is wrong I think I can develop an estimate assuming that HDPE is a perfectly plastic material that the impacter is flat and some other things and then change my answer Let me know if you'd like this Also if you want an estimate of the force required to continue penetrating the medium I can find an old terminal ballistics book I read that has some models for such things but I don't know how accurate they are"
597,inzinjerstvo,"The protactinium separation is a nice benefit of a liquid fluoride thorium reactors made possible by the fact that the fuel (and the protactinium) are in liquid form That it easy to pump around and do chemistry-stuff with The Shippingport reactor was a solid fueled (thorium oxide) reactor with water as a coolant and moderator So the protactinium would have been stuck in the fuel elements Other fuel cycles (e g U-235) generate reactor poisons as well These actually render solid fuel elements useless before all of the fuel has been consumed It is possible to melt the fuel down and recover the useful fissionable material This process has not enjoyed the level of adoption that it might otherwise due to politics bureaucracy etc Often spent fuel is simply disposed of without reprocessing"
599,inzinjerstvo,"I'm not sure of the specifics of your problem it seems to be relatively straightforward to me I have previoulsy considered addressing the subject of measurement errors with schoolchildren in the following manner - it may be of help to you and your audience or change in subject may confuse and distract them from your topic that's up to you to decide Step 1 - give all the students and length of string (considerably longer than a metre) and ask them to put two knots in the string exactly a metre apart based on their own estimation The smart ones will use a length they already know as a guide - most common is their own height Step 2 - measure the lengths achieved against a ruler and see who got closest A leaderboard of some kind can make this more fun This can lead into a discussion of estimation in general Plotting a scatter plot of the measurements might also be interesting and lead into a discussion about distributions (with enough students I think I'd expect a roughly normal distribution ) Step 3 - now change the ruler for a tape measure and get everyone to remeasure someone elses string - they will inevitably get slightly different results Write these down and look at the differences Perhaps now also draw out a scatter plot of the differences The change in results should bring up all sorts of questions to prompt your discussions on metrology Examples could be Are the metre ruler and the tape measure the same length Which of these is the most accurate measure of a metre How can we test them Why do two measures of the same thing come out different Was there an element of judgement by eye on the part of the measurer Did the string stretch Did the tape measure stretch Were the conditions in the room the same Have the knots tightened or slipped Was the level of precision used in recording the measurements the same What is the right level of precision Did we agree on what part of the knots we were measuring to Is there a general trend in the difference results Are they dominated by bias or random errors I've done steps 1 and 2 with students along with similar experiments on time and mass (Generally they are roughly ok at estimating a metre largely underestimate how long a second is and are wildly inaccurate at estimating a kilogram) Step 3 is an extension that I've planned but not yet gotten round to trying out I think you could also carry out similar experiments/demonstrations with your exact problem of liquids in containers as well"
606,inzinjerstvo,"So let's break this down piece by piece The output must only show LOW when BOTH inputs are LOW We also know that the sensors read High (5V) when the tank volume is above 25% | Tank 1 | Tank 2 | Monitor LED | |--------|--------|-------------| | High | High | Lit | | High | Low | Lit | | Low | High | Lit | | Low | Low | Dark | Please note For the sake of this explanation we're assuming 0 means low and 1 means high So the logic diagram above is upside down from a traditional representation that starts low and iterates to high And if we look at a typical AND gate we find that we're close but our logic is backwards So let's introduce the NOT gate But if we put the NOT gate after our AND gate we end up with a NAND gate and that's not the logic diagram we want Dang that brings us back to square one right Nope not quite What happens if we use two NOT gates and move them to the other side of our AND gate That seems to do the trick Our High signals become Low and our Low signals become High I'll let you work out the algebraic expressions for that diagram; it should be trivial to translate now The fun part is trying to express this using just NAND gates The trick is to use the NAND gate as an inverter and realize that our logic is backwards from the voltage that's present If we layout the three NAND gates like this And if Tank 1's sensor is fed to both gates of U1 and Tank 2's sensor is fed to both gates of U2 we're using the first tier of NAND gates as inverters Looking at the logic diagram we see that our green LED will be lit in the cases we want and not lit in the case when both tanks are below 25% | Tank 1 | Tank 2 | T1 V | T2 V | T1 V | T2 V | 2nd NAND | LED | |--------|--------|------|------|-------|-------|----------|-----| | High | High | 5 | 5 | 0 | 0 | 5 | Lit | | High | Low | 5 | 0 | 0 | 5 | 5 | Lit | | Low | High | 0 | 5 | 5 | 0 | 5 | Lit | | Low | Low | 0 | 0 | 5 | 5 | 0 | | You also asked Therefore my solution for 3 1 and 3 2 would be using an OR gate Would I be correct on this Looking at the truth table with 0 for Low and 1 for High And that would work too But using an OR gate doesn't necessarily set you up to easily understand how to use the NAND gate configuration If nothing else this exercise should help you understand the duality in being able to express positive and negative boolean logic"
614,inzinjerstvo,"My answer of that question covers this a little bit and the link fully explains it but I can go into more detail so you don't have to wade through that giant PDF I'll link it again here at the bottom because I'm going to reference equations in it for this answer As you might imagine the total strength of a composite such as the example above is a combination of the two materials The proportions in which each material contribute to the overall strength are related to the volume fractions of the constituent materials Because in this case the lengths of the bars is always equal and the bars are of constant area along their length we can simplify the volumes to areas This means that we can compute the elastic modulus using a version of equation 3 1 in the book $$E_c=E_{St}A_{St}+E_{Al}A_{Al}$$ Here St and Al are steel and alumin(i)um respectively the c subscript refers to the composite and A is the area fraction not the total area So in the example problem $A_{St}=0 5$ and $A_{Al}=0 5$ To calculate the stress in the bars we simply use the definition of stress given that we can measure the strain and we know the strain is the same in all bars $$\sigma_c=\epsilon E_c=\epsilon E_{St}A_{St}+\epsilon E_{Al}A_{St}=\sigma_{St} A_{St}+\sigma_{Al} A_{Al}$$ At this point though it's important to note that we are seeing different stresses in the bars of different material which is a natural result of them having the same strain but different elastic moduli Unless the materials break at the same strain one of them will fail first at which point the same load will be applied to the whole system but only one type of bar will still be intact to carry that load The total cross sectional area of the system is reduced as well though We need to determine which material breaks first Because we're applying a known strain rate here we can find the ultimate strain of each material using the stiffness and ultimate strength Once we know the strain at which the first material breaks we plug that strain into the second equation above to see the total stress in the composite at this elongation However because the same force is being applied over a smaller area the strength of the composite after the breakage of the first material (in this case the steel) is given by $$\sigma_{TS c}=\sigma_{Al} A_{Al}$$ We can compare this value of tensile strength to the value of tensile strength just before the steel breaks If the value before steel fracture is greater then the steel fracture results in the fracture of the entire system If the value after steel fracture is greater the system will continue to carry load until it reaches the tensile strength of just the aluminum bars Mechanical Properties of Materials David Roylance 2008"
624,inzinjerstvo,"This is a question about the law not engineering and the law is entirely dependent on the jurisdiction Assuming that we are dealing with a common-law country like Australia the UK or the US then liability can arise from three sources the contract statutory liability and the tort of negligence Contract Basically you owe an obligation under a contract to do what it says in the contract; noting that this may contain implicit terms and terms read into the contract by statute Liability under the contract arises to one party when the other party breaches a term or condition of the contract If you have complied with the contract then the delay cannot have caused you to stop complying Statute There may be laws in the relevant jurisdiction (which could be the jurisdiction of the owner the engineer the site or all of these) which impose a liability on the engineer If this is an obligation to be read into a contract then see above If this is a duty owed to the State then it would generally be for the state to prosecute either criminally or civilly In most jurisdictions compliance with a code or standard is at best a partial defence so if you have not complied with the law initially then you would be liable Most laws are not retrospective so it is difficult to see how the delay could change your liability However in circumstances where you know that the project is being constructed to obsolete codes and these may cause a breach of the new statute (particularly one going to WHS or environmental damage) there is probably an obligation to do what you can in the circumstances Drawing this to the owners attention would probably suffice for this Negligence In order to establish negligence as a Cause of Action under the law of torts a plaintiff must prove that the defendant had a duty to the plaintiff breached that duty by failing to conform to the required standard of conduct (generally the standard of a reasonable person) the negligent conduct was the cause of the harm to the plaintiff and the plaintiff was in fact harmed or damaged A case bought where the engineer was in ignorance of the project proceeding would fail on the first limb If the engineer was aware the owner would still find this a difficult hurdle - what duty does the engineer owe to the ex-client If they could overcome this they would probably fall foul of 2 and 3 - the engineer could quite reasonably assume that the owner had engaged others to update the plans Note that I have not even considered that there must be actual harm or damage Notwithstanding if the engineer on becoming aware wrote to the owner to say Hey I hope you are not using my plans because they are out of date then I would suggest that this would be all that a reasonable person could do"
626,inzinjerstvo,"The difference between the two equations The cavitation number is the ratio of the static pressure difference to the dynamic pressure difference So if you want to use the first equation you would need to take the pressure using a Pitot tube to measure the total pressure whereas if you want to use the second equation you will need to measure the freestream velocity but I would recommend measuring it upstream rather than downstream because of possible effects of acceleration and boundary layer growth Also your $V$ should be $V_{in}$ such that it corresponds to the same location where $p_{in}$ is measured because this equation is derived from Bernoulli's equation which says the energy is conserved along a streamline Is one form preferred over the other In all my experience working in cavitation research for many years we have almost always used the latter equation you mentioned (although I have mainly been working in hydrofoils and propulsion systems) The reason is that we could get more accurate non-intrusive velocity measures using Laser Doppler velocimetry (LDV) than by using an intrusive method Where can I get accurate data to predict cavitation based on these numbers It is difficult to use experimental data to predict the cavitation number because of the differences in things like turbulence intensity and air nuclei content which are difficult to match in reality with controlled laboratory methods Traditionally in my circles this is done by running some CFD analysis codes on your design There are two different approaches here (1) compute the average mean flow using a RANS or LES technique and (2) using a bubble dynamics code which will model the air nuclei but requires a flowfield (either from experimental measures or from the from the CFD model) If you use a typical RANS CFD model to compute the flow-field it should give you the the pressure coefficient which has a very similar definition to the cavitation number $$C_P = \frac{P-P_\infty}{\frac{1}{2}V^2}$$ If you are doing some CFD calculation on your nozzle you should find the location of minimum pressure and that is the place where cavitation should occur You can infer the cavitation number from this pressure coefficient as $$\sigma = -C_P^{min}$$ where $C_P^{min}$ is the minimum pressure in your nozzle I explain this in more detail in this paper However this will only give you an idea of the time averaged cavitation inception number Most people don't go to such detail in trying to get such an accurate prediction of cavitation inception unless it is absolutely critical If you want to get a more accurate number you need to consider that cavitation inception requires three things to happen at the same time (1) a local area of pressure which is below the vapor pressure of water (2) an air nuclei which enters into that low pressure region and (3) the air nuclei must be in the low pressure for a significant enough time that it basically rapidly grows becomes unstable and hence collapses The way people have been able to more accurately estimate this is by using a using a Lagrangian method that simulates sending air nuclei through an Eulerian CFD dataset Some of the real experts in this field are the people at Dynaflow-inc com I might suggest taking a look at this paper Chahine G L Nuclei Effects on Cavitation Inception and Noise 25th Symposium on Naval Hydrodynamics St John's NL Canada Aug 8-13 2004 <a href= http //www dynaflow-inc com/publications/pdf_docume"
627,inzinjerstvo,"Superhydrophobic surfaces generally consist of a series of micro/nano scale pillars that increase the contact angle of water on the surface Larger contact angle means water forms more spherical drops and can roll off more easily Changing the chemical properties of the surface layer can also help change the contact angle These surfaces are optimized to effect the contact angle of water so will not have a significant effect on the drag of vehicles (except maybe boats) or radar detection However surfaces with other patterns/shapes of structures can have these effects See riblets or sharkskin suits for reducing drag in aircraft/swimmers and moth-eye anti-reflection coatings for light not radar but that is probably just the size of features Common uses for super-hydrophobic coatings are in water repellant windscreens for cars and on waterproof jackets"
632,inzinjerstvo,"There are a number of techniques for meshing complex domains for Finite Element Analysis They generally fall into two categories Structured vs Unstructured For structured meshes basically the entire mesh can be mapped directly to a 3D array of XYZ coordinates whereas unstructured grids cannot There is a good description of the classifications with pictures here http //en wikipedia org/wiki/Grid_classification Within structured meshing there are two specific types Structured meshes Cartesian mesh - This is basically using hexahedral cubes to represent the elements A well known package that uses Cartesian meshing would be Cart3D This is not really complicated but the difficulty is defining where the cubes intersect the surface Body-fitted mesh - in body-fitted curvilinear meshes they can be divided into algebraic grids or elliptic grids In either case the user has to define the points on the boundaries of the domain To generate points in the interior of the domain algebraic grids usually use some variation of a technique called Hermite interpolation to generate the interior points Elliptic grids can produce curvilinear grids where basically all the gridlines are orthogonal and are generally what is used when it comes to body-fitted meshes The interior points here are basically computed by solving an elliptic partial differential equation The defacto textbook for these types of body-fitted techniques is available online here http //www erc msstate edu/publications/gridbook/ The author of this book is basically considered the father of grid generation because he came up with the Elliptic mesh for mesh generation Unstructured meshes Since unstructured grids cannot be mapped to a 3D array so they must also specify a connectivity mapping which can relate which elements are related to other elements The basic algorithm that is used is called Delauney triangulation which is discussed in detail here http //en wikipedia org/wiki/Delaunay_triangulation One of the popular books that covers this topic is called The Handbook of Grid Generation The basic algorithm here is given an initial set of points on the boundary (1) Compute an initial triangulation (2) Perform a quality check based on Ruppert's refinement algorithm ( http //en wikipedia org/wiki/Ruppert%27s_algorithm ) (3) Insert or delete points based on Ruppert's algorithm such that the Tetrahedra that are generated have a minimum angle (e g 24 degrees) To answer your question about criteria what makes a good mesh has to do with a number of factors but a couple of the most important factors are (1) grid resolution (is there enough grid points to get the resolution required) and (2) the geometry of the elements (skew minimum angle aspect ratio etc ) This is discussed here http //en wikipedia org/wiki/Types_of_mesh These both will effect the quality of a Finite Element solution There is another aspect of unstructured grid meshing called Advancing Front which is used to produce points near the boundary in the case of Fluid Dynamics After saying all that most techniques require some work up front and then are somewhat automatic as well In any type of mesh algorithm the user is going to have to spend some time to define the geometry and some initial point distribution on the surface"
634,inzinjerstvo,"Disclaimer I've studied control theory from a mathematics perspective not an engineer's perspective Classical Control Theory is based on linear systems and is also limited to them as well Linearization is helpful in many cases but not entirely applicable in others Analysis tools (laplace transform pole placement root locus routh hurwitz etc ) are easiest to conduct on linear systems Optimal Control Theory is the more modern approach because it can handle nonlinear systems directly without using the frequency domain The idea is that control can be achieved by minimizing a suitably chosen cost function over time Usually this cost function is a function of the state space variables and is usually subject to constraints defined by the dynamics boundary conditions and/or feasible paths The word optimal doesn't mean that its necessarily better or superior to classical control theory Here optimal means that control is obtained by finding a relative extrema (usually a minimum) of a function Depending on how the objective function is chosen minimization can require either solving a linear or non-linear system of equations Fortunately many numerical/analytical tools from optimization and/or calculus of variations can handle such problems Of course minimizing a non-linear function can be a rather costly endeavor from a computational point of view If it is possible to analyze by some other means (i e classical control theory) then it may be more worthwhile to do so"
638,inzinjerstvo,"You might be interested in the National Society of Professional Engineers' page of professional liability resources which has a section for insurance There are quite a few entities offering professional liability insurance for engineers; NSPE surveyed 18 of them last year with questions like Question #6 What percentage of your total book of [Architecture/Engineering] premium comes from firms with a revenue of Less than \$500 000 \$500 000 to \$5 000 000 More than \$5 000 000 Professional liability insurance is a very important part of running a business and engineering firms are no exception Independent engineering contractors may also carry insurance However even in jurisdictions that require professional liability insurance in certain fields there may be lesser-known alternatives or loopholes that allow the requirements to be circumvented For example the state of Florida does not require all practicing physicians to carry medical malpractice insurance As an alternative to having an actual malpractice insurance policy Florida law also allows doctors to use other types of pre-arranged secured assets to cover claims in these amounts like trust accounts bank letters of credit and similar arrangements There is nothing inherently wrong with these other types of security but they are rarely used the law also has a loophole that allows doctors to carry no insurance at all If your doctor practices without insurance he should have a sign posted on the wall of his office advising his patients of that fact So when you ask if engineers are insured in a similar fashion to doctors you need to understand that even for doctors the situation varies from jurisdiction to jurisdiction I'm not aware of any US state that requires individual engineers or engineering firms to carry professional liability insurance; that said I studied in California and I've never worked for a firm practicing in another state so such laws may very well exist To quote NSPE again Many firms carry and maintain professional liability insurance policies and/or use specific methods to transfer or mitigate professional liability risk exposure NSPE encourages individual employed engineers and their companies to work in harmony to minimize professional liability risk exposure consistent with good professional practice and sound business judgment While insurance coverage is not the only way to protect the firm and employees it is NSPE's position that companies that do not carry professional liability insurance should disclose that fact in writing to their employed engineers For a more in-depth treatment of the subject I recommend auditing or enrolling in an online course such as Concepts of Engineering Practice from MIT OpenCourseWare"
647,inzinjerstvo,"For a given method of transport to become the most popular it needs to be the safest cheapest most efficient way to get from point A to point B relative to comparable forms of transportation In this case the comparable transportation method is to use normal trains which run on coal electricity etc Let's compare the two Safety This is sort of hard to compare because the two methods involve different ways of killing people There are the old run-someone-over and hit-a-service-vehicle scenarios and in fact the only major (and fatal) accident on a maglev train occurred about eight and a half years ago 23 people died when the train hit a service vehicle However the crash was attributed to human error not the maglev technology Putting these types of accidents aside maglev trains aren't any more dangerous that normal trains People can die if they accidentally touch power lines but normal electric trains pose the same risks Fires aren't any more likely to be started and fuel can't be spilled So safety isn't an issue Cost Here there is a difference In a Congressional report Report to Congress Costs and Benefits of Magnetic Levitation it was estimated that maglev tracks would cost 40-100 million dollars per mile compared to 10 million dollars for high-speed rail (HSR) The cost can vary given the area the train is passing through The report estimates that a maglev system would cost 1 92 times as much as an HSR system in rural areas 1 22 times as much in suburban areas 1 20 times as much in mountainous areas and 1 13 times as much in urban areas That's very interesting because it shows that the technology might be accepted in some areas but not others Maglev subway systems and elevated railways could become more prevalent in cities but not in rural areas It wouldn't take too much of a change in infrastructure Different routes lead to different costs per mile with the cost of routes in the American Northeast Corridor being one and a half times as much as average Fortunately thus setup is an outlier and is really due to the high population in the region and the replacement of the existing rail lines Different methods range in price too The Congressional report also compares the programs of different countries They vary quite a lot though and there doesn't seem to be a definite pattern One last thing The report was written in 2005 Since then the value of the dollar has changed and technology has improved I think that the cost of maglev trains has gone down quite a lot since then Efficiency This paper (admittedly short) compares a German Transrapid maglev train and the ICE 3 high-speed train also developed in Germany The energy usage is about the same though at lower speeds Transrapid has the edge over ICE 3 There isn't data for higher speeds though which is odd because the newest Transrapid models can go much faster than the data explains This article shows that maglev trains are still many times more efficient than airplanes or cars It's also a bit speculative (using maglev as a launch system for spacecraft ) but it's fairly comprehensive But it's a bit dated like the Congressional report"
654,inzinjerstvo,"Electrical powered domestic refrigerators developed from ice boxes also known as cold closets which were tin or zinc lined insulated cabinets that had a ice compartment As the ice melted it kept the inside of the ice box cold For premises not connected to the electric grid kerosene refrigerators were developed and are still available kerosene refrigerator There are even refrigerators powered by gas and propane Alternatively powered refrigeration units are still produced With mains/grid powered refrigerators as you allude to the compressor and power unit of electrically powered refrigerators are part of the domestic refrigeration unit for convenience It is possible to have the compressor and power unit outside the premises as occurs with split system air conditioners but it would be inconvenient By having grid powered refrigerators as a compact unit they can be placed anywhere inside or out connected to a power outlet and they start operating For refrigeration on an industrial scale cold rooms are used Depending on their scale and portability the compressor and power unit are on the outside"
657,inzinjerstvo,"The Oak Ridge National Laboratory had an experimental thorium reactor in the 1960s but the program was terminated in the 1970s This can be partially be attributed to the needs of both the military and businesses at the time Unlike $^{235}{U}$ thorium is not naturally fissile it needs to be bombarded with slow neutrons to ultimately produce $^{233}{U}$ which is fissile and can be used as a nuclear fuel One of the issues for the military was that the products of the thorium fuel cycle cannot easily be used for nuclear weapons production which was an aim of the nuclear powers at the time $^{233}{U}$ can be used in nuclear weapons but it is contaminated by $^{232}{U}$ which has a half life of 68 9 years but is an intense gamma radiation emitter which makes it hazardous to handle This also makes it a problem in thorium based nuclear power systems Another reason is business had invested heavily in uranium reactors and developed that technology and it wanted a return on its investment Thorium reactors were perceived as a competing technology Research into thorium reactors is being done by Norway China USA India Some more information about thorium and thorium reactors Liquid Fluoride Thorium Reactors Pros &amp; Cons of Thorium Thorium - World Nuclear Association"
658,inzinjerstvo,"The main high-level technical difference is that flexible panels are pretty much useless They're an absolutely tiny part of the market A novelty A toy To understand why have a think about what a solar panel does It converts sunlight to electricity To do that in needs to capture sunlight And to do that it needs to be facing the sun So careful calculations are done to select the correct angle and alignment when designing PV plants Flexibility (beyond the minimum needed to avoid damage under high wind loading) is a useless attribute here Once you've aligned your PV the last thing you want it to do is to go flapping off in different directions And although PV is cheap (pretty much the cheapest power there is now in the tropics and sub-tropics once you've accounted for a sensible carbon price) it's not so cheap that you can afford to waste 80% of the potential generation because it's not facing the sun at the right times To make things worse the longevity of PV depends on the encapsulation staying complete If there are breaks in it then water vapour and other chemicals get in and the performance degrades quickly Too much flexing will lead to breaks in the encapsulation"
665,inzinjerstvo,"The place where air and fuel are mixed is the combustor also known as the flame holder The diffuser takes in the compressed air and slows it down (remember that aircraft with jet engines are traveling extremely fast) If the air goes too fast it won't burn enough; if it goes too slow it won't provide enough thrust Air goes in through gaps in the liner There are holes all along it called primary holes intermediate holes and dilution holes* These further slow the air down and help it enter the main part of the combustion chamber However most of the air goes through the swirler which mixes the air as it enters The dome is a related device Inside this chamber is an igniter which creates the sparks needed for the fuel to burn Next to it is the fuel injector a pipe which inserts fuel into the chamber The air follows a series of complex paths inside the combustor This picture should give you a good idea of them Now you probably understand the names of the holes into the main chamber They cool the combusted air (entering through the swirler) as it goes out the other end and into the next (and rear) section of the engine"
670,inzinjerstvo,"To clarify further it's important to understand that that a Byte is 8 Bits A single bit is a single 1/0 So lets look at bits first Bits are base 2 numbers That means they can be a one or a zero and that's it For example 1001 0101 = 149 The reason for this is that each place goes up by 2^(n+position) starting at the 0th position on the right most bit The space I've added above is only to make it clear this is 8 bits and a digital device wouldn't use spaces So to count in binary (what bits use literally being equivalent to base 2 number system) 0001 = 1 0010 = 2 0011 = 3 0100 = 4 0101 = 5 0110 = 6 0111 = 7 1000 = 8 1001 = 9 etc As you can see this rolls along quite nicely Recall bytes are groups of 8 bits and 11111111 = 255 Thus 8 bits (or one byte) can represent 256 different numbers (0-255) And because as you said kilo is 10 for bites 2^10 = 1024 instead of 1000 And that would equal a Kilobit The reason I went into bytes at all is because it's a notable difference that is easy to get confused with storage sizes and network speeds Because hard drives are formatted in bytes instead of bits (simply as presented to the OS) and network speeds are given in bits (eg 100Megabit per second Mbps) this can easily cause some confusion For instance say you're downloading a 100 MB (that's MegaByte) file on a 100Mbps connection If your connection was 100MBps (megabytes per second) you could literally download that file in one second However because our real transfer rate is Megabits and there's 8 bits to a byte it would actually take around 8 seconds So because of all this base 2 to base 10 conversion that we have going on we end up with some strange numbers that one may not expect in a normal base 10 metric system"
671,inzinjerstvo,"The total length of the pipeline has little to do with the length that oil can be pumped in a pipe This is because a pipeline is broken into many smaller segments between pumping stations Stations are conveniently located either where required (see discussion below) or where another pipeline joins in Pipelines are rarely one single pipe between point A and point B They have lots of smaller pipelines connecting into them Each of these pipeline will have their own pump stations These stations also help to divide the pipeline into segments With segments you can ensure that different oil is sent to specific locations You can think about the system from a logistics point of view Any long pipeline is almost guaranteed to pass close to another location that either produces or consumes oil It only makes sense to connect to these locations as the pipeline goes past Factors that affect the length that oil can be pumped in a single pipe This is a large topic Everything that affects regular pipe design also effects pipelines Length of pipe Smoothness of pipe Temperature of the oil - Oil in pipeline is usually heated because warmer oil has a lower viscosity and is easier to pump Elevation changes Number and degree of turns Pressure capacity of pipe Diameter of pipe Allowable pressure drop between ends A lot more Structural Integrity of Pipe The length of the pipe does not influence the strength of the pipe All pipe lengths in a pipeline will be welded and tested to the same requirements"
673,inzinjerstvo,"If it were a chemical protection issue it would most likely be sensitivity to oxidation or moisture damage The other possibility is double protection against tampering which has in the past lead to people dying as in the case of Tylenol being laced with cyanide in Chicago in 1982 The other issue about tampering or hoax tampering is that it has been used in extortion attempts against pharmaceutical companies As a result of the Tylenol tampering case in 1982 the company involved introduced tripled sealed packaging That crisis cost the company involved $100 million at the time Edit After the question was altered to specify prescription medication I agree with GlenH7's comment below after the question was altered that being prescription medication would mollify the case for tampering but not totally eliminate it The only other thing I can think of is to increase the shelf life of the medication if the outer foil envelope was hermetically sealed"
676,inzinjerstvo,"In the Fluid Dynamics community about 40 years ago the group was primarily divided into experimentalists and theorists However at that time CFD was quite new had to be run on expensive supercomputers and untrusted It was quite common that a theorist or experimentalist would at best discount the results of the CFD while others may totally disregard the CFD results as useless In fact my former PhD advisor Dr David Whitfield was one of the early pioneers of using CFD alongside aerodynamics experiments at the Arnold Engineering Development Complex (AEDC) This reference explains well the thinking about CFD in those days At AEDC CFD was used to supplement wind tunnel testing but according to Dr Whitfield not a lot of people believed in CFD in the early 1970s In fact he said my efforts to promote CFD within AEDC in the early 1970s probably got me kicked-out or through most mahogany doors However when CFD was used to explain the source of the flow angularity problem in the test section of 16T and when AEDC Fellow Dr John Adams' CFD group in VKF explained how a tunnel there was actually operating at Mach 12 and not Mach 16 as previously thought CFD found new life I was told once that 'AEDC is a test-data place and there is no place for CFD ' he explained Our objective was to help those running the tunnels to be able to do their jobs better I don't think AEDC should just be a 'test-data' place Rather it should be a place for solutions and physical understanding of the problems and this can be accomplished better by the mutual cooperation between those focusing on experiments and those focusing on numerics In those days generally the designer would design a new prototype and send it to the wind tunnel to test and maybe some CFD would be performed at the same time There would generally be many prototypes built and tested which was very costly One such experimental facility where I used to work charged $16 000 per day of testing On the other hand with the development of robust open source CFD codes such as OpenFoam and cluster computers CFD simulations are quite cheap So over time CFD began to mature and with the popularization of cluster computers became quite feasible to run cheaply With more and more validations with experiments being published in such journals as the AIAA Journal CFD models have begun to be trusted more and more Nowadays the cost of running experiments is much more expensive than running CFD simulations Therefore more CFD simulations are used in the initial design stages with many iterations back and forth and even these days CFD-based design optimization (CDO) is often used in the design process Nowadays it is my understanding that wind tunnels are used these days primarily for the following reasons (1) testing finalized prototypes and (2) conducting fundamental research in supersonic flows especially in order to develop more accurate numerical models Regarding achieving flow similarity when you have two different non-dimensional numbers such as the Reynolds Number and the Mach number the experimentalist must choose which number is most important to match For subsonic flows Reynolds number should be used whereas for transonic and supersonic flows the Mach number should be used Often times one is not able to match the Reynolds number of the actual prototype by using a model test in the wind tunnel Consider for example a 747 which has a Reynolds number of 2 000 000 000 ( reference ) It is almost impossible to produce a w"
679,inzinjerstvo,"If you measure the gain there is usually some variability in the measurements This means that you would have to measure enough times to get a statistic distribution for the gain The computed value should be equal to the mean of the distribution What I'm also curious about is the gain given in Decibels (dB)"
685,inzinjerstvo,"This paper from the Heavy Movable Structures group goes into detail about how roadway movable spans are balanced It references AASHTO ( American Association of State Highway and Transportation Officials ) manuals such as those below AASHTO LRFD Movable Highway Bridge Design Specifications AASHTO Maintenance Manual for Roadways and Bridges The general guidelines are as follows The spans are balanced so that the dead load reaction is greater than 1 000 kips per girder when the span is down This keeps the span down even if the locking mechanism isn't working The balance is calculated multiple times by both the contractor and the engineer based on theoretical and fabricated weights The counterweight is designed so that the weight can be adjusted from the theoretical It must accommodate a 5% heavier or 3 5% lighter span The counterweight can be adjusted by adding or removing individual 1 cu ft blocks These blocks are typically concrete but can also be steel or lead The final check of weight can be done with strain gauges"
688,inzinjerstvo,"Often it's a lack of real estate for any other interchange As the interchange is very narrow compared to a partial cloverleaf The other option is then a diamond interchange The diamond interchange is more economical because it doesn't need a particularly wide bridge (4-6 lanes depending on the traffic of the other road) The single point interchange also has a large conflict area in the middle which is less safe than putting 2 intersections next to each other The one major plus is that left-turning traffic from both off ramps of the Highway can go together The left turning traffic of the other road can go together if a contra-flowing setup is used"
689,inzinjerstvo,"Rod &amp; Nut drive A threaded rod and captive nut with either rod or nut driven rotationally by a motor is liable to offer a good solution Because Power level is set by thread pitch and attachment point to the door While the bigger the better always helps almost any sensible size of motor should be able to be used I say sensible to eliminate utterly tiny motors such as pager vibrator motors But usually anything in the 100 milliWatt to 100 Watt range COULD be used Lower wattage requires longer time The threaded rod provides positive locking at zero power with no prospect of overhauling As long as the door will not flex under Foxy assault when pulled solidly into its frame then dinner is off Level of travel can be set to suit by length of rod and mounting Any door size up to a full dometic house door could be handled in this manner - so a small coop door is well within capability A cordless drill motor + gearbox is liable to be an excellent drive unit These can usually be operated from 12V (9V to 18V units) and usually have a two stage reduction gearbox They usually have a reversing switch which is not useful in this context if remote operation is required To use dismantle the drill bypass or remove the switch and feed voltage to the two motor wires directly Reversing the polarity reverses direction Here is a Barn-door star tracker which illustrates the principle well Their calculations page is here - overkill in this case but potentially useful A zillion versions of how you might do this - each image links to a webpage RF Link As a bonus a radio link using 2 x Arduinos and 2 RF modules with a range of 10's of metres to a few kilometres can be constructed for about $10 in components all up Ask if interested This also applies to the wiper based system below Wiper motor &amp; mechanism A possible solution depending on power availability is a wind screen wiper motor and mechanism These are made to sweep a wiper arm across a sometimes dirty windscreen with considerable drag force Units made for truck use are substantially more powerful A typical automotive unit is rated at 50-100 Watts at 12V but will operate at lower voltage with reduced power I have some Indian made truck wiper units rated at about 300 Watts A 12V motor can be operated from a very small lead acid battery - say 12 V x 1 2 Ah These can be charged by solar power The battery should be maintained at a constant 13 7 Volts You can obtain dedicated regulators for this purpose - PB137 is similar to the standard LM317 but rated at 13 7 Volts out PB137 in stock Digikey $1 27 in 1's PB137 Data Sheet Note that a wiper mechanism is liable to have substantial backlash and depending on which way the door swings may allow entry by pressing on the door If the door is externa"
692,inzinjerstvo,"BD37/01 has the required loading in Chapter 4 Section 4 1 gives the HB loading required for various bridges uses It doesn't segregate based on superstructure type; it only segregates based on roadway class Motorway and Trunk Road Principal roads and Other public roads This means that both steel and concrete bridges are designed to the same number of HB units"
696,inzinjerstvo,"As Hazzey pointed out momentum isn't particularly an issue with a plow with hand or animal plows the resistance that the plow meets will be strong enough to essentially hinder momentum Your real issue is preventing the plow from being uprooted by the resistance force usually the weight of the plow will assist here However a lower weight is not a big issue here as you can easily modify your design to meet the new requirements Perhaps you can anchor/fish hook the plow into the ground Another potential design change is to add a pivot somewhere in the design to apply a constant downward force"
700,inzinjerstvo,"This will depend very much on the heat flow around the enclosure There is a thing called the H factor which describes the heat transport properties between a surface and a fluid The value of H varies with surface properties and flow So to do your calculation you need a few different assumptions Let's simplify 1) the air in the box is uniformly heated 2) air flows around five sides of the box the same way; no heat transfer takes place through the bottom Now we can consider the box a thermal resistance (the walls) in series with a heat sink (the air flowing) The thermal resistance (K/W) of the walls is given by $$R=\frac{t}{kA}$$ For your box t = 1 6 mm A = 3 4E5 mm$^2$ k = 17 K/W so R = 2 8E-4 K/W the walls themselves won't present much thermal resistance So we need to look at the air around it The heat transfer coefficient is a strong function of air flow rate (this is why you have fans for cooling ) Let's make a simple assumption that there is some natural air flow but no forced convection The heat transfer coefficient will then be at the lower end of the scale or about 0 5 W/m^2/K (See for example this reference ) With the used area of the box of 0 34 m$^2$ (12x16+(12+16+12+16)x6 in$^2$) you get about 1 5 K temperature rise inside the box for every Watt of power dissipated A little fan around the outside could easily cut that by a factor 10 So just to drive the point home explicitly this calculation is conservative and VERY APPROXIMATE ; the number calculated can be easily an order of magnitude off from the number you get when you build your box because there are SO many factors that will affect this Controlling the air flow explicitly (versus relying on whatever air flow lies around ) is really essential to get any kind of believable limits on your answer This is reflected in the comments above"
702,inzinjerstvo,"I am going to start out by saying that aluminum is a mess There are too many designations and things that you can do to it after it solidifies To directly answer your question what might necessitate an additional round of heat treating is directly affected by the desired final mechanical properties Each of the designations that you listed is a complete heat treatment Either one is chosen or the other is chosen they are not done in a sequence other than what is already defined by the string of numbers From the 7075 chart on page 11 of this pdf the only difference between the -T6 and the -T7 is that the -T7 has a slightly better resistance to stress corrosion cracking If this is what you are looking for choose the -T7 The reason that there are so many different alloys and heat treatments is that each combination produces a material with slightly different physical properties Properties of the two heat treatments that you listed 7075-T651 7075-T7351 (slightly different from the one you listed) Without very detailed requirements (and lots of experimenting) it is almost impossible to say which alloy and heat treatment of aluminum will be the best for a given situation You can always go with the most common types"
704,inzinjerstvo,"As stated the problem is probably not well enough constrained Given the specification the way you get minimum temperature out is to pump as much volume as you can through the primary and then regulate the input temperature to achieve desired output I e the more primary flow there is the lower delta temperature drop you need in the primary side Pumping flat out may not have been what you had in mind Defining $T_{p in}$ = Temperature primary in $T_{s out}$ = Temperature secondary out etc $$kV_{in}(T_{p in}-T_{p out}) = V_{out}(T_{s out}-T_{s in})$$ k is an efficiency factor for your heat exchanger i e Energy removed from the primary stream is $V_{in}\Delta T_{in}$ and energy received by the secondary stream is $V_{out}\Delta T_{out}$ with a factor to account for losses and inefficiencies in the system Even with a perfectly insulated lossless system there will be a temperature delta above ideal due to the thermal transfer resistance of the exchanger I assume that heating due to flow / pumping losses in the heat exchanger are ignored (or otherwise they add to the energy available to transfer) $T_{p out}$ must always be greater than or equal to $T_{s out}$ and may be greater than $T_{s in}$ depending on design (but you hope not) For an ideal non counterflow exchanger $T_{p out}$ is usually much greater than $T_{s out}$ If minimum $T_{p out}$ is a genuine need then you want to use a counterflow heat exchanger A counterflow exchanger allows $T_{p out}$ to approach $T_{s in}$ and $T_{s out}$ to approach $T_{p in}$"
705,inzinjerstvo,"I can say using accelerometers to detect displacement won't work These devices have so much of inherent noise that discerning very slow acceleration from remaining immobile is about impossible You won't be able to detect acceleration of order of 0 1 mm/s 2 and it takes just a several seconds at such acceleration to exceed your 5mm You'd have a hard time with a camera as well especially if the device is hand-held True the camera looking along X axis would work with Y Z plane displacement in confined environments (relatively nearby objects in focus) but it would get easily confused by rotation You'd need at least two cameras looking in different (and not just opposite) directions and environment that is conductive to image detection (you won't get much displacement measurement from the image of the sky) Also rotation will be a problem What will work with ~5mm distance ranges is frequency-based distance metering - preferably ultrasound though infrared/visible light would be possible too (though obviously more difficult due to frequencies being much much higher) Unfortunately plain reflective ultrasound distance sensors won't help much about rotation/tilt of the hand distance from walls changing rapidly despite the device not moving What you need are three ultrasound emitters (speakers) each of a different frequency placed around the location where the measurement is to take place and a sampler (microphone) in the handheld By filtering given frequency you get signal from one emitter and then by analyzing the change of phase of the wave in time intervals equal to the wave period you have displacement of the device relative to the emitter By triangulating the three distances you have total displacement (also a gyroscope will help you account for rotation ) One more word of note as to why accelerometers are so ungrateful to work with For your 3-axis accelerometer the immobile [X Y Z] readouts are [0 0 -9 807] m/s 2 That last factor changes with latitude somewhat with longitudal location and altitude not to mention internal properties of the accelerometer like temperature or air pressure A cursory peek at some generic 3-axis accelerometer datasheet will give you a ballpark figure of maybe 1% precision (0 3% non-linearity 1% cross-talk 300?g/?Hz noise RMS thermal and other minor ones) Let's be overly generous and change it to 0 1% 0 1% of 9 807m/s 2 is 9 807mm/s 2 That's the error you can expect from your accelerometers - changing from immobile to nearly a centimeter per second over course of one second sunk in the noise"
1711,inzinjerstvo,"Yes this is normal By default SolidWorks renders curves on the screen using less than the highest possible level of detail that your monitor is capable of displaying The point of this is to allow the screen to be redrawn quickly as you edit your design and change the view The more detailed the curve the longer it takes to redraw the screen; depending on your hardware this could make the software feel unresponsive or laggy and make it harder to use You can adjust the Image Options in the Document Properties tab to render curves with more detail; see documentation here and this screenshot from the API help pages The slider at the top of the window controls how precisely curves are drawn Low - High (slower) and Deviation (For assemblies available only if Apply to all reference part documents (below) is selected ) The slider controls the image quality resolution and Deviation is the maximum chordal deviation in effect Move the slider or type a value in Deviation The slider setting and deviation value are coupled and are inversely proportional If all you're concerned about is confirming that your circle is actually a circle even when it is rendered with corners you may not want to increase this setting as it will have a negative impact on performance"
1723,inzinjerstvo,"Parked vehicles vs moving vehicles Closely spaced parked (or slow moving) vehicles are definitely more onerous as stated on page 89 Appendix 2 A Clause 2 A 1 of the South African bridge design code TMH7 It is generally accepted and can readily be shown that except in the very small span range the worst loading condition occurs under congested (bumper to bumper) conditions caused by a traffic blockage and that the dispersion of traffic at speed caused by increased vehicular inter-spacing more than off-sets the effects of impact However this is only true by inspection for an unlimited number of vehicles of the same weight per unit length If an unusual extremely heavy vehicle is crossing a bridge then we can’t apply this rule as we only have one of this vehicle and it can’t be bumper to bumper with itself So we turn to bridge design codes to consider relevant configurations Bridge design codes When it comes to bridge design every country seems to come up with their own bridge loading in their design code I have read a lot of these codes - a significant proportion of my job is programming these bridge loading codes into bridge design software Most design codes have various loading configurations to be checked although they rarely specify that they are for parked vehicles or moving vehicles The closest I know of is the United Kingdom's BD86 standard BD86 is for assessment of existing UK bridges to carry exceptionally heavy vehicles (named SO or SOV loads) Clauses 3 20 to 3 25 gives two different configurations Either (i) the SO/SOV moves at normal speed with a dynamic factor and normal loading is not allowed within 25m of the SO/SOV; or (ii) the SO/SOV moves at low speed with no dynamic factor but the normal loading is allowed closer (not within 5m) But what about all the other design standards across the world On the basis that I haven't heard of any bridges failing due to the standard design load being too low I think it is a safe assumption that the design standards definitely consider the worst case So whatever it is that is designed for it must represent the worst case out of either moving load or closely spaced static load Most countries' highway loading standards consider two configurations which must be designed for a configuration of a single vehicle (or a single vehicle per lane) and a configuration of a uniformly distributed load (i e a load per area that is applied wherever it adds to the effect being designed for) This second configuration could be the closely spaced parked vehicles or it could represent many lorries/trucks spaced apart but their weight averaged out over the total length I suspect that it may in fact represent both of these situations An important fact here is the relative weight of a lorry/truck compared to a car In the UK a normal lorry (i e one not requiring special permission for its journey) has a maximum weight of 44 metric tonne and has a length of about 12m This gives 3 7t/m A typical car (I've picked the Vauxhall Astra) is about 2 tonne and 4 5m long giving 0 45t/m I suggest that in a moving load situation you might get lorries/trucks fairly well spaced out but in a traffic jam situation when lorries and cars tend to mix the lorry/truck spacing is about the same because the gaps are filled with cars With the cars producing little load per length compared to a lorry/truck the average load per unit length may be about the same whether traffic is moving or stationery <"
1727,inzinjerstvo,"It is likely that the noise of the motorcycle is causing the sensor to activate This listing of the advantages and disadvantages of ultrasonic sensors lists one of the disadvantages as While ultrasonics exhibit good immunity to background noise these sensors are still likely to falsely respond to some loud noises like the hissing sound produced by air hoses and relief valves A loud motorcycle could fall into that same level of noise (sound pressure) Other sensor spec sheets that I have seen have phrases like Many acoustic noise sources will have little to no effect on the reported range This would likely be why the sensor is not going off all the time but is triggered by loud noises The noise from a motorcycle is about 100dB"
1736,inzinjerstvo,"As Dave Tweed mentioned this is a simple example of a diverter valve It allows flow to be directed to one or the other output Valve manufacturer site"
1738,inzinjerstvo,"There are good reasons why although most of your links are from several years ago very little has come of any of them The economics and the engineering just aren't favourable Part of the art of an engine is dumping the heat as quickly as possible; furthermore heat is a low-quality form of energy so once your energy is heat you've already lost all your best opportunities to get more work done with it Let's look at those two things in detail getting rid of the heat quickly is the name of the game The exhaust is designed to take heat and the products of combustion away from the engine The radiator does a similar job of removing and dissipating the heat Dumping heat quickly is crucial in a heat engine as the efficiency depends on the temperature of the cold reservoir and the delta to the higher temperature So anything you put in the way such as your proposed energy harvesting device will slow the rate at which heat leaves the engine This not only reduces efficiency (it seems bizarre that you've excluded efficiency as a consideration why else would you look at energy harvesting anyway ) It will also raise the equilibrium temperature of the car's working parts shortening its life If fuel is that valuable that energy harvesting looks attractive then it's worth making the car more efficient in the first place so that there was less waste heat first reduce the consumption of high-value energy before trying to recycle low-value energy And that brings me to energy versus exergy Heat is in a large proportion of cases a waste product It's almost always the least useful form of energy That's really what the Carnot efficiency limit is telling you that to get any work out of low grade heat you can only do so with very low efficiency; that is almost all of the heat will stay as heat When doing engineering with heat and other forms of energy it's very useful to build up an intuition to distinguish between energy (the thing measured in joules) and exergy (the thing that gets work done) The form that energy is in determines how much work it can do Chemical energy such as that in fuel can do huge amounts of work efficiently - it has very high exergy But the same amount of energy as heat can do much less work - it has very low exergy Internal combustion engines are so inefficient because they take a high-grade form of energy (chemical) and immediately convert into a low-grade one (heat) The humble alternator is very good at its job Solid-state thermo-electric generators simply don't come close not in cost not in performance Using the relative rotation of a conductor and a magnet is a very effective means of turning heat into electricity that's why it's used in power stations cars bike dynamos and many other uses So although energy harvesting looks clever it's bad engineering its trying to fix one symptom rather than address the underlying cause If you want more work out of those joules then get that work done before those joules are in the form of heat"
1777,inzinjerstvo,"What are tin whiskers The easy explanation is that Tin Whiskers are tiny threads of tin that grow out of lead-free tin soldier They are a problem because they can cause electrical shorts at random times They can begin to grow from weeks to years after a part is produced The exact causes of tin whiskers are not known but their growth seems to be caused by internal stresses in the tin that work to push out the thin whiskers Since the issues are caused by the internal structure of the tin analyzing the basic material properties is hard Tin is not the only material that has whiskers Metallic whiskers have also been researched in zinc cadmium indium silver aluminum and gold How to mitigate tin whiskers From this article from EE Times and this article from Electronics Design the following are strategies in order of mitigating the root cause to mitigating the effects Don't use pure tin Use a matte tin finish instead of a bright tin finish Anneal the tin after plating Refinish the part with a hot-solder dip Apply a conformal coating These are special coatings that help to contain the whiskers as they form They also can help to insulate board components from being shorted by whiskers"
1799,inzinjerstvo,"No the responsibility was squarely with the engineers First the original design couldn't be practically built That's not negligent but not too smart The construction company proposed a alternate design which Gillum then didn't object to I have heard different versions where Gillum failed to respond to Havens Steels proposal which was then taken as implicit approval or they outright approved it Either way it was Gillum's responsibility to oversee the design and make sure it met the requirements and the code Apparently even the original design was not up to code It's hard to see how Gillum and the engineers working for Gillum were not grossly negligent here"
1811,inzinjerstvo,"I would suggest a Dremel-like cutting wheel positioned at the front which can be rotated around 360? such that the wheel stays parallel to the inner surface of the pipe The small diameter of the cutting wheel will mean you can cut quite close without damaging the pipe This is assuming you will have a camera and light on the front to see where to orient the cutter Another approach would be to have a hole-cutter type blade on the front mounted just inside another slightly larger cylinder to prevent the blade from hitting the inside surface of the pipe as it moves when biting into roots"
1813,inzinjerstvo,"CFD I ran this situation through a couple of simple 2D CFD models As you can see from the pictures below one way to keep debris from gathering in this area is to increase the velocity of wind in the corner To do this you need to place an obstacle that will direct the wind into the corner Original Situation Obstacle Added Even with this there will be a dead spot in the corner This should help to lessen it though Another benefit of this configuration is that it works just the same if the wind is coming form the top What kind of obstacle The exact design of the obstacle wouldn't need to be too involved A planter or solid bench may be enough to make a difference The main criteria are Solid so that the wind is directed around it High enough that the wind moving over top of it doesn't affect the wind moving around There will likely be some trial and error"
1831,inzinjerstvo,"Per ACI 318 13 2 two-way slabs are designed based on column strips and middle strips To paraphrase the code A Column strip is a design strip with a width on each side of a column centerline equal to 0 25L 1 or 0 25L 2 whichever is less Column strip includes beams if any L 1 and L 1 are the span lengths in the two slab directions A Middle strip is a design strip bounded by two column strips This is a simplified method in the code and has some criteria to meet This method then allows you to accommodate holes in the slab by placing the missing reinforcement on the edges of the hole I don't know how this compares to the Hilleborg method"
1837,inzinjerstvo,"Perhaps a press-fit spring plunger Some product codes to get you started - MCS 05137617 05137815 65155566 - Grainger 2YLK5 2YLN2 2YLL9"
1941,inzinjerstvo,"An AAC wall has an insulation R rating of 1 43 A cavity brick wall has an R value of under 0 82 So AAC isn't even double the insulation of cavity bricks and at this point AAC may not be as inexpensive as cavity bricks + standard insulation There are many many construction walls that have much greater R values but as you've not given any information on your structure and environment it's impossible for us to weigh tradeoffs for you Keep in mind though that a standard 2x4 framed wall with R-13 insulation will have an R value of 10 - 6 times greater than AAC So unless you have special requirements that require concrete or brick walls standard timber framing should be cheaper and better unless you're looking at basement or foundation walls"
1942,inzinjerstvo,"You are essentially asking can a cyclone be designed that works without or against gravity and the answer is of course yes Most practical industrial designs can use gravity to increase efficiency or reduce cost and the equations you've seen need this simplification but a cyclone doesn't depend on gravity You just need to whip the air around fast enough that the larger particles can't make it into the smaller diameter exit Orientation is irrelevant when you've got high rotation rates and the powerful air pumps used in the Dyson vacuums can produce very high rotation rates Further the vacuums are multi stage cyclones Even my old Dyson has at least two stages one big obvious cyclone and then a bunch of tiny cyclones which are designed for the smaller particles Notably mine does have a two stage air filter after the cyclones but I'm sure the newer models with their even smaller second or third stage cyclones and more powerful motors are even better with smaller particles I would be surprised if they could get a HEPA rating on a cyclone-only design though so I'd be wary of their marketing If you look at the first stage you'll find that inverting it has only one effect - the trash comes to the top rather than the bottom But it still doesn't enter the smaller diameter exit tube for that stage The particles weigh too much and the spinning wind prevents them from ever leaving the edges of the container So while some industrial cyclones and those you've studied all rely on gravity for some portion of their rated design that doesn't need to be the case at all Lastly continuous changes in orientation simply don't matter that much I'm sure they do affect it but the speed of change from one orientation to another is very slow relative to the speed of the spinning in the cyclone Energy is lost perhaps a few more particles get through that wouldn't otherwise get through if it were held stationary but they are caught by the next stage and this would only happen to those particles that are closer to the size the next stage is designed for anyway"
1945,inzinjerstvo,"SC-FDMA is encoded and transmitted from the handset where there are power tradeoffs This encoding is better than OFDMA for the handset for the following related reasons Reduces peak to average power ratio Increases efficiency of the power amplifier Increases battery life So at the base station they can employ transmitters that can handle a higher peak transmit power with lower efficiency as it has a significant amount of power available But on a mobile handset this requires more expensive components more space and probably most importantly more power since peak transmissions are not as efficient as transmitting at a lower level but for a longer period of time (ie higher peak to average ratio means lower energy efficiency more heat more power) The tradeoff is a little lower spectrum efficiency but it's worth it for increased battery life and particularly since most high speed data is going to the handset not coming from it This short video explains further"
1991,inzinjerstvo,"I ended up finding a solution that works (haven't tried the cements suggested by others yet) The trick was to use a spot welder rather than a tig welder With the correct power settings that stuck the TC to the surface without damaging either component It's not exceptionally strong (I used very fine TC wires) but it should result in conditions that are as close as possible to what they would be if the TC were not there"
